[2025-07-10 22:51:18,165][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'tblog', 'wandb_project': None, 'azureml_logging': False, 'seed': 1337, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/home/incantator/Documents/mbari-mae/mae_ast', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 6, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 1048576, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 5, 'validate_interval_updates': 100, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1048576, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 300, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [8], 'lr': [0.0001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 100, 'keep_interval_updates': 1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'mae_ast', 'ast_kernel_size_chan': 16, 'ast_kernel_size_time': 16, 'ast_kernel_stride_chan': 16, 'ast_kernel_stride_time': 16, 'encoder_layers': 4, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_norm_first': False, 'feature_grad_mult': 0.1, 'use_post_enc_proj': False, 'decoder_embed_dim': 768, 'decoder_layers': 2, 'decoder_layerdrop': 0.0, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'random_mask_prob': 0.75, 'mask_length': 10, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'checkpoint_activations': False, 'max_token_length': 48000, 'enc_sine_pos': True, 'enc_conv_pos': False, 'dec_sine_pos': True, 'dec_conv_pos': False}, 'task': {'_name': 'mae_ast_pretraining', 'data': '/home/incantator/Documents/mbari-mae/data/audio_chunks-MARS-20171030T000000Z-10secs', 'sample_rate': 2000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 40000, 'min_sample_size': 5000, 'random_crop': True, 'pad_audio': False, 'feature_type': 'fbank', 'feature_rate': 100, 'feature_dim': 128, 'deltas': False, 'mask_spans': False, 'mask_type': random_mask}, 'criterion': {'_name': 'mae_ast', 'reconstruction_weight': 10.0, 'classification_weight': 1.0}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0001]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 4000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 300.0, 'lr': [0.0001]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2025-07-10 22:51:18,167][mae_ast.tasks.mae_ast_pretraining][INFO] - current directory is /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base
[2025-07-10 22:51:18,167][mae_ast.tasks.mae_ast_pretraining][INFO] - MAEPretrainingTask Config {'_name': 'mae_ast_pretraining', 'data': '/home/incantator/Documents/mbari-mae/data/audio_chunks-MARS-20171030T000000Z-10secs', 'sample_rate': 2000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 40000, 'min_sample_size': 5000, 'random_crop': True, 'pad_audio': False, 'feature_type': 'fbank', 'feature_rate': 100, 'feature_dim': 128, 'deltas': False, 'mask_spans': False, 'mask_type': random_mask}
[2025-07-10 22:51:18,169][mae_ast.models.mae_ast][INFO] - MAEModel Config: {'_name': 'mae_ast', 'ast_kernel_size_chan': 16, 'ast_kernel_size_time': 16, 'ast_kernel_stride_chan': 16, 'ast_kernel_stride_time': 16, 'encoder_layers': 4, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_norm_first': False, 'feature_grad_mult': 0.1, 'use_post_enc_proj': False, 'decoder_embed_dim': 768, 'decoder_layers': 2, 'decoder_layerdrop': 0.0, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'random_mask_prob': 0.75, 'mask_length': 10, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'checkpoint_activations': False, 'max_token_length': 48000, 'enc_sine_pos': True, 'enc_conv_pos': False, 'dec_sine_pos': True, 'dec_conv_pos': False}
[2025-07-10 22:51:18,520][fairseq_cli.train][INFO] - MAE_AST(
  (feature_extractor): Identity()
  (post_extract_proj): Linear(in_features=256, out_features=768, bias=True)
  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (batch_norm): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  (unfold): Unfold(kernel_size=(16, 16), dilation=1, padding=0, stride=(16, 16))
  (dropout_input): Dropout(p=0.1, inplace=False)
  (enc_sine_pos_embed): SinusoidalPositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-3): 4 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (dec_sine_pos_embed): SinusoidalPositionalEncoding()
  (decoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (final_proj_reconstruction): Linear(in_features=768, out_features=256, bias=True)
  (final_proj_classification): Linear(in_features=768, out_features=256, bias=True)
)
[2025-07-10 22:51:18,521][fairseq_cli.train][INFO] - task: MAE_AST_Pretraining_Task
[2025-07-10 22:51:18,521][fairseq_cli.train][INFO] - model: MAE_AST
[2025-07-10 22:51:18,521][fairseq_cli.train][INFO] - criterion: MAE_AST_Criterion
[2025-07-10 22:51:18,521][fairseq_cli.train][INFO] - num. shared model params: 43,123,200 (num. trained: 43,123,200)
[2025-07-10 22:51:18,521][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2025-07-10 22:51:18,523][mae_ast.data.mae_ast_dataset][INFO] - max_keep=None, min_keep=5000, loaded 53, skipped 0 short and 0 long, longest-loaded=20000, shortest-loaded=20000
[2025-07-10 22:51:18,523][mae_ast.data.mae_ast_dataset][INFO] - pad_audio=False, random_crop=True, normalize=False, max_sample_size=40000
[2025-07-10 22:51:18,634][fairseq.trainer][INFO] - detected shared parameter: batch_norm.weight <- batch_norm.bias
[2025-07-10 22:51:18,634][fairseq.utils][INFO] - ***********************CUDA enviroments for all 1 workers***********************
[2025-07-10 22:51:18,634][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 15.687 GB ; name = NVIDIA GeForce RTX 4080 SUPER           
[2025-07-10 22:51:18,634][fairseq.utils][INFO] - ***********************CUDA enviroments for all 1 workers***********************
[2025-07-10 22:51:18,634][fairseq_cli.train][INFO] - training on 1 devices (GPUs/TPUs)
[2025-07-10 22:51:18,634][fairseq_cli.train][INFO] - max tokens per device = 1048576 and max sentences per device = None
[2025-07-10 22:51:18,635][fairseq.trainer][INFO] - Preparing to load checkpoint checkpoints/checkpoint_last.pt
[2025-07-10 22:51:18,635][fairseq.trainer][INFO] - No existing checkpoint found checkpoints/checkpoint_last.pt
[2025-07-10 22:51:18,635][fairseq.trainer][INFO] - loading train data for epoch 1
[2025-07-10 22:51:18,636][mae_ast.data.mae_ast_dataset][INFO] - max_keep=None, min_keep=5000, loaded 989, skipped 0 short and 0 long, longest-loaded=20000, shortest-loaded=20000
[2025-07-10 22:51:18,636][mae_ast.data.mae_ast_dataset][INFO] - pad_audio=False, random_crop=True, normalize=False, max_sample_size=40000
[2025-07-10 22:51:19,033][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:51:19,035][fairseq.trainer][INFO] - begin training epoch 1
[2025-07-10 22:51:19,035][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:51:22,749][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1 @ 3 updates
[2025-07-10 22:51:22,749][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint1.pt
[2025-07-10 22:51:23,194][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint1.pt
[2025-07-10 22:51:23,365][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint1.pt (epoch 1 @ 3 updates, score None) (writing took 0.6164334449995295 seconds)
[2025-07-10 22:51:23,366][fairseq_cli.train][INFO] - end of epoch 1 (average epoch stats below)
[2025-07-10 22:51:23,367][train][INFO] - {"epoch": 1, "train_loss": "39.222", "train_nll_loss": "0.105", "train_loss_recon": "0.845", "train_loss_info_nce": "30.74", "train_ppl": "1.08", "train_wps": "2379.2", "train_ups": "0.91", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "3", "train_lr": "7.5e-08", "train_gnorm": "283.394", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "3", "train_gb_free": "10.4", "train_wall": "5"}
[2025-07-10 22:51:23,405][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:51:23,407][fairseq.trainer][INFO] - begin training epoch 2
[2025-07-10 22:51:23,408][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:51:26,439][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 2 @ 6 updates
[2025-07-10 22:51:26,439][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint2.pt
[2025-07-10 22:51:26,884][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint2.pt
[2025-07-10 22:51:27,231][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint2.pt (epoch 2 @ 6 updates, score None) (writing took 0.7925505969997175 seconds)
[2025-07-10 22:51:27,231][fairseq_cli.train][INFO] - end of epoch 2 (average epoch stats below)
[2025-07-10 22:51:27,232][train][INFO] - {"epoch": 2, "train_loss": "39.242", "train_nll_loss": "0.105", "train_loss_recon": "0.846", "train_loss_info_nce": "30.79", "train_ppl": "1.08", "train_wps": "2118", "train_ups": "0.78", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "6", "train_lr": "1.5e-07", "train_gnorm": "284.583", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "9"}
[2025-07-10 22:51:27,266][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:51:27,268][fairseq.trainer][INFO] - begin training epoch 3
[2025-07-10 22:51:27,268][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:51:30,289][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 3 @ 9 updates
[2025-07-10 22:51:30,289][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint3.pt
[2025-07-10 22:51:30,720][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint3.pt
[2025-07-10 22:51:31,103][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint3.pt (epoch 3 @ 9 updates, score None) (writing took 0.8146580970005743 seconds)
[2025-07-10 22:51:31,104][fairseq_cli.train][INFO] - end of epoch 3 (average epoch stats below)
[2025-07-10 22:51:31,105][train][INFO] - {"epoch": 3, "train_loss": "39.156", "train_nll_loss": "0.105", "train_loss_recon": "0.845", "train_loss_info_nce": "30.695", "train_ppl": "1.08", "train_wps": "2114.1", "train_ups": "0.77", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "9", "train_lr": "2.25e-07", "train_gnorm": "283.147", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "12"}
[2025-07-10 22:51:31,137][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:51:31,138][fairseq.trainer][INFO] - begin training epoch 4
[2025-07-10 22:51:31,139][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:51:34,217][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 4 @ 12 updates
[2025-07-10 22:51:34,217][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint4.pt
[2025-07-10 22:51:34,643][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint4.pt
[2025-07-10 22:51:35,026][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint4.pt (epoch 4 @ 12 updates, score None) (writing took 0.8090517729997373 seconds)
[2025-07-10 22:51:35,026][fairseq_cli.train][INFO] - end of epoch 4 (average epoch stats below)
[2025-07-10 22:51:35,027][train][INFO] - {"epoch": 4, "train_loss": "38.989", "train_nll_loss": "0.105", "train_loss_recon": "0.845", "train_loss_info_nce": "30.523", "train_ppl": "1.08", "train_wps": "2086.8", "train_ups": "0.76", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "12", "train_lr": "3e-07", "train_gnorm": "280.724", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "16"}
[2025-07-10 22:51:35,066][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:51:35,068][fairseq.trainer][INFO] - begin training epoch 5
[2025-07-10 22:51:35,068][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:51:38,144][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:51:38,480][valid][INFO] - {"epoch": 5, "valid_loss": "37.304", "valid_nll_loss": "0.1", "valid_loss_recon": "0.816", "valid_loss_info_nce": "29.145", "valid_ppl": "1.07", "valid_wps": "56914.2", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "15"}
[2025-07-10 22:51:38,481][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 5 @ 15 updates
[2025-07-10 22:51:38,482][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint5.pt
[2025-07-10 22:51:38,919][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint5.pt
[2025-07-10 22:51:39,496][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint5.pt (epoch 5 @ 15 updates, score 37.304) (writing took 1.0145040639999934 seconds)
[2025-07-10 22:51:39,496][fairseq_cli.train][INFO] - end of epoch 5 (average epoch stats below)
[2025-07-10 22:51:39,497][train][INFO] - {"epoch": 5, "train_loss": "38.778", "train_nll_loss": "0.104", "train_loss_recon": "0.845", "train_loss_info_nce": "30.357", "train_ppl": "1.07", "train_wps": "1831.5", "train_ups": "0.67", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "15", "train_lr": "3.75e-07", "train_gnorm": "280.934", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "21"}
[2025-07-10 22:51:39,536][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:51:39,538][fairseq.trainer][INFO] - begin training epoch 6
[2025-07-10 22:51:39,538][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:51:42,586][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 6 @ 18 updates
[2025-07-10 22:51:42,587][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint6.pt
[2025-07-10 22:51:43,025][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint6.pt
[2025-07-10 22:51:43,319][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint6.pt (epoch 6 @ 18 updates, score None) (writing took 0.7329043519994229 seconds)
[2025-07-10 22:51:43,319][fairseq_cli.train][INFO] - end of epoch 6 (average epoch stats below)
[2025-07-10 22:51:43,321][train][INFO] - {"epoch": 6, "train_loss": "38.508", "train_nll_loss": "0.104", "train_loss_recon": "0.846", "train_loss_info_nce": "30.041", "train_ppl": "1.07", "train_wps": "2141", "train_ups": "0.78", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "18", "train_lr": "4.5e-07", "train_gnorm": "278.15", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "25"}
[2025-07-10 22:51:43,360][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:51:43,362][fairseq.trainer][INFO] - begin training epoch 7
[2025-07-10 22:51:43,362][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:51:46,450][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 7 @ 21 updates
[2025-07-10 22:51:46,451][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint7.pt
[2025-07-10 22:51:46,879][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint7.pt
[2025-07-10 22:51:47,255][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint7.pt (epoch 7 @ 21 updates, score None) (writing took 0.804493241000273 seconds)
[2025-07-10 22:51:47,255][fairseq_cli.train][INFO] - end of epoch 7 (average epoch stats below)
[2025-07-10 22:51:47,256][train][INFO] - {"epoch": 7, "train_loss": "37.144", "train_nll_loss": "0.1", "train_loss_recon": "0.844", "train_loss_info_nce": "28.69", "train_ppl": "1.07", "train_wps": "2080", "train_ups": "0.76", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "21", "train_lr": "5.25e-07", "train_gnorm": "266.933", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "29"}
[2025-07-10 22:51:47,297][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:51:47,298][fairseq.trainer][INFO] - begin training epoch 8
[2025-07-10 22:51:47,299][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:51:50,335][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 8 @ 24 updates
[2025-07-10 22:51:50,336][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint8.pt
[2025-07-10 22:51:50,767][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint8.pt
[2025-07-10 22:51:51,263][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint8.pt (epoch 8 @ 24 updates, score None) (writing took 0.9278170030002002 seconds)
[2025-07-10 22:51:51,263][fairseq_cli.train][INFO] - end of epoch 8 (average epoch stats below)
[2025-07-10 22:51:51,264][train][INFO] - {"epoch": 8, "train_loss": "36.531", "train_nll_loss": "0.098", "train_loss_recon": "0.843", "train_loss_info_nce": "28.083", "train_ppl": "1.07", "train_wps": "2042.5", "train_ups": "0.75", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "24", "train_lr": "6e-07", "train_gnorm": "261.422", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "33"}
[2025-07-10 22:51:51,301][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:51:51,303][fairseq.trainer][INFO] - begin training epoch 9
[2025-07-10 22:51:51,303][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:51:54,353][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 9 @ 27 updates
[2025-07-10 22:51:54,354][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint9.pt
[2025-07-10 22:51:54,784][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint9.pt
[2025-07-10 22:51:55,169][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint9.pt (epoch 9 @ 27 updates, score None) (writing took 0.8152878669998245 seconds)
[2025-07-10 22:51:55,169][fairseq_cli.train][INFO] - end of epoch 9 (average epoch stats below)
[2025-07-10 22:51:55,170][train][INFO] - {"epoch": 9, "train_loss": "35.485", "train_nll_loss": "0.095", "train_loss_recon": "0.842", "train_loss_info_nce": "26.863", "train_ppl": "1.07", "train_wps": "2096.1", "train_ups": "0.77", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "27", "train_lr": "6.75e-07", "train_gnorm": "248.462", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "37"}
[2025-07-10 22:51:55,209][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:51:55,211][fairseq.trainer][INFO] - begin training epoch 10
[2025-07-10 22:51:55,211][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:51:58,241][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:51:58,480][valid][INFO] - {"epoch": 10, "valid_loss": "31.947", "valid_nll_loss": "0.086", "valid_loss_recon": "0.81", "valid_loss_info_nce": "23.843", "valid_ppl": "1.06", "valid_wps": "66856.7", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "30", "valid_best_loss": "31.947"}
[2025-07-10 22:51:58,480][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 10 @ 30 updates
[2025-07-10 22:51:58,481][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint10.pt
[2025-07-10 22:51:58,912][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint10.pt
[2025-07-10 22:51:59,930][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint10.pt (epoch 10 @ 30 updates, score 31.947) (writing took 1.4494623359996694 seconds)
[2025-07-10 22:51:59,930][fairseq_cli.train][INFO] - end of epoch 10 (average epoch stats below)
[2025-07-10 22:51:59,931][train][INFO] - {"epoch": 10, "train_loss": "32.887", "train_nll_loss": "0.088", "train_loss_recon": "0.84", "train_loss_info_nce": "24.466", "train_ppl": "1.06", "train_wps": "1719.2", "train_ups": "0.63", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "30", "train_lr": "7.5e-07", "train_gnorm": "220.78", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "41"}
[2025-07-10 22:51:59,970][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:51:59,973][fairseq.trainer][INFO] - begin training epoch 11
[2025-07-10 22:51:59,973][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:52:03,019][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 11 @ 33 updates
[2025-07-10 22:52:03,019][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint11.pt
[2025-07-10 22:52:03,447][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint11.pt
[2025-07-10 22:52:03,851][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint11.pt (epoch 11 @ 33 updates, score None) (writing took 0.8327066129995728 seconds)
[2025-07-10 22:52:03,851][fairseq_cli.train][INFO] - end of epoch 11 (average epoch stats below)
[2025-07-10 22:52:03,852][train][INFO] - {"epoch": 11, "train_loss": "31.936", "train_nll_loss": "0.086", "train_loss_recon": "0.839", "train_loss_info_nce": "23.507", "train_ppl": "1.06", "train_wps": "2087.7", "train_ups": "0.77", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "33", "train_lr": "8.25e-07", "train_gnorm": "208.177", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "45"}
[2025-07-10 22:52:03,891][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:52:03,892][fairseq.trainer][INFO] - begin training epoch 12
[2025-07-10 22:52:03,893][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:52:06,964][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 12 @ 36 updates
[2025-07-10 22:52:06,965][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint12.pt
[2025-07-10 22:52:07,400][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint12.pt
[2025-07-10 22:52:07,800][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint12.pt (epoch 12 @ 36 updates, score None) (writing took 0.8356626699996923 seconds)
[2025-07-10 22:52:07,800][fairseq_cli.train][INFO] - end of epoch 12 (average epoch stats below)
[2025-07-10 22:52:07,801][train][INFO] - {"epoch": 12, "train_loss": "31.111", "train_nll_loss": "0.084", "train_loss_recon": "0.838", "train_loss_info_nce": "22.696", "train_ppl": "1.06", "train_wps": "2073", "train_ups": "0.76", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "36", "train_lr": "9e-07", "train_gnorm": "194.969", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "49"}
[2025-07-10 22:52:07,841][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:52:07,843][fairseq.trainer][INFO] - begin training epoch 13
[2025-07-10 22:52:07,843][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:52:10,949][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 13 @ 39 updates
[2025-07-10 22:52:10,949][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint13.pt
[2025-07-10 22:52:11,377][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint13.pt
[2025-07-10 22:52:11,738][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint13.pt (epoch 13 @ 39 updates, score None) (writing took 0.7891454090004117 seconds)
[2025-07-10 22:52:11,738][fairseq_cli.train][INFO] - end of epoch 13 (average epoch stats below)
[2025-07-10 22:52:11,740][train][INFO] - {"epoch": 13, "train_loss": "29.254", "train_nll_loss": "0.079", "train_loss_recon": "0.835", "train_loss_info_nce": "20.878", "train_ppl": "1.06", "train_wps": "2078.7", "train_ups": "0.76", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "39", "train_lr": "9.75e-07", "train_gnorm": "163.082", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "53"}
[2025-07-10 22:52:11,775][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:52:11,777][fairseq.trainer][INFO] - begin training epoch 14
[2025-07-10 22:52:11,777][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:52:14,812][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 14 @ 42 updates
[2025-07-10 22:52:14,813][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint14.pt
[2025-07-10 22:52:15,239][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint14.pt
[2025-07-10 22:52:15,931][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint14.pt (epoch 14 @ 42 updates, score None) (writing took 1.1189944150000883 seconds)
[2025-07-10 22:52:15,931][fairseq_cli.train][INFO] - end of epoch 14 (average epoch stats below)
[2025-07-10 22:52:15,932][train][INFO] - {"epoch": 14, "train_loss": "28.48", "train_nll_loss": "0.077", "train_loss_recon": "0.834", "train_loss_info_nce": "20.112", "train_ppl": "1.05", "train_wps": "1952.4", "train_ups": "0.72", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "42", "train_lr": "1.05e-06", "train_gnorm": "148.259", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "57"}
[2025-07-10 22:52:15,971][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:52:15,973][fairseq.trainer][INFO] - begin training epoch 15
[2025-07-10 22:52:15,973][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:52:18,978][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:52:19,224][valid][INFO] - {"epoch": 15, "valid_loss": "25.743", "valid_nll_loss": "0.069", "valid_loss_recon": "0.802", "valid_loss_info_nce": "17.721", "valid_ppl": "1.05", "valid_wps": "68036.1", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "45", "valid_best_loss": "25.743"}
[2025-07-10 22:52:19,224][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 15 @ 45 updates
[2025-07-10 22:52:19,225][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint15.pt
[2025-07-10 22:52:19,650][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint15.pt
[2025-07-10 22:52:20,351][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint15.pt (epoch 15 @ 45 updates, score 25.743) (writing took 1.126463663999857 seconds)
[2025-07-10 22:52:20,351][fairseq_cli.train][INFO] - end of epoch 15 (average epoch stats below)
[2025-07-10 22:52:20,352][train][INFO] - {"epoch": 15, "train_loss": "27.209", "train_nll_loss": "0.073", "train_loss_recon": "0.83", "train_loss_info_nce": "18.849", "train_ppl": "1.05", "train_wps": "1852", "train_ups": "0.68", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "45", "train_lr": "1.125e-06", "train_gnorm": "119.755", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "62"}
[2025-07-10 22:52:20,386][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:52:20,388][fairseq.trainer][INFO] - begin training epoch 16
[2025-07-10 22:52:20,388][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:52:23,417][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 16 @ 48 updates
[2025-07-10 22:52:23,417][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint16.pt
[2025-07-10 22:52:23,844][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint16.pt
[2025-07-10 22:52:24,189][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint16.pt (epoch 16 @ 48 updates, score None) (writing took 0.7726271090004957 seconds)
[2025-07-10 22:52:24,189][fairseq_cli.train][INFO] - end of epoch 16 (average epoch stats below)
[2025-07-10 22:52:24,191][train][INFO] - {"epoch": 16, "train_loss": "26.063", "train_nll_loss": "0.07", "train_loss_recon": "0.828", "train_loss_info_nce": "17.764", "train_ppl": "1.05", "train_wps": "2132.9", "train_ups": "0.78", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "48", "train_lr": "1.2e-06", "train_gnorm": "87.33", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "66"}
[2025-07-10 22:52:24,224][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:52:24,226][fairseq.trainer][INFO] - begin training epoch 17
[2025-07-10 22:52:24,226][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:52:27,299][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 17 @ 51 updates
[2025-07-10 22:52:27,299][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint17.pt
[2025-07-10 22:52:27,728][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint17.pt
[2025-07-10 22:52:28,132][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint17.pt (epoch 17 @ 51 updates, score None) (writing took 0.833528256000136 seconds)
[2025-07-10 22:52:28,133][fairseq_cli.train][INFO] - end of epoch 17 (average epoch stats below)
[2025-07-10 22:52:28,134][train][INFO] - {"epoch": 17, "train_loss": "25.583", "train_nll_loss": "0.069", "train_loss_recon": "0.826", "train_loss_info_nce": "17.311", "train_ppl": "1.05", "train_wps": "2076", "train_ups": "0.76", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "51", "train_lr": "1.275e-06", "train_gnorm": "72.04", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "69"}
[2025-07-10 22:52:28,169][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:52:28,171][fairseq.trainer][INFO] - begin training epoch 18
[2025-07-10 22:52:28,171][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:52:31,237][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 18 @ 54 updates
[2025-07-10 22:52:31,238][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint18.pt
[2025-07-10 22:52:31,667][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint18.pt
[2025-07-10 22:52:32,063][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint18.pt (epoch 18 @ 54 updates, score None) (writing took 0.8257844190002288 seconds)
[2025-07-10 22:52:32,063][fairseq_cli.train][INFO] - end of epoch 18 (average epoch stats below)
[2025-07-10 22:52:32,064][train][INFO] - {"epoch": 18, "train_loss": "25.103", "train_nll_loss": "0.067", "train_loss_recon": "0.823", "train_loss_info_nce": "16.853", "train_ppl": "1.05", "train_wps": "2082.7", "train_ups": "0.76", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "54", "train_lr": "1.35e-06", "train_gnorm": "63.04", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "73"}
[2025-07-10 22:52:32,101][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:52:32,103][fairseq.trainer][INFO] - begin training epoch 19
[2025-07-10 22:52:32,104][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:52:35,182][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 19 @ 57 updates
[2025-07-10 22:52:35,182][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint19.pt
[2025-07-10 22:52:35,620][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint19.pt
[2025-07-10 22:52:35,987][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint19.pt (epoch 19 @ 57 updates, score None) (writing took 0.8047136369996224 seconds)
[2025-07-10 22:52:35,987][fairseq_cli.train][INFO] - end of epoch 19 (average epoch stats below)
[2025-07-10 22:52:35,988][train][INFO] - {"epoch": 19, "train_loss": "24.456", "train_nll_loss": "0.066", "train_loss_recon": "0.818", "train_loss_info_nce": "16.247", "train_ppl": "1.05", "train_wps": "2086.4", "train_ups": "0.76", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "57", "train_lr": "1.425e-06", "train_gnorm": "50.532", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "77"}
[2025-07-10 22:52:36,022][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:52:36,023][fairseq.trainer][INFO] - begin training epoch 20
[2025-07-10 22:52:36,024][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:52:39,072][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:52:39,312][valid][INFO] - {"epoch": 20, "valid_loss": "22.964", "valid_nll_loss": "0.062", "valid_loss_recon": "0.779", "valid_loss_info_nce": "15.179", "valid_ppl": "1.04", "valid_wps": "67850.7", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "60", "valid_best_loss": "22.964"}
[2025-07-10 22:52:39,312][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 20 @ 60 updates
[2025-07-10 22:52:39,313][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint20.pt
[2025-07-10 22:52:39,744][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint20.pt
[2025-07-10 22:52:40,941][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint20.pt (epoch 20 @ 60 updates, score 22.964) (writing took 1.6285884749995603 seconds)
[2025-07-10 22:52:40,941][fairseq_cli.train][INFO] - end of epoch 20 (average epoch stats below)
[2025-07-10 22:52:40,942][train][INFO] - {"epoch": 20, "train_loss": "23.919", "train_nll_loss": "0.064", "train_loss_recon": "0.814", "train_loss_info_nce": "15.765", "train_ppl": "1.05", "train_wps": "1652.3", "train_ups": "0.61", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "60", "train_lr": "1.5e-06", "train_gnorm": "42.739", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "82"}
[2025-07-10 22:52:40,982][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:52:40,984][fairseq.trainer][INFO] - begin training epoch 21
[2025-07-10 22:52:40,985][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:52:44,037][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 21 @ 63 updates
[2025-07-10 22:52:44,037][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint21.pt
[2025-07-10 22:52:44,462][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint21.pt
[2025-07-10 22:52:44,838][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint21.pt (epoch 21 @ 63 updates, score None) (writing took 0.8011135410006318 seconds)
[2025-07-10 22:52:44,838][fairseq_cli.train][INFO] - end of epoch 21 (average epoch stats below)
[2025-07-10 22:52:44,839][train][INFO] - {"epoch": 21, "train_loss": "23.465", "train_nll_loss": "0.063", "train_loss_recon": "0.808", "train_loss_info_nce": "15.375", "train_ppl": "1.04", "train_wps": "2100.8", "train_ups": "0.77", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "63", "train_lr": "1.575e-06", "train_gnorm": "38.534", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "86"}
[2025-07-10 22:52:44,876][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:52:44,878][fairseq.trainer][INFO] - begin training epoch 22
[2025-07-10 22:52:44,878][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:52:47,923][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 22 @ 66 updates
[2025-07-10 22:52:47,924][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint22.pt
[2025-07-10 22:52:48,355][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint22.pt
[2025-07-10 22:52:48,744][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint22.pt (epoch 22 @ 66 updates, score None) (writing took 0.820779866000521 seconds)
[2025-07-10 22:52:48,744][fairseq_cli.train][INFO] - end of epoch 22 (average epoch stats below)
[2025-07-10 22:52:48,745][train][INFO] - {"epoch": 22, "train_loss": "23.093", "train_nll_loss": "0.062", "train_loss_recon": "0.802", "train_loss_info_nce": "15.055", "train_ppl": "1.04", "train_wps": "2095.6", "train_ups": "0.77", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "66", "train_lr": "1.65e-06", "train_gnorm": "32.829", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "90"}
[2025-07-10 22:52:48,784][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:52:48,785][fairseq.trainer][INFO] - begin training epoch 23
[2025-07-10 22:52:48,786][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:52:51,822][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 23 @ 69 updates
[2025-07-10 22:52:51,822][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint23.pt
[2025-07-10 22:52:52,254][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint23.pt
[2025-07-10 22:52:52,660][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint23.pt (epoch 23 @ 69 updates, score None) (writing took 0.8378944160003812 seconds)
[2025-07-10 22:52:52,660][fairseq_cli.train][INFO] - end of epoch 23 (average epoch stats below)
[2025-07-10 22:52:52,661][train][INFO] - {"epoch": 23, "train_loss": "22.792", "train_nll_loss": "0.061", "train_loss_recon": "0.797", "train_loss_info_nce": "14.817", "train_ppl": "1.04", "train_wps": "2090.5", "train_ups": "0.77", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "69", "train_lr": "1.725e-06", "train_gnorm": "27.648", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "94"}
[2025-07-10 22:52:52,698][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:52:52,699][fairseq.trainer][INFO] - begin training epoch 24
[2025-07-10 22:52:52,700][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:52:55,745][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 24 @ 72 updates
[2025-07-10 22:52:55,746][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint24.pt
[2025-07-10 22:52:56,180][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint24.pt
[2025-07-10 22:52:56,575][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint24.pt (epoch 24 @ 72 updates, score None) (writing took 0.8302344220001032 seconds)
[2025-07-10 22:52:56,576][fairseq_cli.train][INFO] - end of epoch 24 (average epoch stats below)
[2025-07-10 22:52:56,577][train][INFO] - {"epoch": 24, "train_loss": "22.48", "train_nll_loss": "0.06", "train_loss_recon": "0.79", "train_loss_info_nce": "14.566", "train_ppl": "1.04", "train_wps": "2090.6", "train_ups": "0.77", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "72", "train_lr": "1.8e-06", "train_gnorm": "27.434", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "98"}
[2025-07-10 22:52:56,617][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:52:56,620][fairseq.trainer][INFO] - begin training epoch 25
[2025-07-10 22:52:56,620][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:52:59,729][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:52:59,973][valid][INFO] - {"epoch": 25, "valid_loss": "20.962", "valid_nll_loss": "0.056", "valid_loss_recon": "0.739", "valid_loss_info_nce": "13.574", "valid_ppl": "1.04", "valid_wps": "67975.1", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "75", "valid_best_loss": "20.962"}
[2025-07-10 22:52:59,974][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 25 @ 75 updates
[2025-07-10 22:52:59,974][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint25.pt
[2025-07-10 22:53:00,409][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint25.pt
[2025-07-10 22:53:01,452][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint25.pt (epoch 25 @ 75 updates, score 20.962) (writing took 1.4781600080004864 seconds)
[2025-07-10 22:53:01,452][fairseq_cli.train][INFO] - end of epoch 25 (average epoch stats below)
[2025-07-10 22:53:01,453][train][INFO] - {"epoch": 25, "train_loss": "22.142", "train_nll_loss": "0.06", "train_loss_recon": "0.781", "train_loss_info_nce": "14.314", "train_ppl": "1.04", "train_wps": "1678.5", "train_ups": "0.62", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "75", "train_lr": "1.875e-06", "train_gnorm": "26.223", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "103"}
[2025-07-10 22:53:01,491][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:53:01,493][fairseq.trainer][INFO] - begin training epoch 26
[2025-07-10 22:53:01,493][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:53:04,529][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 26 @ 78 updates
[2025-07-10 22:53:04,529][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint26.pt
[2025-07-10 22:53:04,962][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint26.pt
[2025-07-10 22:53:05,532][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint26.pt (epoch 26 @ 78 updates, score None) (writing took 1.0033849949995783 seconds)
[2025-07-10 22:53:05,533][fairseq_cli.train][INFO] - end of epoch 26 (average epoch stats below)
[2025-07-10 22:53:05,533][train][INFO] - {"epoch": 26, "train_loss": "21.813", "train_nll_loss": "0.059", "train_loss_recon": "0.773", "train_loss_info_nce": "14.068", "train_ppl": "1.04", "train_wps": "2006.3", "train_ups": "0.74", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "78", "train_lr": "1.95e-06", "train_gnorm": "22.632", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "107"}
[2025-07-10 22:53:05,573][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:53:05,576][fairseq.trainer][INFO] - begin training epoch 27
[2025-07-10 22:53:05,577][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:53:08,640][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 27 @ 81 updates
[2025-07-10 22:53:08,641][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint27.pt
[2025-07-10 22:53:09,068][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint27.pt
[2025-07-10 22:53:09,713][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint27.pt (epoch 27 @ 81 updates, score None) (writing took 1.0722450240000398 seconds)
[2025-07-10 22:53:09,713][fairseq_cli.train][INFO] - end of epoch 27 (average epoch stats below)
[2025-07-10 22:53:09,714][train][INFO] - {"epoch": 27, "train_loss": "21.522", "train_nll_loss": "0.058", "train_loss_recon": "0.766", "train_loss_info_nce": "13.855", "train_ppl": "1.04", "train_wps": "1958.1", "train_ups": "0.72", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "81", "train_lr": "2.025e-06", "train_gnorm": "20.503", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "111"}
[2025-07-10 22:53:09,754][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:53:09,756][fairseq.trainer][INFO] - begin training epoch 28
[2025-07-10 22:53:09,756][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:53:12,803][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 28 @ 84 updates
[2025-07-10 22:53:12,803][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint28.pt
[2025-07-10 22:53:13,239][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint28.pt
[2025-07-10 22:53:13,596][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint28.pt (epoch 28 @ 84 updates, score None) (writing took 0.7929653030005284 seconds)
[2025-07-10 22:53:13,596][fairseq_cli.train][INFO] - end of epoch 28 (average epoch stats below)
[2025-07-10 22:53:13,597][train][INFO] - {"epoch": 28, "train_loss": "21.212", "train_nll_loss": "0.057", "train_loss_recon": "0.756", "train_loss_info_nce": "13.642", "train_ppl": "1.04", "train_wps": "2108.1", "train_ups": "0.77", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "84", "train_lr": "2.1e-06", "train_gnorm": "19.735", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "115"}
[2025-07-10 22:53:13,636][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:53:13,638][fairseq.trainer][INFO] - begin training epoch 29
[2025-07-10 22:53:13,638][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:53:16,718][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 29 @ 87 updates
[2025-07-10 22:53:16,719][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint29.pt
[2025-07-10 22:53:17,146][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint29.pt
[2025-07-10 22:53:17,563][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint29.pt (epoch 29 @ 87 updates, score None) (writing took 0.8447144330002629 seconds)
[2025-07-10 22:53:17,563][fairseq_cli.train][INFO] - end of epoch 29 (average epoch stats below)
[2025-07-10 22:53:17,564][train][INFO] - {"epoch": 29, "train_loss": "20.869", "train_nll_loss": "0.056", "train_loss_recon": "0.745", "train_loss_info_nce": "13.414", "train_ppl": "1.04", "train_wps": "2063.3", "train_ups": "0.76", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "87", "train_lr": "2.175e-06", "train_gnorm": "17.949", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "119"}
[2025-07-10 22:53:17,602][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:53:17,604][fairseq.trainer][INFO] - begin training epoch 30
[2025-07-10 22:53:17,604][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:53:20,670][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:53:20,910][valid][INFO] - {"epoch": 30, "valid_loss": "19.235", "valid_nll_loss": "0.052", "valid_loss_recon": "0.687", "valid_loss_info_nce": "12.367", "valid_ppl": "1.04", "valid_wps": "58530.8", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "90", "valid_best_loss": "19.235"}
[2025-07-10 22:53:20,911][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 30 @ 90 updates
[2025-07-10 22:53:20,911][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint30.pt
[2025-07-10 22:53:21,345][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint30.pt
[2025-07-10 22:53:22,241][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint30.pt (epoch 30 @ 90 updates, score 19.235) (writing took 1.3300705869996818 seconds)
[2025-07-10 22:53:22,241][fairseq_cli.train][INFO] - end of epoch 30 (average epoch stats below)
[2025-07-10 22:53:22,243][train][INFO] - {"epoch": 30, "train_loss": "20.571", "train_nll_loss": "0.055", "train_loss_recon": "0.736", "train_loss_info_nce": "13.199", "train_ppl": "1.04", "train_wps": "1749.7", "train_ups": "0.64", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "90", "train_lr": "2.25e-06", "train_gnorm": "17.279", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "124"}
[2025-07-10 22:53:22,283][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:53:22,285][fairseq.trainer][INFO] - begin training epoch 31
[2025-07-10 22:53:22,285][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:53:25,341][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 31 @ 93 updates
[2025-07-10 22:53:25,342][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint31.pt
[2025-07-10 22:53:25,774][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint31.pt
[2025-07-10 22:53:26,560][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint31.pt (epoch 31 @ 93 updates, score None) (writing took 1.2189754529999846 seconds)
[2025-07-10 22:53:26,560][fairseq_cli.train][INFO] - end of epoch 31 (average epoch stats below)
[2025-07-10 22:53:26,561][train][INFO] - {"epoch": 31, "train_loss": "20.287", "train_nll_loss": "0.055", "train_loss_recon": "0.725", "train_loss_info_nce": "13.024", "train_ppl": "1.04", "train_wps": "1895.4", "train_ups": "0.69", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "93", "train_lr": "2.325e-06", "train_gnorm": "16.909", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "128"}
[2025-07-10 22:53:26,599][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:53:26,600][fairseq.trainer][INFO] - begin training epoch 32
[2025-07-10 22:53:26,600][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:53:29,664][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 32 @ 96 updates
[2025-07-10 22:53:29,664][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint32.pt
[2025-07-10 22:53:30,106][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint32.pt
[2025-07-10 22:53:31,015][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint32.pt (epoch 32 @ 96 updates, score None) (writing took 1.351613415000429 seconds)
[2025-07-10 22:53:31,016][fairseq_cli.train][INFO] - end of epoch 32 (average epoch stats below)
[2025-07-10 22:53:31,017][train][INFO] - {"epoch": 32, "train_loss": "19.969", "train_nll_loss": "0.054", "train_loss_recon": "0.714", "train_loss_info_nce": "12.823", "train_ppl": "1.04", "train_wps": "1837.3", "train_ups": "0.67", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "96", "train_lr": "2.4e-06", "train_gnorm": "15.705", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "132"}
[2025-07-10 22:53:31,055][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:53:31,056][fairseq.trainer][INFO] - begin training epoch 33
[2025-07-10 22:53:31,057][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:53:34,147][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 33 @ 99 updates
[2025-07-10 22:53:34,147][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint33.pt
[2025-07-10 22:53:34,580][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint33.pt
[2025-07-10 22:53:34,954][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint33.pt (epoch 33 @ 99 updates, score None) (writing took 0.8067407550006465 seconds)
[2025-07-10 22:53:34,954][fairseq_cli.train][INFO] - end of epoch 33 (average epoch stats below)
[2025-07-10 22:53:34,955][train][INFO] - {"epoch": 33, "train_loss": "19.662", "train_nll_loss": "0.053", "train_loss_recon": "0.702", "train_loss_info_nce": "12.629", "train_ppl": "1.04", "train_wps": "2078.6", "train_ups": "0.76", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "99", "train_lr": "2.475e-06", "train_gnorm": "14.985", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "136"}
[2025-07-10 22:53:34,989][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:53:34,991][fairseq.trainer][INFO] - begin training epoch 34
[2025-07-10 22:53:34,991][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:53:36,562][train_inner][INFO] - {"epoch": 34, "update": 33.333, "loss": "28.055", "nll_loss": "0.075", "loss_recon": "0.806", "loss_info_nce": "19.986", "ppl": "1.05", "wps": "1994.9", "ups": "0.73", "wpb": "2730.5", "bsz": "330.2", "num_updates": "100", "lr": "2.5e-06", "gnorm": "124.246", "clip": "100", "loss_scale": "128", "train_wall": "82", "gb_free": "10.4", "wall": "138"}
[2025-07-10 22:53:36,562][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:53:36,811][valid][INFO] - {"epoch": 34, "valid_loss": "18.17", "valid_nll_loss": "0.049", "valid_loss_recon": "0.647", "valid_loss_info_nce": "11.701", "valid_ppl": "1.03", "valid_wps": "66463.9", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "100", "valid_best_loss": "18.17"}
[2025-07-10 22:53:36,812][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 34 @ 100 updates
[2025-07-10 22:53:36,813][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint_34_100.pt
[2025-07-10 22:53:37,245][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint_34_100.pt
[2025-07-10 22:53:37,964][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_34_100.pt (epoch 34 @ 100 updates, score 18.17) (writing took 1.1512741989999995 seconds)
[2025-07-10 22:53:39,447][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 34 @ 102 updates
[2025-07-10 22:53:39,447][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint34.pt
[2025-07-10 22:53:39,888][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint34.pt
[2025-07-10 22:53:40,680][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint34.pt (epoch 34 @ 102 updates, score None) (writing took 1.2326144310000018 seconds)
[2025-07-10 22:53:40,680][fairseq_cli.train][INFO] - end of epoch 34 (average epoch stats below)
[2025-07-10 22:53:40,681][train][INFO] - {"epoch": 34, "train_loss": "19.37", "train_nll_loss": "0.052", "train_loss_recon": "0.69", "train_loss_info_nce": "12.462", "train_ppl": "1.04", "train_wps": "1429.6", "train_ups": "0.52", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "102", "train_lr": "2.55e-06", "train_gnorm": "14.093", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "142"}
[2025-07-10 22:53:40,716][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:53:40,717][fairseq.trainer][INFO] - begin training epoch 35
[2025-07-10 22:53:40,718][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:53:43,743][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:53:43,983][valid][INFO] - {"epoch": 35, "valid_loss": "17.635", "valid_nll_loss": "0.047", "valid_loss_recon": "0.624", "valid_loss_info_nce": "11.395", "valid_ppl": "1.03", "valid_wps": "68522.8", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "105", "valid_best_loss": "17.635"}
[2025-07-10 22:53:43,984][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 35 @ 105 updates
[2025-07-10 22:53:43,984][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint35.pt
[2025-07-10 22:53:44,419][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint35.pt
[2025-07-10 22:53:45,414][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint35.pt (epoch 35 @ 105 updates, score 17.635) (writing took 1.429571788999965 seconds)
[2025-07-10 22:53:45,414][fairseq_cli.train][INFO] - end of epoch 35 (average epoch stats below)
[2025-07-10 22:53:45,415][train][INFO] - {"epoch": 35, "train_loss": "19.086", "train_nll_loss": "0.051", "train_loss_recon": "0.679", "train_loss_info_nce": "12.287", "train_ppl": "1.04", "train_wps": "1729.1", "train_ups": "0.63", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "105", "train_lr": "2.625e-06", "train_gnorm": "14.027", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "147"}
[2025-07-10 22:53:45,453][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:53:45,455][fairseq.trainer][INFO] - begin training epoch 36
[2025-07-10 22:53:45,455][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:53:48,513][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 36 @ 108 updates
[2025-07-10 22:53:48,513][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint36.pt
[2025-07-10 22:53:48,949][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint36.pt
[2025-07-10 22:53:49,652][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint36.pt (epoch 36 @ 108 updates, score None) (writing took 1.1395337080002719 seconds)
[2025-07-10 22:53:49,653][fairseq_cli.train][INFO] - end of epoch 36 (average epoch stats below)
[2025-07-10 22:53:49,653][train][INFO] - {"epoch": 36, "train_loss": "18.797", "train_nll_loss": "0.051", "train_loss_recon": "0.667", "train_loss_info_nce": "12.119", "train_ppl": "1.04", "train_wps": "1931.1", "train_ups": "0.71", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "108", "train_lr": "2.7e-06", "train_gnorm": "14.162", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "151"}
[2025-07-10 22:53:49,686][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:53:49,688][fairseq.trainer][INFO] - begin training epoch 37
[2025-07-10 22:53:49,688][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:53:52,710][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 37 @ 111 updates
[2025-07-10 22:53:52,710][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint37.pt
[2025-07-10 22:53:53,144][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint37.pt
[2025-07-10 22:53:54,066][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint37.pt (epoch 37 @ 111 updates, score None) (writing took 1.3565787810002803 seconds)
[2025-07-10 22:53:54,067][fairseq_cli.train][INFO] - end of epoch 37 (average epoch stats below)
[2025-07-10 22:53:54,067][train][INFO] - {"epoch": 37, "train_loss": "18.529", "train_nll_loss": "0.05", "train_loss_recon": "0.655", "train_loss_info_nce": "11.968", "train_ppl": "1.04", "train_wps": "1854.5", "train_ups": "0.68", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "111", "train_lr": "2.775e-06", "train_gnorm": "12.44", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "155"}
[2025-07-10 22:53:54,099][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:53:54,101][fairseq.trainer][INFO] - begin training epoch 38
[2025-07-10 22:53:54,101][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:53:57,131][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 38 @ 114 updates
[2025-07-10 22:53:57,132][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint38.pt
[2025-07-10 22:53:57,563][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint38.pt
[2025-07-10 22:53:57,925][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint38.pt (epoch 38 @ 114 updates, score None) (writing took 0.7938020720002896 seconds)
[2025-07-10 22:53:57,926][fairseq_cli.train][INFO] - end of epoch 38 (average epoch stats below)
[2025-07-10 22:53:57,926][train][INFO] - {"epoch": 38, "train_loss": "18.245", "train_nll_loss": "0.049", "train_loss_recon": "0.643", "train_loss_info_nce": "11.808", "train_ppl": "1.03", "train_wps": "2121.3", "train_ups": "0.78", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "114", "train_lr": "2.85e-06", "train_gnorm": "11.583", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "159"}
[2025-07-10 22:53:57,958][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:53:57,960][fairseq.trainer][INFO] - begin training epoch 39
[2025-07-10 22:53:57,960][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:54:01,004][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 39 @ 117 updates
[2025-07-10 22:54:01,005][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint39.pt
[2025-07-10 22:54:01,435][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint39.pt
[2025-07-10 22:54:01,786][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint39.pt (epoch 39 @ 117 updates, score None) (writing took 0.7815153040000951 seconds)
[2025-07-10 22:54:01,786][fairseq_cli.train][INFO] - end of epoch 39 (average epoch stats below)
[2025-07-10 22:54:01,787][train][INFO] - {"epoch": 39, "train_loss": "17.97", "train_nll_loss": "0.048", "train_loss_recon": "0.632", "train_loss_info_nce": "11.645", "train_ppl": "1.03", "train_wps": "2120.3", "train_ups": "0.78", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "117", "train_lr": "2.925e-06", "train_gnorm": "10.871", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "163"}
[2025-07-10 22:54:01,821][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:54:01,823][fairseq.trainer][INFO] - begin training epoch 40
[2025-07-10 22:54:01,823][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:54:04,883][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:54:05,121][valid][INFO] - {"epoch": 40, "valid_loss": "16.12", "valid_nll_loss": "0.043", "valid_loss_recon": "0.556", "valid_loss_info_nce": "10.556", "valid_ppl": "1.03", "valid_wps": "68281.1", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "120", "valid_best_loss": "16.12"}
[2025-07-10 22:54:05,122][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 40 @ 120 updates
[2025-07-10 22:54:05,122][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint40.pt
[2025-07-10 22:54:05,556][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint40.pt
[2025-07-10 22:54:06,288][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint40.pt (epoch 40 @ 120 updates, score 16.12) (writing took 1.166263256000093 seconds)
[2025-07-10 22:54:06,288][fairseq_cli.train][INFO] - end of epoch 40 (average epoch stats below)
[2025-07-10 22:54:06,289][train][INFO] - {"epoch": 40, "train_loss": "17.695", "train_nll_loss": "0.048", "train_loss_recon": "0.619", "train_loss_info_nce": "11.494", "train_ppl": "1.03", "train_wps": "1818.2", "train_ups": "0.67", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "120", "train_lr": "3e-06", "train_gnorm": "11.139", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "168"}
[2025-07-10 22:54:06,322][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:54:06,323][fairseq.trainer][INFO] - begin training epoch 41
[2025-07-10 22:54:06,323][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:54:09,379][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 41 @ 123 updates
[2025-07-10 22:54:09,380][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint41.pt
[2025-07-10 22:54:09,808][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint41.pt
[2025-07-10 22:54:10,501][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint41.pt (epoch 41 @ 123 updates, score None) (writing took 1.1213980419997824 seconds)
[2025-07-10 22:54:10,501][fairseq_cli.train][INFO] - end of epoch 41 (average epoch stats below)
[2025-07-10 22:54:10,502][train][INFO] - {"epoch": 41, "train_loss": "17.441", "train_nll_loss": "0.047", "train_loss_recon": "0.607", "train_loss_info_nce": "11.366", "train_ppl": "1.03", "train_wps": "1943.2", "train_ups": "0.71", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "123", "train_lr": "3.075e-06", "train_gnorm": "10.78", "train_clip": "66.7", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "172"}
[2025-07-10 22:54:10,539][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:54:10,541][fairseq.trainer][INFO] - begin training epoch 42
[2025-07-10 22:54:10,541][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:54:13,653][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 42 @ 126 updates
[2025-07-10 22:54:13,653][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint42.pt
[2025-07-10 22:54:14,087][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint42.pt
[2025-07-10 22:54:14,736][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint42.pt (epoch 42 @ 126 updates, score None) (writing took 1.0830165389998 seconds)
[2025-07-10 22:54:14,736][fairseq_cli.train][INFO] - end of epoch 42 (average epoch stats below)
[2025-07-10 22:54:14,737][train][INFO] - {"epoch": 42, "train_loss": "17.161", "train_nll_loss": "0.046", "train_loss_recon": "0.594", "train_loss_info_nce": "11.206", "train_ppl": "1.03", "train_wps": "1932.8", "train_ups": "0.71", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "126", "train_lr": "3.15e-06", "train_gnorm": "9.604", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "176"}
[2025-07-10 22:54:14,773][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:54:14,775][fairseq.trainer][INFO] - begin training epoch 43
[2025-07-10 22:54:14,775][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:54:17,839][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 43 @ 129 updates
[2025-07-10 22:54:17,840][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint43.pt
[2025-07-10 22:54:18,270][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint43.pt
[2025-07-10 22:54:18,638][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint43.pt (epoch 43 @ 129 updates, score None) (writing took 0.7984390759993403 seconds)
[2025-07-10 22:54:18,638][fairseq_cli.train][INFO] - end of epoch 43 (average epoch stats below)
[2025-07-10 22:54:18,639][train][INFO] - {"epoch": 43, "train_loss": "16.907", "train_nll_loss": "0.045", "train_loss_recon": "0.583", "train_loss_info_nce": "11.074", "train_ppl": "1.03", "train_wps": "2097.9", "train_ups": "0.77", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "129", "train_lr": "3.225e-06", "train_gnorm": "9.282", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "180"}
[2025-07-10 22:54:18,675][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:54:18,678][fairseq.trainer][INFO] - begin training epoch 44
[2025-07-10 22:54:18,678][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:54:21,728][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 44 @ 132 updates
[2025-07-10 22:54:21,729][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint44.pt
[2025-07-10 22:54:22,159][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint44.pt
[2025-07-10 22:54:22,523][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint44.pt (epoch 44 @ 132 updates, score None) (writing took 0.7947651229997064 seconds)
[2025-07-10 22:54:22,523][fairseq_cli.train][INFO] - end of epoch 44 (average epoch stats below)
[2025-07-10 22:54:22,524][train][INFO] - {"epoch": 44, "train_loss": "16.657", "train_nll_loss": "0.045", "train_loss_recon": "0.571", "train_loss_info_nce": "10.936", "train_ppl": "1.03", "train_wps": "2107", "train_ups": "0.77", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "132", "train_lr": "3.3e-06", "train_gnorm": "8.687", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "184"}
[2025-07-10 22:54:22,564][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:54:22,565][fairseq.trainer][INFO] - begin training epoch 45
[2025-07-10 22:54:22,566][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:54:25,611][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:54:25,852][valid][INFO] - {"epoch": 45, "valid_loss": "14.933", "valid_nll_loss": "0.04", "valid_loss_recon": "0.502", "valid_loss_info_nce": "9.916", "valid_ppl": "1.03", "valid_wps": "68919", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "135", "valid_best_loss": "14.933"}
[2025-07-10 22:54:25,853][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 45 @ 135 updates
[2025-07-10 22:54:25,853][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint45.pt
[2025-07-10 22:54:26,298][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint45.pt
[2025-07-10 22:54:27,030][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint45.pt (epoch 45 @ 135 updates, score 14.933) (writing took 1.1775447780000832 seconds)
[2025-07-10 22:54:27,031][fairseq_cli.train][INFO] - end of epoch 45 (average epoch stats below)
[2025-07-10 22:54:27,032][train][INFO] - {"epoch": 45, "train_loss": "16.434", "train_nll_loss": "0.044", "train_loss_recon": "0.56", "train_loss_info_nce": "10.822", "train_ppl": "1.03", "train_wps": "1816.1", "train_ups": "0.67", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "135", "train_lr": "3.375e-06", "train_gnorm": "8.241", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "188"}
[2025-07-10 22:54:27,068][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:54:27,070][fairseq.trainer][INFO] - begin training epoch 46
[2025-07-10 22:54:27,070][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:54:30,105][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 46 @ 138 updates
[2025-07-10 22:54:30,106][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint46.pt
[2025-07-10 22:54:30,545][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint46.pt
[2025-07-10 22:54:30,969][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint46.pt (epoch 46 @ 138 updates, score None) (writing took 0.8640011289999165 seconds)
[2025-07-10 22:54:30,969][fairseq_cli.train][INFO] - end of epoch 46 (average epoch stats below)
[2025-07-10 22:54:30,970][train][INFO] - {"epoch": 46, "train_loss": "16.208", "train_nll_loss": "0.044", "train_loss_recon": "0.551", "train_loss_info_nce": "10.698", "train_ppl": "1.03", "train_wps": "2078.3", "train_ups": "0.76", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "138", "train_lr": "3.45e-06", "train_gnorm": "8.048", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "192"}
[2025-07-10 22:54:31,004][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:54:31,006][fairseq.trainer][INFO] - begin training epoch 47
[2025-07-10 22:54:31,006][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:54:34,070][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 47 @ 141 updates
[2025-07-10 22:54:34,071][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint47.pt
[2025-07-10 22:54:34,511][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint47.pt
[2025-07-10 22:54:34,934][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint47.pt (epoch 47 @ 141 updates, score None) (writing took 0.8639267570006268 seconds)
[2025-07-10 22:54:34,935][fairseq_cli.train][INFO] - end of epoch 47 (average epoch stats below)
[2025-07-10 22:54:34,936][train][INFO] - {"epoch": 47, "train_loss": "15.975", "train_nll_loss": "0.043", "train_loss_recon": "0.54", "train_loss_info_nce": "10.567", "train_ppl": "1.03", "train_wps": "2064.5", "train_ups": "0.76", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "141", "train_lr": "3.525e-06", "train_gnorm": "7.787", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "196"}
[2025-07-10 22:54:34,970][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:54:34,971][fairseq.trainer][INFO] - begin training epoch 48
[2025-07-10 22:54:34,972][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:54:38,031][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 48 @ 144 updates
[2025-07-10 22:54:38,031][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint48.pt
[2025-07-10 22:54:38,462][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint48.pt
[2025-07-10 22:54:39,294][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint48.pt (epoch 48 @ 144 updates, score None) (writing took 1.2635598370006846 seconds)
[2025-07-10 22:54:39,295][fairseq_cli.train][INFO] - end of epoch 48 (average epoch stats below)
[2025-07-10 22:54:39,296][train][INFO] - {"epoch": 48, "train_loss": "15.793", "train_nll_loss": "0.042", "train_loss_recon": "0.531", "train_loss_info_nce": "10.475", "train_ppl": "1.03", "train_wps": "1877.5", "train_ups": "0.69", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "144", "train_lr": "3.6e-06", "train_gnorm": "7.752", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "201"}
[2025-07-10 22:54:39,334][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:54:39,336][fairseq.trainer][INFO] - begin training epoch 49
[2025-07-10 22:54:39,337][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:54:42,391][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 49 @ 147 updates
[2025-07-10 22:54:42,392][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint49.pt
[2025-07-10 22:54:42,838][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint49.pt
[2025-07-10 22:54:43,209][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint49.pt (epoch 49 @ 147 updates, score None) (writing took 0.8181324970000787 seconds)
[2025-07-10 22:54:43,210][fairseq_cli.train][INFO] - end of epoch 49 (average epoch stats below)
[2025-07-10 22:54:43,210][train][INFO] - {"epoch": 49, "train_loss": "15.585", "train_nll_loss": "0.042", "train_loss_recon": "0.522", "train_loss_info_nce": "10.363", "train_ppl": "1.03", "train_wps": "2091.1", "train_ups": "0.77", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "147", "train_lr": "3.675e-06", "train_gnorm": "7.324", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "205"}
[2025-07-10 22:54:43,246][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:54:43,247][fairseq.trainer][INFO] - begin training epoch 50
[2025-07-10 22:54:43,248][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:54:46,288][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:54:46,525][valid][INFO] - {"epoch": 50, "valid_loss": "14.01", "valid_nll_loss": "0.038", "valid_loss_recon": "0.455", "valid_loss_info_nce": "9.461", "valid_ppl": "1.03", "valid_wps": "68715.3", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "150", "valid_best_loss": "14.01"}
[2025-07-10 22:54:46,526][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 50 @ 150 updates
[2025-07-10 22:54:46,526][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint50.pt
[2025-07-10 22:54:46,965][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint50.pt
[2025-07-10 22:54:48,285][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint50.pt (epoch 50 @ 150 updates, score 14.01) (writing took 1.7592340100000001 seconds)
[2025-07-10 22:54:48,285][fairseq_cli.train][INFO] - end of epoch 50 (average epoch stats below)
[2025-07-10 22:54:48,286][train][INFO] - {"epoch": 50, "train_loss": "15.416", "train_nll_loss": "0.041", "train_loss_recon": "0.514", "train_loss_info_nce": "10.276", "train_ppl": "1.03", "train_wps": "1612.7", "train_ups": "0.59", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "150", "train_lr": "3.75e-06", "train_gnorm": "7.144", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "210"}
[2025-07-10 22:54:48,324][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:54:48,326][fairseq.trainer][INFO] - begin training epoch 51
[2025-07-10 22:54:48,326][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:54:51,399][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 51 @ 153 updates
[2025-07-10 22:54:51,400][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint51.pt
[2025-07-10 22:54:51,849][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint51.pt
[2025-07-10 22:54:52,230][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint51.pt (epoch 51 @ 153 updates, score None) (writing took 0.830838648000281 seconds)
[2025-07-10 22:54:52,230][fairseq_cli.train][INFO] - end of epoch 51 (average epoch stats below)
[2025-07-10 22:54:52,231][train][INFO] - {"epoch": 51, "train_loss": "15.222", "train_nll_loss": "0.041", "train_loss_recon": "0.505", "train_loss_info_nce": "10.169", "train_ppl": "1.03", "train_wps": "2075", "train_ups": "0.76", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "153", "train_lr": "3.825e-06", "train_gnorm": "6.832", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "214"}
[2025-07-10 22:54:52,268][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:54:52,269][fairseq.trainer][INFO] - begin training epoch 52
[2025-07-10 22:54:52,270][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:54:55,322][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 52 @ 156 updates
[2025-07-10 22:54:55,323][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint52.pt
[2025-07-10 22:54:55,761][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint52.pt
[2025-07-10 22:54:56,152][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint52.pt (epoch 52 @ 156 updates, score None) (writing took 0.8290879559999667 seconds)
[2025-07-10 22:54:56,152][fairseq_cli.train][INFO] - end of epoch 52 (average epoch stats below)
[2025-07-10 22:54:56,153][train][INFO] - {"epoch": 52, "train_loss": "15.069", "train_nll_loss": "0.041", "train_loss_recon": "0.498", "train_loss_info_nce": "10.083", "train_ppl": "1.03", "train_wps": "2087.4", "train_ups": "0.77", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "156", "train_lr": "3.9e-06", "train_gnorm": "6.551", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "218"}
[2025-07-10 22:54:56,191][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:54:56,192][fairseq.trainer][INFO] - begin training epoch 53
[2025-07-10 22:54:56,192][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:54:59,261][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 53 @ 159 updates
[2025-07-10 22:54:59,261][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint53.pt
[2025-07-10 22:54:59,699][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint53.pt
[2025-07-10 22:55:00,103][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint53.pt (epoch 53 @ 159 updates, score None) (writing took 0.8417423669998243 seconds)
[2025-07-10 22:55:00,103][fairseq_cli.train][INFO] - end of epoch 53 (average epoch stats below)
[2025-07-10 22:55:00,104][train][INFO] - {"epoch": 53, "train_loss": "14.903", "train_nll_loss": "0.04", "train_loss_recon": "0.49", "train_loss_info_nce": "9.997", "train_ppl": "1.03", "train_wps": "2071.8", "train_ups": "0.76", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "159", "train_lr": "3.975e-06", "train_gnorm": "6.229", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "221"}
[2025-07-10 22:55:00,137][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:55:00,139][fairseq.trainer][INFO] - begin training epoch 54
[2025-07-10 22:55:00,139][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:55:03,234][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 54 @ 162 updates
[2025-07-10 22:55:03,234][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint54.pt
[2025-07-10 22:55:03,674][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint54.pt
[2025-07-10 22:55:04,041][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint54.pt (epoch 54 @ 162 updates, score None) (writing took 0.8071816709998529 seconds)
[2025-07-10 22:55:04,041][fairseq_cli.train][INFO] - end of epoch 54 (average epoch stats below)
[2025-07-10 22:55:04,042][train][INFO] - {"epoch": 54, "train_loss": "14.742", "train_nll_loss": "0.04", "train_loss_recon": "0.482", "train_loss_info_nce": "9.917", "train_ppl": "1.03", "train_wps": "2078.6", "train_ups": "0.76", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "162", "train_lr": "4.05e-06", "train_gnorm": "5.877", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "225"}
[2025-07-10 22:55:04,077][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:55:04,079][fairseq.trainer][INFO] - begin training epoch 55
[2025-07-10 22:55:04,079][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:55:07,101][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:55:07,347][valid][INFO] - {"epoch": 55, "valid_loss": "13.329", "valid_nll_loss": "0.036", "valid_loss_recon": "0.424", "valid_loss_info_nce": "9.088", "valid_ppl": "1.03", "valid_wps": "63832.1", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "165", "valid_best_loss": "13.329"}
[2025-07-10 22:55:07,348][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 55 @ 165 updates
[2025-07-10 22:55:07,349][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint55.pt
[2025-07-10 22:55:07,783][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint55.pt
[2025-07-10 22:55:08,641][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint55.pt (epoch 55 @ 165 updates, score 13.329) (writing took 1.2923669419997168 seconds)
[2025-07-10 22:55:08,641][fairseq_cli.train][INFO] - end of epoch 55 (average epoch stats below)
[2025-07-10 22:55:08,642][train][INFO] - {"epoch": 55, "train_loss": "14.631", "train_nll_loss": "0.039", "train_loss_recon": "0.478", "train_loss_info_nce": "9.85", "train_ppl": "1.03", "train_wps": "1779.6", "train_ups": "0.65", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "165", "train_lr": "4.125e-06", "train_gnorm": "6.117", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "230"}
[2025-07-10 22:55:08,677][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:55:08,679][fairseq.trainer][INFO] - begin training epoch 56
[2025-07-10 22:55:08,679][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:55:11,737][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 56 @ 168 updates
[2025-07-10 22:55:11,737][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint56.pt
[2025-07-10 22:55:12,179][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint56.pt
[2025-07-10 22:55:12,889][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint56.pt (epoch 56 @ 168 updates, score None) (writing took 1.152734412999962 seconds)
[2025-07-10 22:55:12,890][fairseq_cli.train][INFO] - end of epoch 56 (average epoch stats below)
[2025-07-10 22:55:12,891][train][INFO] - {"epoch": 56, "train_loss": "14.485", "train_nll_loss": "0.039", "train_loss_recon": "0.47", "train_loss_info_nce": "9.779", "train_ppl": "1.03", "train_wps": "1926.7", "train_ups": "0.71", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "168", "train_lr": "4.2e-06", "train_gnorm": "5.444", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "234"}
[2025-07-10 22:55:12,924][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:55:12,926][fairseq.trainer][INFO] - begin training epoch 57
[2025-07-10 22:55:12,926][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:55:15,981][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 57 @ 171 updates
[2025-07-10 22:55:15,982][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint57.pt
[2025-07-10 22:55:16,418][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint57.pt
[2025-07-10 22:55:16,779][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint57.pt (epoch 57 @ 171 updates, score None) (writing took 0.7978900299995075 seconds)
[2025-07-10 22:55:16,779][fairseq_cli.train][INFO] - end of epoch 57 (average epoch stats below)
[2025-07-10 22:55:16,780][train][INFO] - {"epoch": 57, "train_loss": "14.356", "train_nll_loss": "0.039", "train_loss_recon": "0.465", "train_loss_info_nce": "9.706", "train_ppl": "1.03", "train_wps": "2104.6", "train_ups": "0.77", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "171", "train_lr": "4.275e-06", "train_gnorm": "5.838", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "238"}
[2025-07-10 22:55:16,814][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:55:16,816][fairseq.trainer][INFO] - begin training epoch 58
[2025-07-10 22:55:16,816][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:55:19,848][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 58 @ 174 updates
[2025-07-10 22:55:19,848][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint58.pt
[2025-07-10 22:55:20,279][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint58.pt
[2025-07-10 22:55:20,687][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint58.pt (epoch 58 @ 174 updates, score None) (writing took 0.8391430900001069 seconds)
[2025-07-10 22:55:20,687][fairseq_cli.train][INFO] - end of epoch 58 (average epoch stats below)
[2025-07-10 22:55:20,689][train][INFO] - {"epoch": 58, "train_loss": "14.281", "train_nll_loss": "0.038", "train_loss_recon": "0.461", "train_loss_info_nce": "9.663", "train_ppl": "1.03", "train_wps": "2094.6", "train_ups": "0.77", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "174", "train_lr": "4.35e-06", "train_gnorm": "5.59", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "242"}
[2025-07-10 22:55:20,725][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:55:20,727][fairseq.trainer][INFO] - begin training epoch 59
[2025-07-10 22:55:20,727][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:55:23,771][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 59 @ 177 updates
[2025-07-10 22:55:23,772][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint59.pt
[2025-07-10 22:55:24,210][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint59.pt
[2025-07-10 22:55:24,618][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint59.pt (epoch 59 @ 177 updates, score None) (writing took 0.8467317659997207 seconds)
[2025-07-10 22:55:24,618][fairseq_cli.train][INFO] - end of epoch 59 (average epoch stats below)
[2025-07-10 22:55:24,619][train][INFO] - {"epoch": 59, "train_loss": "14.15", "train_nll_loss": "0.038", "train_loss_recon": "0.455", "train_loss_info_nce": "9.599", "train_ppl": "1.03", "train_wps": "2082.5", "train_ups": "0.76", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "177", "train_lr": "4.425e-06", "train_gnorm": "5.164", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "246"}
[2025-07-10 22:55:24,655][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:55:24,657][fairseq.trainer][INFO] - begin training epoch 60
[2025-07-10 22:55:24,657][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:55:27,749][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:55:27,985][valid][INFO] - {"epoch": 60, "valid_loss": "12.811", "valid_nll_loss": "0.034", "valid_loss_recon": "0.397", "valid_loss_info_nce": "8.845", "valid_ppl": "1.02", "valid_wps": "68195.6", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "180", "valid_best_loss": "12.811"}
[2025-07-10 22:55:27,986][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 60 @ 180 updates
[2025-07-10 22:55:27,986][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint60.pt
[2025-07-10 22:55:28,421][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint60.pt
[2025-07-10 22:55:29,311][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint60.pt (epoch 60 @ 180 updates, score 12.811) (writing took 1.3244848120002644 seconds)
[2025-07-10 22:55:29,311][fairseq_cli.train][INFO] - end of epoch 60 (average epoch stats below)
[2025-07-10 22:55:29,312][train][INFO] - {"epoch": 60, "train_loss": "14.036", "train_nll_loss": "0.038", "train_loss_recon": "0.449", "train_loss_info_nce": "9.537", "train_ppl": "1.03", "train_wps": "1744.4", "train_ups": "0.64", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "180", "train_lr": "4.5e-06", "train_gnorm": "5.503", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "251"}
[2025-07-10 22:55:29,350][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:55:29,353][fairseq.trainer][INFO] - begin training epoch 61
[2025-07-10 22:55:29,353][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:55:32,424][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 61 @ 183 updates
[2025-07-10 22:55:32,424][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint61.pt
[2025-07-10 22:55:32,863][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint61.pt
[2025-07-10 22:55:33,220][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint61.pt (epoch 61 @ 183 updates, score None) (writing took 0.7960683780002 seconds)
[2025-07-10 22:55:33,220][fairseq_cli.train][INFO] - end of epoch 61 (average epoch stats below)
[2025-07-10 22:55:33,222][train][INFO] - {"epoch": 61, "train_loss": "13.936", "train_nll_loss": "0.037", "train_loss_recon": "0.445", "train_loss_info_nce": "9.485", "train_ppl": "1.03", "train_wps": "2094", "train_ups": "0.77", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "183", "train_lr": "4.575e-06", "train_gnorm": "4.449", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "255"}
[2025-07-10 22:55:33,254][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:55:33,256][fairseq.trainer][INFO] - begin training epoch 62
[2025-07-10 22:55:33,256][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:55:36,331][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 62 @ 186 updates
[2025-07-10 22:55:36,332][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint62.pt
[2025-07-10 22:55:36,765][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint62.pt
[2025-07-10 22:55:37,276][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint62.pt (epoch 62 @ 186 updates, score None) (writing took 0.9444658669999626 seconds)
[2025-07-10 22:55:37,276][fairseq_cli.train][INFO] - end of epoch 62 (average epoch stats below)
[2025-07-10 22:55:37,277][train][INFO] - {"epoch": 62, "train_loss": "13.826", "train_nll_loss": "0.037", "train_loss_recon": "0.44", "train_loss_info_nce": "9.428", "train_ppl": "1.03", "train_wps": "2018.5", "train_ups": "0.74", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "186", "train_lr": "4.65e-06", "train_gnorm": "4.13", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "259"}
[2025-07-10 22:55:37,315][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:55:37,317][fairseq.trainer][INFO] - begin training epoch 63
[2025-07-10 22:55:37,317][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:55:40,337][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 63 @ 189 updates
[2025-07-10 22:55:40,337][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint63.pt
[2025-07-10 22:55:40,777][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint63.pt
[2025-07-10 22:55:41,153][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint63.pt (epoch 63 @ 189 updates, score None) (writing took 0.8161887079995722 seconds)
[2025-07-10 22:55:41,153][fairseq_cli.train][INFO] - end of epoch 63 (average epoch stats below)
[2025-07-10 22:55:41,154][train][INFO] - {"epoch": 63, "train_loss": "13.772", "train_nll_loss": "0.037", "train_loss_recon": "0.437", "train_loss_info_nce": "9.398", "train_ppl": "1.03", "train_wps": "2111.5", "train_ups": "0.77", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "189", "train_lr": "4.725e-06", "train_gnorm": "4.096", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "263"}
[2025-07-10 22:55:41,193][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:55:41,194][fairseq.trainer][INFO] - begin training epoch 64
[2025-07-10 22:55:41,195][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:55:44,240][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 64 @ 192 updates
[2025-07-10 22:55:44,240][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint64.pt
[2025-07-10 22:55:44,674][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint64.pt
[2025-07-10 22:55:45,081][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint64.pt (epoch 64 @ 192 updates, score None) (writing took 0.8408094460000939 seconds)
[2025-07-10 22:55:45,081][fairseq_cli.train][INFO] - end of epoch 64 (average epoch stats below)
[2025-07-10 22:55:45,082][train][INFO] - {"epoch": 64, "train_loss": "13.693", "train_nll_loss": "0.037", "train_loss_recon": "0.434", "train_loss_info_nce": "9.354", "train_ppl": "1.03", "train_wps": "2084", "train_ups": "0.76", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "192", "train_lr": "4.8e-06", "train_gnorm": "3.449", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "266"}
[2025-07-10 22:55:45,122][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:55:45,124][fairseq.trainer][INFO] - begin training epoch 65
[2025-07-10 22:55:45,124][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:55:48,171][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:55:48,413][valid][INFO] - {"epoch": 65, "valid_loss": "12.482", "valid_nll_loss": "0.034", "valid_loss_recon": "0.379", "valid_loss_info_nce": "8.696", "valid_ppl": "1.02", "valid_wps": "67656", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "195", "valid_best_loss": "12.482"}
[2025-07-10 22:55:48,414][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 65 @ 195 updates
[2025-07-10 22:55:48,414][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint65.pt
[2025-07-10 22:55:48,851][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint65.pt
[2025-07-10 22:55:49,782][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint65.pt (epoch 65 @ 195 updates, score 12.482) (writing took 1.3678233689997796 seconds)
[2025-07-10 22:55:49,782][fairseq_cli.train][INFO] - end of epoch 65 (average epoch stats below)
[2025-07-10 22:55:49,783][train][INFO] - {"epoch": 65, "train_loss": "13.587", "train_nll_loss": "0.037", "train_loss_recon": "0.429", "train_loss_info_nce": "9.298", "train_ppl": "1.03", "train_wps": "1741.4", "train_ups": "0.64", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "195", "train_lr": "4.875e-06", "train_gnorm": "5.067", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "271"}
[2025-07-10 22:55:49,824][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:55:49,825][fairseq.trainer][INFO] - begin training epoch 66
[2025-07-10 22:55:49,826][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:55:52,876][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 66 @ 198 updates
[2025-07-10 22:55:52,877][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint66.pt
[2025-07-10 22:55:53,303][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint66.pt
[2025-07-10 22:55:54,017][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint66.pt (epoch 66 @ 198 updates, score None) (writing took 1.1404363459996603 seconds)
[2025-07-10 22:55:54,017][fairseq_cli.train][INFO] - end of epoch 66 (average epoch stats below)
[2025-07-10 22:55:54,018][train][INFO] - {"epoch": 66, "train_loss": "13.528", "train_nll_loss": "0.036", "train_loss_recon": "0.426", "train_loss_info_nce": "9.263", "train_ppl": "1.03", "train_wps": "1933", "train_ups": "0.71", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "198", "train_lr": "4.95e-06", "train_gnorm": "3.602", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "275"}
[2025-07-10 22:55:54,055][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:55:54,057][fairseq.trainer][INFO] - begin training epoch 67
[2025-07-10 22:55:54,057][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:55:56,371][train_inner][INFO] - {"epoch": 67, "update": 66.667, "loss": "15.712", "nll_loss": "0.042", "loss_recon": "0.527", "loss_info_nce": "10.442", "ppl": "1.03", "wps": "1953", "ups": "0.72", "wpb": "2730.5", "bsz": "329.5", "num_updates": "200", "lr": "5e-06", "gnorm": "7.646", "clip": "22", "loss_scale": "128", "train_wall": "81", "gb_free": "10.4", "wall": "278"}
[2025-07-10 22:55:56,371][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:55:56,614][valid][INFO] - {"epoch": 67, "valid_loss": "12.397", "valid_nll_loss": "0.033", "valid_loss_recon": "0.374", "valid_loss_info_nce": "8.659", "valid_ppl": "1.02", "valid_wps": "68213.6", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "200", "valid_best_loss": "12.397"}
[2025-07-10 22:55:56,615][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 67 @ 200 updates
[2025-07-10 22:55:56,615][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint_67_200.pt
[2025-07-10 22:55:57,047][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint_67_200.pt
[2025-07-10 22:55:58,261][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_67_200.pt (epoch 67 @ 200 updates, score 12.397) (writing took 1.6464344650003113 seconds)
[2025-07-10 22:55:59,030][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 67 @ 201 updates
[2025-07-10 22:55:59,031][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint67.pt
[2025-07-10 22:55:59,463][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint67.pt
[2025-07-10 22:55:59,827][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint67.pt (epoch 67 @ 201 updates, score None) (writing took 0.7964303120006662 seconds)
[2025-07-10 22:55:59,827][fairseq_cli.train][INFO] - end of epoch 67 (average epoch stats below)
[2025-07-10 22:55:59,828][train][INFO] - {"epoch": 67, "train_loss": "13.462", "train_nll_loss": "0.036", "train_loss_recon": "0.423", "train_loss_info_nce": "9.231", "train_ppl": "1.03", "train_wps": "1409", "train_ups": "0.52", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "201", "train_lr": "5.025e-06", "train_gnorm": "4.036", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "281"}
[2025-07-10 22:55:59,866][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:55:59,868][fairseq.trainer][INFO] - begin training epoch 68
[2025-07-10 22:55:59,868][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:56:02,912][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 68 @ 204 updates
[2025-07-10 22:56:02,912][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint68.pt
[2025-07-10 22:56:03,340][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint68.pt
[2025-07-10 22:56:03,796][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint68.pt (epoch 68 @ 204 updates, score None) (writing took 0.8840636340000856 seconds)
[2025-07-10 22:56:03,796][fairseq_cli.train][INFO] - end of epoch 68 (average epoch stats below)
[2025-07-10 22:56:03,797][train][INFO] - {"epoch": 68, "train_loss": "13.417", "train_nll_loss": "0.036", "train_loss_recon": "0.421", "train_loss_info_nce": "9.204", "train_ppl": "1.03", "train_wps": "2062.2", "train_ups": "0.76", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "204", "train_lr": "5.1e-06", "train_gnorm": "2.909", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "285"}
[2025-07-10 22:56:03,830][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:56:03,832][fairseq.trainer][INFO] - begin training epoch 69
[2025-07-10 22:56:03,833][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:56:06,882][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 69 @ 207 updates
[2025-07-10 22:56:06,882][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint69.pt
[2025-07-10 22:56:07,328][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint69.pt
[2025-07-10 22:56:07,731][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint69.pt (epoch 69 @ 207 updates, score None) (writing took 0.848941608000132 seconds)
[2025-07-10 22:56:07,731][fairseq_cli.train][INFO] - end of epoch 69 (average epoch stats below)
[2025-07-10 22:56:07,732][train][INFO] - {"epoch": 69, "train_loss": "13.331", "train_nll_loss": "0.036", "train_loss_recon": "0.417", "train_loss_info_nce": "9.163", "train_ppl": "1.03", "train_wps": "2080.3", "train_ups": "0.76", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "207", "train_lr": "5.175e-06", "train_gnorm": "2.907", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "289"}
[2025-07-10 22:56:07,765][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:56:07,767][fairseq.trainer][INFO] - begin training epoch 70
[2025-07-10 22:56:07,767][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:56:10,823][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:56:11,064][valid][INFO] - {"epoch": 70, "valid_loss": "12.26", "valid_nll_loss": "0.033", "valid_loss_recon": "0.37", "valid_loss_info_nce": "8.561", "valid_ppl": "1.02", "valid_wps": "67942.3", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "210", "valid_best_loss": "12.26"}
[2025-07-10 22:56:11,064][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 70 @ 210 updates
[2025-07-10 22:56:11,065][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint70.pt
[2025-07-10 22:56:11,497][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint70.pt
[2025-07-10 22:56:12,538][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint70.pt (epoch 70 @ 210 updates, score 12.26) (writing took 1.473997690000033 seconds)
[2025-07-10 22:56:12,539][fairseq_cli.train][INFO] - end of epoch 70 (average epoch stats below)
[2025-07-10 22:56:12,540][train][INFO] - {"epoch": 70, "train_loss": "13.287", "train_nll_loss": "0.036", "train_loss_recon": "0.414", "train_loss_info_nce": "9.141", "train_ppl": "1.03", "train_wps": "1702.7", "train_ups": "0.62", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "210", "train_lr": "5.25e-06", "train_gnorm": "3.332", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "294"}
[2025-07-10 22:56:12,574][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:56:12,575][fairseq.trainer][INFO] - begin training epoch 71
[2025-07-10 22:56:12,575][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:56:15,457][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 71 @ 213 updates
[2025-07-10 22:56:15,457][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint71.pt
[2025-07-10 22:56:15,893][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint71.pt
[2025-07-10 22:56:16,810][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint71.pt (epoch 71 @ 213 updates, score None) (writing took 1.352842917000089 seconds)
[2025-07-10 22:56:16,810][fairseq_cli.train][INFO] - end of epoch 71 (average epoch stats below)
[2025-07-10 22:56:16,811][train][INFO] - {"epoch": 71, "train_loss": "13.263", "train_nll_loss": "0.036", "train_loss_recon": "0.414", "train_loss_info_nce": "9.121", "train_ppl": "1.03", "train_wps": "1916.6", "train_ups": "0.7", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "213", "train_lr": "5.325e-06", "train_gnorm": "5.286", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "298"}
[2025-07-10 22:56:16,844][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:56:16,846][fairseq.trainer][INFO] - begin training epoch 72
[2025-07-10 22:56:16,846][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:56:19,870][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 72 @ 216 updates
[2025-07-10 22:56:19,870][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint72.pt
[2025-07-10 22:56:20,307][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint72.pt
[2025-07-10 22:56:21,121][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint72.pt (epoch 72 @ 216 updates, score None) (writing took 1.251106271000026 seconds)
[2025-07-10 22:56:21,121][fairseq_cli.train][INFO] - end of epoch 72 (average epoch stats below)
[2025-07-10 22:56:21,122][train][INFO] - {"epoch": 72, "train_loss": "13.189", "train_nll_loss": "0.035", "train_loss_recon": "0.41", "train_loss_info_nce": "9.086", "train_ppl": "1.02", "train_wps": "1898.9", "train_ups": "0.7", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "216", "train_lr": "5.4e-06", "train_gnorm": "4.967", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "302"}
[2025-07-10 22:56:21,156][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:56:21,158][fairseq.trainer][INFO] - begin training epoch 73
[2025-07-10 22:56:21,158][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:56:24,184][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 73 @ 219 updates
[2025-07-10 22:56:24,184][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint73.pt
[2025-07-10 22:56:24,617][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint73.pt
[2025-07-10 22:56:24,985][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint73.pt (epoch 73 @ 219 updates, score None) (writing took 0.8017127309994976 seconds)
[2025-07-10 22:56:24,986][fairseq_cli.train][INFO] - end of epoch 73 (average epoch stats below)
[2025-07-10 22:56:24,987][train][INFO] - {"epoch": 73, "train_loss": "13.159", "train_nll_loss": "0.035", "train_loss_recon": "0.409", "train_loss_info_nce": "9.066", "train_ppl": "1.02", "train_wps": "2118.3", "train_ups": "0.78", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "219", "train_lr": "5.475e-06", "train_gnorm": "4.694", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "306"}
[2025-07-10 22:56:25,020][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:56:25,021][fairseq.trainer][INFO] - begin training epoch 74
[2025-07-10 22:56:25,022][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:56:28,066][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 74 @ 222 updates
[2025-07-10 22:56:28,067][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint74.pt
[2025-07-10 22:56:28,509][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint74.pt
[2025-07-10 22:56:28,909][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint74.pt (epoch 74 @ 222 updates, score None) (writing took 0.8425413569993907 seconds)
[2025-07-10 22:56:28,909][fairseq_cli.train][INFO] - end of epoch 74 (average epoch stats below)
[2025-07-10 22:56:28,910][train][INFO] - {"epoch": 74, "train_loss": "13.115", "train_nll_loss": "0.035", "train_loss_recon": "0.407", "train_loss_info_nce": "9.044", "train_ppl": "1.02", "train_wps": "2086.6", "train_ups": "0.76", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "222", "train_lr": "5.55e-06", "train_gnorm": "4.096", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "310"}
[2025-07-10 22:56:28,944][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:56:28,945][fairseq.trainer][INFO] - begin training epoch 75
[2025-07-10 22:56:28,946][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:56:32,014][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:56:32,250][valid][INFO] - {"epoch": 75, "valid_loss": "12.097", "valid_nll_loss": "0.033", "valid_loss_recon": "0.359", "valid_loss_info_nce": "8.503", "valid_ppl": "1.02", "valid_wps": "68052.1", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "225", "valid_best_loss": "12.097"}
[2025-07-10 22:56:32,251][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 75 @ 225 updates
[2025-07-10 22:56:32,251][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint75.pt
[2025-07-10 22:56:32,688][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint75.pt
[2025-07-10 22:56:33,479][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint75.pt (epoch 75 @ 225 updates, score 12.097) (writing took 1.228122020000228 seconds)
[2025-07-10 22:56:33,479][fairseq_cli.train][INFO] - end of epoch 75 (average epoch stats below)
[2025-07-10 22:56:33,480][train][INFO] - {"epoch": 75, "train_loss": "13.059", "train_nll_loss": "0.035", "train_loss_recon": "0.404", "train_loss_info_nce": "9.016", "train_ppl": "1.02", "train_wps": "1791.3", "train_ups": "0.66", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "225", "train_lr": "5.625e-06", "train_gnorm": "2.287", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "315"}
[2025-07-10 22:56:33,513][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:56:33,515][fairseq.trainer][INFO] - begin training epoch 76
[2025-07-10 22:56:33,515][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:56:36,597][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 76 @ 228 updates
[2025-07-10 22:56:36,597][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint76.pt
[2025-07-10 22:56:37,032][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint76.pt
[2025-07-10 22:56:37,892][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint76.pt (epoch 76 @ 228 updates, score None) (writing took 1.2953402019993518 seconds)
[2025-07-10 22:56:37,893][fairseq_cli.train][INFO] - end of epoch 76 (average epoch stats below)
[2025-07-10 22:56:37,894][train][INFO] - {"epoch": 76, "train_loss": "13.024", "train_nll_loss": "0.035", "train_loss_recon": "0.403", "train_loss_info_nce": "8.996", "train_ppl": "1.02", "train_wps": "1854.7", "train_ups": "0.68", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "228", "train_lr": "5.7e-06", "train_gnorm": "1.933", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "319"}
[2025-07-10 22:56:37,934][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:56:37,937][fairseq.trainer][INFO] - begin training epoch 77
[2025-07-10 22:56:37,937][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:56:41,055][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 77 @ 231 updates
[2025-07-10 22:56:41,055][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint77.pt
[2025-07-10 22:56:41,493][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint77.pt
[2025-07-10 22:56:42,306][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint77.pt (epoch 77 @ 231 updates, score None) (writing took 1.2515019100001155 seconds)
[2025-07-10 22:56:42,307][fairseq_cli.train][INFO] - end of epoch 77 (average epoch stats below)
[2025-07-10 22:56:42,308][train][INFO] - {"epoch": 77, "train_loss": "12.989", "train_nll_loss": "0.035", "train_loss_recon": "0.401", "train_loss_info_nce": "8.975", "train_ppl": "1.02", "train_wps": "1854.5", "train_ups": "0.68", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "231", "train_lr": "5.775e-06", "train_gnorm": "2.334", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "324"}
[2025-07-10 22:56:42,347][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:56:42,349][fairseq.trainer][INFO] - begin training epoch 78
[2025-07-10 22:56:42,349][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:56:45,414][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 78 @ 234 updates
[2025-07-10 22:56:45,415][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint78.pt
[2025-07-10 22:56:45,853][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint78.pt
[2025-07-10 22:56:46,446][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint78.pt (epoch 78 @ 234 updates, score None) (writing took 1.0317622719994688 seconds)
[2025-07-10 22:56:46,446][fairseq_cli.train][INFO] - end of epoch 78 (average epoch stats below)
[2025-07-10 22:56:46,447][train][INFO] - {"epoch": 78, "train_loss": "12.958", "train_nll_loss": "0.035", "train_loss_recon": "0.4", "train_loss_info_nce": "8.961", "train_ppl": "1.02", "train_wps": "1977.5", "train_ups": "0.72", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "234", "train_lr": "5.85e-06", "train_gnorm": "2.775", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "328"}
[2025-07-10 22:56:46,482][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:56:46,484][fairseq.trainer][INFO] - begin training epoch 79
[2025-07-10 22:56:46,484][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:56:49,515][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 79 @ 237 updates
[2025-07-10 22:56:49,515][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint79.pt
[2025-07-10 22:56:49,949][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint79.pt
[2025-07-10 22:56:50,313][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint79.pt (epoch 79 @ 237 updates, score None) (writing took 0.7979420219999156 seconds)
[2025-07-10 22:56:50,313][fairseq_cli.train][INFO] - end of epoch 79 (average epoch stats below)
[2025-07-10 22:56:50,314][train][INFO] - {"epoch": 79, "train_loss": "12.933", "train_nll_loss": "0.035", "train_loss_recon": "0.399", "train_loss_info_nce": "8.943", "train_ppl": "1.02", "train_wps": "2117.1", "train_ups": "0.78", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "237", "train_lr": "5.925e-06", "train_gnorm": "2.82", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "332"}
[2025-07-10 22:56:50,346][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:56:50,347][fairseq.trainer][INFO] - begin training epoch 80
[2025-07-10 22:56:50,348][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:56:53,381][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:56:53,626][valid][INFO] - {"epoch": 80, "valid_loss": "11.99", "valid_nll_loss": "0.032", "valid_loss_recon": "0.354", "valid_loss_info_nce": "8.453", "valid_ppl": "1.02", "valid_wps": "68508.4", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "240", "valid_best_loss": "11.99"}
[2025-07-10 22:56:53,627][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 80 @ 240 updates
[2025-07-10 22:56:53,627][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint80.pt
[2025-07-10 22:56:54,063][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint80.pt
[2025-07-10 22:56:54,818][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint80.pt (epoch 80 @ 240 updates, score 11.99) (writing took 1.191327839999758 seconds)
[2025-07-10 22:56:54,818][fairseq_cli.train][INFO] - end of epoch 80 (average epoch stats below)
[2025-07-10 22:56:54,819][train][INFO] - {"epoch": 80, "train_loss": "12.897", "train_nll_loss": "0.035", "train_loss_recon": "0.397", "train_loss_info_nce": "8.93", "train_ppl": "1.02", "train_wps": "1817", "train_ups": "0.67", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "240", "train_lr": "6e-06", "train_gnorm": "3.237", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "336"}
[2025-07-10 22:56:54,852][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:56:54,854][fairseq.trainer][INFO] - begin training epoch 81
[2025-07-10 22:56:54,854][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:56:57,872][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 81 @ 243 updates
[2025-07-10 22:56:57,872][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint81.pt
[2025-07-10 22:56:58,304][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint81.pt
[2025-07-10 22:56:58,700][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint81.pt (epoch 81 @ 243 updates, score None) (writing took 0.8280698090002261 seconds)
[2025-07-10 22:56:58,700][fairseq_cli.train][INFO] - end of epoch 81 (average epoch stats below)
[2025-07-10 22:56:58,701][train][INFO] - {"epoch": 81, "train_loss": "12.876", "train_nll_loss": "0.035", "train_loss_recon": "0.396", "train_loss_info_nce": "8.913", "train_ppl": "1.02", "train_wps": "2108.8", "train_ups": "0.77", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "243", "train_lr": "6.075e-06", "train_gnorm": "2.102", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "340"}
[2025-07-10 22:56:58,735][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:56:58,737][fairseq.trainer][INFO] - begin training epoch 82
[2025-07-10 22:56:58,737][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:57:01,792][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 82 @ 246 updates
[2025-07-10 22:57:01,792][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint82.pt
[2025-07-10 22:57:02,218][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint82.pt
[2025-07-10 22:57:03,090][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint82.pt (epoch 82 @ 246 updates, score None) (writing took 1.2976934090002032 seconds)
[2025-07-10 22:57:03,090][fairseq_cli.train][INFO] - end of epoch 82 (average epoch stats below)
[2025-07-10 22:57:03,091][train][INFO] - {"epoch": 82, "train_loss": "12.86", "train_nll_loss": "0.035", "train_loss_recon": "0.396", "train_loss_info_nce": "8.902", "train_ppl": "1.02", "train_wps": "1864.9", "train_ups": "0.68", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "246", "train_lr": "6.15e-06", "train_gnorm": "3.172", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "344"}
[2025-07-10 22:57:03,124][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:57:03,126][fairseq.trainer][INFO] - begin training epoch 83
[2025-07-10 22:57:03,126][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:57:06,240][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 83 @ 249 updates
[2025-07-10 22:57:06,241][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint83.pt
[2025-07-10 22:57:06,677][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint83.pt
[2025-07-10 22:57:07,042][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint83.pt (epoch 83 @ 249 updates, score None) (writing took 0.8022049189994505 seconds)
[2025-07-10 22:57:07,043][fairseq_cli.train][INFO] - end of epoch 83 (average epoch stats below)
[2025-07-10 22:57:07,044][train][INFO] - {"epoch": 83, "train_loss": "12.82", "train_nll_loss": "0.034", "train_loss_recon": "0.394", "train_loss_info_nce": "8.884", "train_ppl": "1.02", "train_wps": "2070.9", "train_ups": "0.76", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "249", "train_lr": "6.225e-06", "train_gnorm": "5.311", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "348"}
[2025-07-10 22:57:07,076][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:57:07,078][fairseq.trainer][INFO] - begin training epoch 84
[2025-07-10 22:57:07,078][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:57:10,142][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 84 @ 252 updates
[2025-07-10 22:57:10,142][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint84.pt
[2025-07-10 22:57:10,575][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint84.pt
[2025-07-10 22:57:11,595][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint84.pt (epoch 84 @ 252 updates, score None) (writing took 1.453392573000201 seconds)
[2025-07-10 22:57:11,595][fairseq_cli.train][INFO] - end of epoch 84 (average epoch stats below)
[2025-07-10 22:57:11,597][train][INFO] - {"epoch": 84, "train_loss": "12.808", "train_nll_loss": "0.034", "train_loss_recon": "0.393", "train_loss_info_nce": "8.874", "train_ppl": "1.02", "train_wps": "1798", "train_ups": "0.66", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "252", "train_lr": "6.3e-06", "train_gnorm": "5.099", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "353"}
[2025-07-10 22:57:11,629][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:57:11,631][fairseq.trainer][INFO] - begin training epoch 85
[2025-07-10 22:57:11,631][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:57:14,683][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:57:14,924][valid][INFO] - {"epoch": 85, "valid_loss": "11.87", "valid_nll_loss": "0.032", "valid_loss_recon": "0.345", "valid_loss_info_nce": "8.416", "valid_ppl": "1.02", "valid_wps": "59925.2", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "255", "valid_best_loss": "11.87"}
[2025-07-10 22:57:14,925][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 85 @ 255 updates
[2025-07-10 22:57:14,925][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint85.pt
[2025-07-10 22:57:15,367][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint85.pt
[2025-07-10 22:57:16,105][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint85.pt (epoch 85 @ 255 updates, score 11.87) (writing took 1.1804766029999882 seconds)
[2025-07-10 22:57:16,106][fairseq_cli.train][INFO] - end of epoch 85 (average epoch stats below)
[2025-07-10 22:57:16,106][train][INFO] - {"epoch": 85, "train_loss": "12.781", "train_nll_loss": "0.034", "train_loss_recon": "0.392", "train_loss_info_nce": "8.86", "train_ppl": "1.02", "train_wps": "1815", "train_ups": "0.67", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "255", "train_lr": "6.375e-06", "train_gnorm": "3.346", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "357"}
[2025-07-10 22:57:16,140][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:57:16,142][fairseq.trainer][INFO] - begin training epoch 86
[2025-07-10 22:57:16,142][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:57:19,213][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 86 @ 258 updates
[2025-07-10 22:57:19,213][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint86.pt
[2025-07-10 22:57:19,662][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint86.pt
[2025-07-10 22:57:20,031][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint86.pt (epoch 86 @ 258 updates, score None) (writing took 0.8181812780003384 seconds)
[2025-07-10 22:57:20,031][fairseq_cli.train][INFO] - end of epoch 86 (average epoch stats below)
[2025-07-10 22:57:20,032][train][INFO] - {"epoch": 86, "train_loss": "12.759", "train_nll_loss": "0.034", "train_loss_recon": "0.391", "train_loss_info_nce": "8.85", "train_ppl": "1.02", "train_wps": "2085.1", "train_ups": "0.76", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "258", "train_lr": "6.45e-06", "train_gnorm": "3.598", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "361"}
[2025-07-10 22:57:20,066][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:57:20,068][fairseq.trainer][INFO] - begin training epoch 87
[2025-07-10 22:57:20,068][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:57:23,109][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 87 @ 261 updates
[2025-07-10 22:57:23,109][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint87.pt
[2025-07-10 22:57:23,548][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint87.pt
[2025-07-10 22:57:23,955][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint87.pt (epoch 87 @ 261 updates, score None) (writing took 0.8462561110000024 seconds)
[2025-07-10 22:57:23,955][fairseq_cli.train][INFO] - end of epoch 87 (average epoch stats below)
[2025-07-10 22:57:23,957][train][INFO] - {"epoch": 87, "train_loss": "12.739", "train_nll_loss": "0.034", "train_loss_recon": "0.39", "train_loss_info_nce": "8.842", "train_ppl": "1.02", "train_wps": "2086.2", "train_ups": "0.76", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "261", "train_lr": "6.525e-06", "train_gnorm": "4.195", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "365"}
[2025-07-10 22:57:23,991][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:57:23,993][fairseq.trainer][INFO] - begin training epoch 88
[2025-07-10 22:57:23,993][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:57:27,037][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 88 @ 264 updates
[2025-07-10 22:57:27,037][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint88.pt
[2025-07-10 22:57:27,471][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint88.pt
[2025-07-10 22:57:27,881][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint88.pt (epoch 88 @ 264 updates, score None) (writing took 0.8439085049994901 seconds)
[2025-07-10 22:57:27,881][fairseq_cli.train][INFO] - end of epoch 88 (average epoch stats below)
[2025-07-10 22:57:27,882][train][INFO] - {"epoch": 88, "train_loss": "12.723", "train_nll_loss": "0.034", "train_loss_recon": "0.39", "train_loss_info_nce": "8.823", "train_ppl": "1.02", "train_wps": "2085.3", "train_ups": "0.76", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "264", "train_lr": "6.6e-06", "train_gnorm": "3.609", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "369"}
[2025-07-10 22:57:27,923][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:57:27,925][fairseq.trainer][INFO] - begin training epoch 89
[2025-07-10 22:57:27,925][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:57:30,878][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 89 @ 267 updates
[2025-07-10 22:57:30,879][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint89.pt
[2025-07-10 22:57:31,314][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint89.pt
[2025-07-10 22:57:31,804][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint89.pt (epoch 89 @ 267 updates, score None) (writing took 0.9263157100003809 seconds)
[2025-07-10 22:57:31,805][fairseq_cli.train][INFO] - end of epoch 89 (average epoch stats below)
[2025-07-10 22:57:31,806][train][INFO] - {"epoch": 89, "train_loss": "12.702", "train_nll_loss": "0.034", "train_loss_recon": "0.389", "train_loss_info_nce": "8.816", "train_ppl": "1.02", "train_wps": "2086.4", "train_ups": "0.76", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "267", "train_lr": "6.675e-06", "train_gnorm": "4.39", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "373"}
[2025-07-10 22:57:31,841][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:57:31,843][fairseq.trainer][INFO] - begin training epoch 90
[2025-07-10 22:57:31,843][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:57:34,882][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:57:35,120][valid][INFO] - {"epoch": 90, "valid_loss": "11.819", "valid_nll_loss": "0.032", "valid_loss_recon": "0.343", "valid_loss_info_nce": "8.394", "valid_ppl": "1.02", "valid_wps": "68652.5", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "270", "valid_best_loss": "11.819"}
[2025-07-10 22:57:35,120][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 90 @ 270 updates
[2025-07-10 22:57:35,121][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint90.pt
[2025-07-10 22:57:35,554][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint90.pt
[2025-07-10 22:57:36,858][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint90.pt (epoch 90 @ 270 updates, score 11.819) (writing took 1.7378924679996999 seconds)
[2025-07-10 22:57:36,859][fairseq_cli.train][INFO] - end of epoch 90 (average epoch stats below)
[2025-07-10 22:57:36,860][train][INFO] - {"epoch": 90, "train_loss": "12.683", "train_nll_loss": "0.034", "train_loss_recon": "0.388", "train_loss_info_nce": "8.802", "train_ppl": "1.02", "train_wps": "1619.7", "train_ups": "0.59", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "270", "train_lr": "6.75e-06", "train_gnorm": "4.221", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "378"}
[2025-07-10 22:57:36,897][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:57:36,899][fairseq.trainer][INFO] - begin training epoch 91
[2025-07-10 22:57:36,899][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:57:40,000][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 91 @ 273 updates
[2025-07-10 22:57:40,001][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint91.pt
[2025-07-10 22:57:40,432][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint91.pt
[2025-07-10 22:57:41,388][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint91.pt (epoch 91 @ 273 updates, score None) (writing took 1.3882789790004608 seconds)
[2025-07-10 22:57:41,389][fairseq_cli.train][INFO] - end of epoch 91 (average epoch stats below)
[2025-07-10 22:57:41,390][train][INFO] - {"epoch": 91, "train_loss": "12.677", "train_nll_loss": "0.034", "train_loss_recon": "0.388", "train_loss_info_nce": "8.797", "train_ppl": "1.02", "train_wps": "1807.1", "train_ups": "0.66", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "273", "train_lr": "6.825e-06", "train_gnorm": "3.49", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "383"}
[2025-07-10 22:57:41,428][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:57:41,430][fairseq.trainer][INFO] - begin training epoch 92
[2025-07-10 22:57:41,430][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:57:44,503][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 92 @ 276 updates
[2025-07-10 22:57:44,504][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint92.pt
[2025-07-10 22:57:44,938][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint92.pt
[2025-07-10 22:57:45,322][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint92.pt (epoch 92 @ 276 updates, score None) (writing took 0.8185447330006355 seconds)
[2025-07-10 22:57:45,322][fairseq_cli.train][INFO] - end of epoch 92 (average epoch stats below)
[2025-07-10 22:57:45,323][train][INFO] - {"epoch": 92, "train_loss": "12.655", "train_nll_loss": "0.034", "train_loss_recon": "0.387", "train_loss_info_nce": "8.783", "train_ppl": "1.02", "train_wps": "2081.1", "train_ups": "0.76", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "276", "train_lr": "6.9e-06", "train_gnorm": "2.415", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "387"}
[2025-07-10 22:57:45,356][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:57:45,358][fairseq.trainer][INFO] - begin training epoch 93
[2025-07-10 22:57:45,358][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:57:48,353][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 93 @ 279 updates
[2025-07-10 22:57:48,354][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint93.pt
[2025-07-10 22:57:48,790][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint93.pt
[2025-07-10 22:57:49,215][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint93.pt (epoch 93 @ 279 updates, score None) (writing took 0.861561974000324 seconds)
[2025-07-10 22:57:49,215][fairseq_cli.train][INFO] - end of epoch 93 (average epoch stats below)
[2025-07-10 22:57:49,216][train][INFO] - {"epoch": 93, "train_loss": "12.65", "train_nll_loss": "0.034", "train_loss_recon": "0.387", "train_loss_info_nce": "8.778", "train_ppl": "1.02", "train_wps": "2102.7", "train_ups": "0.77", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "279", "train_lr": "6.975e-06", "train_gnorm": "2.212", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "391"}
[2025-07-10 22:57:49,249][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:57:49,251][fairseq.trainer][INFO] - begin training epoch 94
[2025-07-10 22:57:49,251][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:57:52,291][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 94 @ 282 updates
[2025-07-10 22:57:52,291][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint94.pt
[2025-07-10 22:57:52,727][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint94.pt
[2025-07-10 22:57:53,129][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint94.pt (epoch 94 @ 282 updates, score None) (writing took 0.8381580610002857 seconds)
[2025-07-10 22:57:53,129][fairseq_cli.train][INFO] - end of epoch 94 (average epoch stats below)
[2025-07-10 22:57:53,130][train][INFO] - {"epoch": 94, "train_loss": "12.623", "train_nll_loss": "0.034", "train_loss_recon": "0.385", "train_loss_info_nce": "8.769", "train_ppl": "1.02", "train_wps": "2091.5", "train_ups": "0.77", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "282", "train_lr": "7.05e-06", "train_gnorm": "4.653", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "394"}
[2025-07-10 22:57:53,163][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:57:53,166][fairseq.trainer][INFO] - begin training epoch 95
[2025-07-10 22:57:53,166][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:57:56,241][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:57:56,479][valid][INFO] - {"epoch": 95, "valid_loss": "11.837", "valid_nll_loss": "0.032", "valid_loss_recon": "0.347", "valid_loss_info_nce": "8.365", "valid_ppl": "1.02", "valid_wps": "68174.3", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "285", "valid_best_loss": "11.819"}
[2025-07-10 22:57:56,480][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 95 @ 285 updates
[2025-07-10 22:57:56,480][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint95.pt
[2025-07-10 22:57:56,919][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint95.pt
[2025-07-10 22:57:57,203][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint95.pt (epoch 95 @ 285 updates, score 11.837) (writing took 0.7233336959998269 seconds)
[2025-07-10 22:57:57,203][fairseq_cli.train][INFO] - end of epoch 95 (average epoch stats below)
[2025-07-10 22:57:57,204][train][INFO] - {"epoch": 95, "train_loss": "12.612", "train_nll_loss": "0.034", "train_loss_recon": "0.385", "train_loss_info_nce": "8.763", "train_ppl": "1.02", "train_wps": "2009.1", "train_ups": "0.74", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "285", "train_lr": "7.125e-06", "train_gnorm": "7.322", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "399"}
[2025-07-10 22:57:57,241][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:57:57,244][fairseq.trainer][INFO] - begin training epoch 96
[2025-07-10 22:57:57,244][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:58:00,342][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 96 @ 288 updates
[2025-07-10 22:58:00,343][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint96.pt
[2025-07-10 22:58:00,789][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint96.pt
[2025-07-10 22:58:01,175][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint96.pt (epoch 96 @ 288 updates, score None) (writing took 0.8326270170000498 seconds)
[2025-07-10 22:58:01,175][fairseq_cli.train][INFO] - end of epoch 96 (average epoch stats below)
[2025-07-10 22:58:01,176][train][INFO] - {"epoch": 96, "train_loss": "12.587", "train_nll_loss": "0.034", "train_loss_recon": "0.384", "train_loss_info_nce": "8.746", "train_ppl": "1.02", "train_wps": "2061.1", "train_ups": "0.76", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "288", "train_lr": "7.2e-06", "train_gnorm": "5.582", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "403"}
[2025-07-10 22:58:01,210][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:58:01,212][fairseq.trainer][INFO] - begin training epoch 97
[2025-07-10 22:58:01,212][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:58:04,244][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 97 @ 291 updates
[2025-07-10 22:58:04,244][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint97.pt
[2025-07-10 22:58:04,682][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint97.pt
[2025-07-10 22:58:05,221][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint97.pt (epoch 97 @ 291 updates, score None) (writing took 0.977324172000408 seconds)
[2025-07-10 22:58:05,222][fairseq_cli.train][INFO] - end of epoch 97 (average epoch stats below)
[2025-07-10 22:58:05,223][train][INFO] - {"epoch": 97, "train_loss": "12.585", "train_nll_loss": "0.034", "train_loss_recon": "0.385", "train_loss_info_nce": "8.739", "train_ppl": "1.02", "train_wps": "2022.9", "train_ups": "0.74", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "291", "train_lr": "7.275e-06", "train_gnorm": "2.166", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "407"}
[2025-07-10 22:58:05,255][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:58:05,257][fairseq.trainer][INFO] - begin training epoch 98
[2025-07-10 22:58:05,257][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:58:08,314][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 98 @ 294 updates
[2025-07-10 22:58:08,315][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint98.pt
[2025-07-10 22:58:08,746][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint98.pt
[2025-07-10 22:58:09,120][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint98.pt (epoch 98 @ 294 updates, score None) (writing took 0.8057370100004846 seconds)
[2025-07-10 22:58:09,120][fairseq_cli.train][INFO] - end of epoch 98 (average epoch stats below)
[2025-07-10 22:58:09,121][train][INFO] - {"epoch": 98, "train_loss": "12.564", "train_nll_loss": "0.034", "train_loss_recon": "0.383", "train_loss_info_nce": "8.73", "train_ppl": "1.02", "train_wps": "2099.7", "train_ups": "0.77", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "294", "train_lr": "7.35e-06", "train_gnorm": "3.002", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "410"}
[2025-07-10 22:58:09,155][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:58:09,156][fairseq.trainer][INFO] - begin training epoch 99
[2025-07-10 22:58:09,156][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:58:12,180][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 99 @ 297 updates
[2025-07-10 22:58:12,181][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint99.pt
[2025-07-10 22:58:12,619][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint99.pt
[2025-07-10 22:58:13,039][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint99.pt (epoch 99 @ 297 updates, score None) (writing took 0.8585141370003839 seconds)
[2025-07-10 22:58:13,039][fairseq_cli.train][INFO] - end of epoch 99 (average epoch stats below)
[2025-07-10 22:58:13,040][train][INFO] - {"epoch": 99, "train_loss": "12.566", "train_nll_loss": "0.034", "train_loss_recon": "0.384", "train_loss_info_nce": "8.731", "train_ppl": "1.02", "train_wps": "2088.9", "train_ups": "0.77", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "297", "train_lr": "7.425e-06", "train_gnorm": "6.052", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "414"}
[2025-07-10 22:58:13,075][fairseq.data.iterators][INFO] - grouped total_num_itrs = 3
[2025-07-10 22:58:13,077][fairseq.trainer][INFO] - begin training epoch 100
[2025-07-10 22:58:13,077][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:58:16,117][train_inner][INFO] - {"epoch": 100, "update": 100.0, "loss": "12.866", "nll_loss": "0.035", "loss_recon": "0.396", "loss_info_nce": "8.905", "ppl": "1.02", "wps": "1948.6", "ups": "0.72", "wpb": "2723", "bsz": "329.2", "num_updates": "300", "lr": "7.5e-06", "gnorm": "3.829", "clip": "1", "loss_scale": "128", "train_wall": "81", "gb_free": "10.4", "wall": "417"}
[2025-07-10 22:58:16,117][fairseq_cli.train][INFO] - Stopping training due to num_updates: 300 >= max_update: 300
[2025-07-10 22:58:16,118][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:58:16,361][valid][INFO] - {"epoch": 100, "valid_loss": "11.711", "valid_nll_loss": "0.031", "valid_loss_recon": "0.338", "valid_loss_info_nce": "8.336", "valid_ppl": "1.02", "valid_wps": "67581.9", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "300", "valid_best_loss": "11.711"}
[2025-07-10 22:58:16,361][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 100 @ 300 updates
[2025-07-10 22:58:16,362][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint100.pt
[2025-07-10 22:58:16,800][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_15_4enc_2dec_base/checkpoints/checkpoint100.pt
[2025-07-10 22:58:17,832][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint100.pt (epoch 100 @ 300 updates, score 11.711) (writing took 1.4707775950000723 seconds)
[2025-07-10 22:58:17,832][fairseq_cli.train][INFO] - end of epoch 100 (average epoch stats below)
[2025-07-10 22:58:17,833][train][INFO] - {"epoch": 100, "train_loss": "12.542", "train_nll_loss": "0.034", "train_loss_recon": "0.383", "train_loss_info_nce": "8.715", "train_ppl": "1.02", "train_wps": "1707.8", "train_ups": "0.63", "train_wpb": "2728", "train_bsz": "329.7", "train_num_updates": "300", "train_lr": "7.5e-06", "train_gnorm": "6.761", "train_clip": "33.3", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "10.4", "train_wall": "419"}
[2025-07-10 22:58:17,834][fairseq_cli.train][INFO] - done training in 418.8 seconds
