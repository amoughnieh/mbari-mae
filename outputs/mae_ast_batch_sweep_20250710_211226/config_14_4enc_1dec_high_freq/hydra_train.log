[2025-07-10 22:41:14,556][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'tblog', 'wandb_project': None, 'azureml_logging': False, 'seed': 1337, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/home/incantator/Documents/mbari-mae/mae_ast', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 6, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 1048576, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 5, 'validate_interval_updates': 100, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1048576, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 300, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [16], 'lr': [0.0001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 100, 'keep_interval_updates': 1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'mae_ast', 'ast_kernel_size_chan': 16, 'ast_kernel_size_time': 16, 'ast_kernel_stride_chan': 16, 'ast_kernel_stride_time': 16, 'encoder_layers': 4, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_norm_first': False, 'feature_grad_mult': 0.1, 'use_post_enc_proj': False, 'decoder_embed_dim': 768, 'decoder_layers': 1, 'decoder_layerdrop': 0.0, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'random_mask_prob': 0.75, 'mask_length': 10, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'checkpoint_activations': False, 'max_token_length': 48000, 'enc_sine_pos': True, 'enc_conv_pos': False, 'dec_sine_pos': True, 'dec_conv_pos': False}, 'task': {'_name': 'mae_ast_pretraining', 'data': '/home/incantator/Documents/mbari-mae/data/audio_chunks-MARS-20171030T000000Z-10secs', 'sample_rate': 2000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 40000, 'min_sample_size': 5000, 'random_crop': True, 'pad_audio': False, 'feature_type': 'fbank', 'feature_rate': 100, 'feature_dim': 128, 'deltas': False, 'mask_spans': False, 'mask_type': random_mask}, 'criterion': {'_name': 'mae_ast', 'reconstruction_weight': 10.0, 'classification_weight': 1.0}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0001]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 4000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 300.0, 'lr': [0.0001]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2025-07-10 22:41:14,557][mae_ast.tasks.mae_ast_pretraining][INFO] - current directory is /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq
[2025-07-10 22:41:14,558][mae_ast.tasks.mae_ast_pretraining][INFO] - MAEPretrainingTask Config {'_name': 'mae_ast_pretraining', 'data': '/home/incantator/Documents/mbari-mae/data/audio_chunks-MARS-20171030T000000Z-10secs', 'sample_rate': 2000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 40000, 'min_sample_size': 5000, 'random_crop': True, 'pad_audio': False, 'feature_type': 'fbank', 'feature_rate': 100, 'feature_dim': 128, 'deltas': False, 'mask_spans': False, 'mask_type': random_mask}
[2025-07-10 22:41:14,560][mae_ast.models.mae_ast][INFO] - MAEModel Config: {'_name': 'mae_ast', 'ast_kernel_size_chan': 16, 'ast_kernel_size_time': 16, 'ast_kernel_stride_chan': 16, 'ast_kernel_stride_time': 16, 'encoder_layers': 4, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_norm_first': False, 'feature_grad_mult': 0.1, 'use_post_enc_proj': False, 'decoder_embed_dim': 768, 'decoder_layers': 1, 'decoder_layerdrop': 0.0, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'random_mask_prob': 0.75, 'mask_length': 10, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'checkpoint_activations': False, 'max_token_length': 48000, 'enc_sine_pos': True, 'enc_conv_pos': False, 'dec_sine_pos': True, 'dec_conv_pos': False}
[2025-07-10 22:41:14,861][fairseq_cli.train][INFO] - MAE_AST(
  (feature_extractor): Identity()
  (post_extract_proj): Linear(in_features=256, out_features=768, bias=True)
  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (batch_norm): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  (unfold): Unfold(kernel_size=(16, 16), dilation=1, padding=0, stride=(16, 16))
  (dropout_input): Dropout(p=0.1, inplace=False)
  (enc_sine_pos_embed): SinusoidalPositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-3): 4 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (dec_sine_pos_embed): SinusoidalPositionalEncoding()
  (decoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (final_proj_reconstruction): Linear(in_features=768, out_features=256, bias=True)
  (final_proj_classification): Linear(in_features=768, out_features=256, bias=True)
)
[2025-07-10 22:41:14,862][fairseq_cli.train][INFO] - task: MAE_AST_Pretraining_Task
[2025-07-10 22:41:14,862][fairseq_cli.train][INFO] - model: MAE_AST
[2025-07-10 22:41:14,862][fairseq_cli.train][INFO] - criterion: MAE_AST_Criterion
[2025-07-10 22:41:14,862][fairseq_cli.train][INFO] - num. shared model params: 36,035,328 (num. trained: 36,035,328)
[2025-07-10 22:41:14,862][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2025-07-10 22:41:14,864][mae_ast.data.mae_ast_dataset][INFO] - max_keep=None, min_keep=5000, loaded 53, skipped 0 short and 0 long, longest-loaded=20000, shortest-loaded=20000
[2025-07-10 22:41:14,864][mae_ast.data.mae_ast_dataset][INFO] - pad_audio=False, random_crop=True, normalize=False, max_sample_size=40000
[2025-07-10 22:41:14,958][fairseq.trainer][INFO] - detected shared parameter: batch_norm.weight <- batch_norm.bias
[2025-07-10 22:41:14,958][fairseq.utils][INFO] - ***********************CUDA enviroments for all 1 workers***********************
[2025-07-10 22:41:14,958][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 15.687 GB ; name = NVIDIA GeForce RTX 4080 SUPER           
[2025-07-10 22:41:14,958][fairseq.utils][INFO] - ***********************CUDA enviroments for all 1 workers***********************
[2025-07-10 22:41:14,958][fairseq_cli.train][INFO] - training on 1 devices (GPUs/TPUs)
[2025-07-10 22:41:14,958][fairseq_cli.train][INFO] - max tokens per device = 1048576 and max sentences per device = None
[2025-07-10 22:41:14,959][fairseq.trainer][INFO] - Preparing to load checkpoint checkpoints/checkpoint_last.pt
[2025-07-10 22:41:14,959][fairseq.trainer][INFO] - No existing checkpoint found checkpoints/checkpoint_last.pt
[2025-07-10 22:41:14,959][fairseq.trainer][INFO] - loading train data for epoch 1
[2025-07-10 22:41:14,960][mae_ast.data.mae_ast_dataset][INFO] - max_keep=None, min_keep=5000, loaded 989, skipped 0 short and 0 long, longest-loaded=20000, shortest-loaded=20000
[2025-07-10 22:41:14,960][mae_ast.data.mae_ast_dataset][INFO] - pad_audio=False, random_crop=True, normalize=False, max_sample_size=40000
[2025-07-10 22:41:15,358][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:41:15,360][fairseq.trainer][INFO] - begin training epoch 1
[2025-07-10 22:41:15,360][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:41:18,885][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1 @ 2 updates
[2025-07-10 22:41:18,885][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint1.pt
[2025-07-10 22:41:19,273][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint1.pt
[2025-07-10 22:41:19,423][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint1.pt (epoch 1 @ 2 updates, score None) (writing took 0.5377976690006108 seconds)
[2025-07-10 22:41:19,423][fairseq_cli.train][INFO] - end of epoch 1 (average epoch stats below)
[2025-07-10 22:41:19,425][train][INFO] - {"epoch": 1, "train_loss": "26.596", "train_nll_loss": "0.071", "train_loss_recon": "0.869", "train_loss_info_nce": "17.896", "train_ppl": "1.05", "train_wps": "2074.9", "train_ups": "0.93", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "2", "train_lr": "5e-08", "train_gnorm": "66.573", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "3", "train_gb_free": "11.9", "train_wall": "4"}
[2025-07-10 22:41:19,463][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:41:19,465][fairseq.trainer][INFO] - begin training epoch 2
[2025-07-10 22:41:19,466][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:41:22,187][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 2 @ 4 updates
[2025-07-10 22:41:22,187][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint2.pt
[2025-07-10 22:41:22,575][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint2.pt
[2025-07-10 22:41:22,912][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint2.pt (epoch 2 @ 4 updates, score None) (writing took 0.725646529000187 seconds)
[2025-07-10 22:41:22,913][fairseq_cli.train][INFO] - end of epoch 2 (average epoch stats below)
[2025-07-10 22:41:22,914][train][INFO] - {"epoch": 2, "train_loss": "26.608", "train_nll_loss": "0.072", "train_loss_recon": "0.869", "train_loss_info_nce": "17.893", "train_ppl": "1.05", "train_wps": "2346.8", "train_ups": "0.57", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "4", "train_lr": "1e-07", "train_gnorm": "66.61", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "8"}
[2025-07-10 22:41:22,950][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:41:22,952][fairseq.trainer][INFO] - begin training epoch 3
[2025-07-10 22:41:22,952][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:41:25,634][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 3 @ 6 updates
[2025-07-10 22:41:25,635][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint3.pt
[2025-07-10 22:41:26,021][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint3.pt
[2025-07-10 22:41:26,332][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint3.pt (epoch 3 @ 6 updates, score None) (writing took 0.6980304870003238 seconds)
[2025-07-10 22:41:26,333][fairseq_cli.train][INFO] - end of epoch 3 (average epoch stats below)
[2025-07-10 22:41:26,334][train][INFO] - {"epoch": 3, "train_loss": "26.604", "train_nll_loss": "0.072", "train_loss_recon": "0.868", "train_loss_info_nce": "17.929", "train_ppl": "1.05", "train_wps": "2394", "train_ups": "0.59", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "6", "train_lr": "1.5e-07", "train_gnorm": "66.441", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "11"}
[2025-07-10 22:41:26,366][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:41:26,368][fairseq.trainer][INFO] - begin training epoch 4
[2025-07-10 22:41:26,369][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:41:29,083][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 4 @ 8 updates
[2025-07-10 22:41:29,083][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint4.pt
[2025-07-10 22:41:29,461][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint4.pt
[2025-07-10 22:41:29,775][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint4.pt (epoch 4 @ 8 updates, score None) (writing took 0.6920087560001775 seconds)
[2025-07-10 22:41:29,775][fairseq_cli.train][INFO] - end of epoch 4 (average epoch stats below)
[2025-07-10 22:41:29,776][train][INFO] - {"epoch": 4, "train_loss": "26.599", "train_nll_loss": "0.072", "train_loss_recon": "0.868", "train_loss_info_nce": "17.951", "train_ppl": "1.05", "train_wps": "2378.1", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "8", "train_lr": "2e-07", "train_gnorm": "67.327", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "15"}
[2025-07-10 22:41:29,815][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:41:29,818][fairseq.trainer][INFO] - begin training epoch 5
[2025-07-10 22:41:29,818][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:41:32,539][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:41:32,848][valid][INFO] - {"epoch": 5, "valid_loss": "26.034", "valid_nll_loss": "0.07", "valid_loss_recon": "0.846", "valid_loss_info_nce": "17.574", "valid_ppl": "1.05", "valid_wps": "78673.9", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "10"}
[2025-07-10 22:41:32,849][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 5 @ 10 updates
[2025-07-10 22:41:32,849][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint5.pt
[2025-07-10 22:41:33,262][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint5.pt
[2025-07-10 22:41:33,757][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint5.pt (epoch 5 @ 10 updates, score 26.034) (writing took 0.908487516000605 seconds)
[2025-07-10 22:41:33,758][fairseq_cli.train][INFO] - end of epoch 5 (average epoch stats below)
[2025-07-10 22:41:33,759][train][INFO] - {"epoch": 5, "train_loss": "26.556", "train_nll_loss": "0.071", "train_loss_recon": "0.868", "train_loss_info_nce": "17.854", "train_ppl": "1.05", "train_wps": "2055.4", "train_ups": "0.5", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "10", "train_lr": "2.5e-07", "train_gnorm": "65.82", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "19"}
[2025-07-10 22:41:33,797][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:41:33,800][fairseq.trainer][INFO] - begin training epoch 6
[2025-07-10 22:41:33,800][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:41:36,516][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 6 @ 12 updates
[2025-07-10 22:41:36,517][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint6.pt
[2025-07-10 22:41:36,905][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint6.pt
[2025-07-10 22:41:37,202][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint6.pt (epoch 6 @ 12 updates, score None) (writing took 0.6855068849999952 seconds)
[2025-07-10 22:41:37,202][fairseq_cli.train][INFO] - end of epoch 6 (average epoch stats below)
[2025-07-10 22:41:37,203][train][INFO] - {"epoch": 6, "train_loss": "26.56", "train_nll_loss": "0.071", "train_loss_recon": "0.868", "train_loss_info_nce": "17.88", "train_ppl": "1.05", "train_wps": "2376.6", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "12", "train_lr": "3e-07", "train_gnorm": "66.341", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "22"}
[2025-07-10 22:41:37,237][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:41:37,239][fairseq.trainer][INFO] - begin training epoch 7
[2025-07-10 22:41:37,239][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:41:40,001][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 7 @ 14 updates
[2025-07-10 22:41:40,002][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint7.pt
[2025-07-10 22:41:40,376][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint7.pt
[2025-07-10 22:41:40,615][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint7.pt (epoch 7 @ 14 updates, score None) (writing took 0.6141854450006576 seconds)
[2025-07-10 22:41:40,616][fairseq_cli.train][INFO] - end of epoch 7 (average epoch stats below)
[2025-07-10 22:41:40,617][train][INFO] - {"epoch": 7, "train_loss": "26.554", "train_nll_loss": "0.071", "train_loss_recon": "0.868", "train_loss_info_nce": "17.871", "train_ppl": "1.05", "train_wps": "2398.3", "train_ups": "0.59", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "14", "train_lr": "3.5e-07", "train_gnorm": "64.591", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "26"}
[2025-07-10 22:41:40,654][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:41:40,655][fairseq.trainer][INFO] - begin training epoch 8
[2025-07-10 22:41:40,656][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:41:43,413][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 8 @ 16 updates
[2025-07-10 22:41:43,414][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint8.pt
[2025-07-10 22:41:43,807][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint8.pt
[2025-07-10 22:41:44,123][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint8.pt (epoch 8 @ 16 updates, score None) (writing took 0.7094945239996377 seconds)
[2025-07-10 22:41:44,123][fairseq_cli.train][INFO] - end of epoch 8 (average epoch stats below)
[2025-07-10 22:41:44,124][train][INFO] - {"epoch": 8, "train_loss": "26.496", "train_nll_loss": "0.071", "train_loss_recon": "0.868", "train_loss_info_nce": "17.789", "train_ppl": "1.05", "train_wps": "2334.1", "train_ups": "0.57", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "16", "train_lr": "4e-07", "train_gnorm": "63.208", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "29"}
[2025-07-10 22:41:44,160][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:41:44,162][fairseq.trainer][INFO] - begin training epoch 9
[2025-07-10 22:41:44,162][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:41:46,879][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 9 @ 18 updates
[2025-07-10 22:41:46,880][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint9.pt
[2025-07-10 22:41:47,249][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint9.pt
[2025-07-10 22:41:47,575][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint9.pt (epoch 9 @ 18 updates, score None) (writing took 0.6956970740002362 seconds)
[2025-07-10 22:41:47,575][fairseq_cli.train][INFO] - end of epoch 9 (average epoch stats below)
[2025-07-10 22:41:47,576][train][INFO] - {"epoch": 9, "train_loss": "26.478", "train_nll_loss": "0.071", "train_loss_recon": "0.868", "train_loss_info_nce": "17.791", "train_ppl": "1.05", "train_wps": "2371.5", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "18", "train_lr": "4.5e-07", "train_gnorm": "63.446", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "33"}
[2025-07-10 22:41:47,618][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:41:47,620][fairseq.trainer][INFO] - begin training epoch 10
[2025-07-10 22:41:47,620][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:41:50,309][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:41:50,534][valid][INFO] - {"epoch": 10, "valid_loss": "25.737", "valid_nll_loss": "0.069", "valid_loss_recon": "0.848", "valid_loss_info_nce": "17.259", "valid_ppl": "1.05", "valid_wps": "80162.8", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "20", "valid_best_loss": "25.737"}
[2025-07-10 22:41:50,535][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 10 @ 20 updates
[2025-07-10 22:41:50,536][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint10.pt
[2025-07-10 22:41:50,920][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint10.pt
[2025-07-10 22:41:51,552][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint10.pt (epoch 10 @ 20 updates, score 25.737) (writing took 1.0164311549997365 seconds)
[2025-07-10 22:41:51,552][fairseq_cli.train][INFO] - end of epoch 10 (average epoch stats below)
[2025-07-10 22:41:51,553][train][INFO] - {"epoch": 10, "train_loss": "26.315", "train_nll_loss": "0.071", "train_loss_recon": "0.867", "train_loss_info_nce": "17.667", "train_ppl": "1.05", "train_wps": "2058.4", "train_ups": "0.5", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "20", "train_lr": "5e-07", "train_gnorm": "59.544", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "37"}
[2025-07-10 22:41:51,591][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:41:51,593][fairseq.trainer][INFO] - begin training epoch 11
[2025-07-10 22:41:51,593][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:41:54,302][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 11 @ 22 updates
[2025-07-10 22:41:54,302][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint11.pt
[2025-07-10 22:41:54,692][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint11.pt
[2025-07-10 22:41:55,023][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint11.pt (epoch 11 @ 22 updates, score None) (writing took 0.7206103720000101 seconds)
[2025-07-10 22:41:55,023][fairseq_cli.train][INFO] - end of epoch 11 (average epoch stats below)
[2025-07-10 22:41:55,024][train][INFO] - {"epoch": 11, "train_loss": "26.261", "train_nll_loss": "0.071", "train_loss_recon": "0.866", "train_loss_info_nce": "17.583", "train_ppl": "1.05", "train_wps": "2358.7", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "22", "train_lr": "5.5e-07", "train_gnorm": "57.843", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "40"}
[2025-07-10 22:41:55,058][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:41:55,060][fairseq.trainer][INFO] - begin training epoch 12
[2025-07-10 22:41:55,060][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:41:57,793][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 12 @ 24 updates
[2025-07-10 22:41:57,794][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint12.pt
[2025-07-10 22:41:58,178][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint12.pt
[2025-07-10 22:41:58,491][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint12.pt (epoch 12 @ 24 updates, score None) (writing took 0.6978322670001944 seconds)
[2025-07-10 22:41:58,492][fairseq_cli.train][INFO] - end of epoch 12 (average epoch stats below)
[2025-07-10 22:41:58,492][train][INFO] - {"epoch": 12, "train_loss": "26.19", "train_nll_loss": "0.07", "train_loss_recon": "0.866", "train_loss_info_nce": "17.529", "train_ppl": "1.05", "train_wps": "2359.9", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "24", "train_lr": "6e-07", "train_gnorm": "55.37", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "44"}
[2025-07-10 22:41:58,532][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:41:58,534][fairseq.trainer][INFO] - begin training epoch 13
[2025-07-10 22:41:58,534][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:42:01,254][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 13 @ 26 updates
[2025-07-10 22:42:01,254][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint13.pt
[2025-07-10 22:42:01,628][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint13.pt
[2025-07-10 22:42:01,955][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint13.pt (epoch 13 @ 26 updates, score None) (writing took 0.7013171549997423 seconds)
[2025-07-10 22:42:01,955][fairseq_cli.train][INFO] - end of epoch 13 (average epoch stats below)
[2025-07-10 22:42:01,956][train][INFO] - {"epoch": 13, "train_loss": "26.165", "train_nll_loss": "0.07", "train_loss_recon": "0.866", "train_loss_info_nce": "17.528", "train_ppl": "1.05", "train_wps": "2363.2", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "26", "train_lr": "6.5e-07", "train_gnorm": "56.272", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "47"}
[2025-07-10 22:42:01,996][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:42:01,998][fairseq.trainer][INFO] - begin training epoch 14
[2025-07-10 22:42:01,998][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:42:04,708][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 14 @ 28 updates
[2025-07-10 22:42:04,709][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint14.pt
[2025-07-10 22:42:05,103][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint14.pt
[2025-07-10 22:42:05,407][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint14.pt (epoch 14 @ 28 updates, score None) (writing took 0.6988998730003004 seconds)
[2025-07-10 22:42:05,408][fairseq_cli.train][INFO] - end of epoch 14 (average epoch stats below)
[2025-07-10 22:42:05,409][train][INFO] - {"epoch": 14, "train_loss": "25.714", "train_nll_loss": "0.069", "train_loss_recon": "0.862", "train_loss_info_nce": "17.129", "train_ppl": "1.05", "train_wps": "2371.4", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "28", "train_lr": "7e-07", "train_gnorm": "44.937", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "50"}
[2025-07-10 22:42:05,449][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:42:05,451][fairseq.trainer][INFO] - begin training epoch 15
[2025-07-10 22:42:05,451][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:42:08,161][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:42:08,381][valid][INFO] - {"epoch": 15, "valid_loss": "25.046", "valid_nll_loss": "0.067", "valid_loss_recon": "0.838", "valid_loss_info_nce": "16.664", "valid_ppl": "1.05", "valid_wps": "80575.7", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "30", "valid_best_loss": "25.046"}
[2025-07-10 22:42:08,382][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 15 @ 30 updates
[2025-07-10 22:42:08,382][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint15.pt
[2025-07-10 22:42:08,765][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint15.pt
[2025-07-10 22:42:09,405][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint15.pt (epoch 15 @ 30 updates, score 25.046) (writing took 1.0227816720007468 seconds)
[2025-07-10 22:42:09,405][fairseq_cli.train][INFO] - end of epoch 15 (average epoch stats below)
[2025-07-10 22:42:09,406][train][INFO] - {"epoch": 15, "train_loss": "25.66", "train_nll_loss": "0.069", "train_loss_recon": "0.861", "train_loss_info_nce": "17.059", "train_ppl": "1.05", "train_wps": "2047.9", "train_ups": "0.5", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "30", "train_lr": "7.5e-07", "train_gnorm": "44.004", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "54"}
[2025-07-10 22:42:09,439][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:42:09,440][fairseq.trainer][INFO] - begin training epoch 16
[2025-07-10 22:42:09,440][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:42:12,159][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 16 @ 32 updates
[2025-07-10 22:42:12,160][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint16.pt
[2025-07-10 22:42:12,551][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint16.pt
[2025-07-10 22:42:12,892][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint16.pt (epoch 16 @ 32 updates, score None) (writing took 0.732851933999882 seconds)
[2025-07-10 22:42:12,892][fairseq_cli.train][INFO] - end of epoch 16 (average epoch stats below)
[2025-07-10 22:42:12,893][train][INFO] - {"epoch": 16, "train_loss": "25.58", "train_nll_loss": "0.069", "train_loss_recon": "0.86", "train_loss_info_nce": "16.987", "train_ppl": "1.05", "train_wps": "2347.3", "train_ups": "0.57", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "32", "train_lr": "8e-07", "train_gnorm": "43.053", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "58"}
[2025-07-10 22:42:12,932][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:42:12,934][fairseq.trainer][INFO] - begin training epoch 17
[2025-07-10 22:42:12,934][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:42:15,635][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 17 @ 34 updates
[2025-07-10 22:42:15,635][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint17.pt
[2025-07-10 22:42:16,028][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint17.pt
[2025-07-10 22:42:16,365][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint17.pt (epoch 17 @ 34 updates, score None) (writing took 0.7299408050002967 seconds)
[2025-07-10 22:42:16,365][fairseq_cli.train][INFO] - end of epoch 17 (average epoch stats below)
[2025-07-10 22:42:16,366][train][INFO] - {"epoch": 17, "train_loss": "25.459", "train_nll_loss": "0.068", "train_loss_recon": "0.859", "train_loss_info_nce": "16.878", "train_ppl": "1.05", "train_wps": "2357.4", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "34", "train_lr": "8.5e-07", "train_gnorm": "41.662", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "61"}
[2025-07-10 22:42:16,404][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:42:16,405][fairseq.trainer][INFO] - begin training epoch 18
[2025-07-10 22:42:16,406][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:42:19,129][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 18 @ 36 updates
[2025-07-10 22:42:19,129][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint18.pt
[2025-07-10 22:42:19,507][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint18.pt
[2025-07-10 22:42:19,862][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint18.pt (epoch 18 @ 36 updates, score None) (writing took 0.7337046820002797 seconds)
[2025-07-10 22:42:19,863][fairseq_cli.train][INFO] - end of epoch 18 (average epoch stats below)
[2025-07-10 22:42:19,863][train][INFO] - {"epoch": 18, "train_loss": "25.393", "train_nll_loss": "0.068", "train_loss_recon": "0.858", "train_loss_info_nce": "16.789", "train_ppl": "1.05", "train_wps": "2340.4", "train_ups": "0.57", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "36", "train_lr": "9e-07", "train_gnorm": "40.994", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "65"}
[2025-07-10 22:42:19,901][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:42:19,903][fairseq.trainer][INFO] - begin training epoch 19
[2025-07-10 22:42:19,903][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:42:22,563][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 19 @ 38 updates
[2025-07-10 22:42:22,564][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint19.pt
[2025-07-10 22:42:22,952][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint19.pt
[2025-07-10 22:42:23,277][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint19.pt (epoch 19 @ 38 updates, score None) (writing took 0.7138213959997302 seconds)
[2025-07-10 22:42:23,278][fairseq_cli.train][INFO] - end of epoch 19 (average epoch stats below)
[2025-07-10 22:42:23,279][train][INFO] - {"epoch": 19, "train_loss": "25.185", "train_nll_loss": "0.068", "train_loss_recon": "0.855", "train_loss_info_nce": "16.578", "train_ppl": "1.05", "train_wps": "2397.1", "train_ups": "0.59", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "38", "train_lr": "9.5e-07", "train_gnorm": "38.991", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "68"}
[2025-07-10 22:42:23,315][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:42:23,317][fairseq.trainer][INFO] - begin training epoch 20
[2025-07-10 22:42:23,317][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:42:26,021][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:42:26,243][valid][INFO] - {"epoch": 20, "valid_loss": "24.185", "valid_nll_loss": "0.065", "valid_loss_recon": "0.829", "valid_loss_info_nce": "15.895", "valid_ppl": "1.05", "valid_wps": "80344", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "40", "valid_best_loss": "24.185"}
[2025-07-10 22:42:26,244][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 20 @ 40 updates
[2025-07-10 22:42:26,245][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint20.pt
[2025-07-10 22:42:26,634][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint20.pt
[2025-07-10 22:42:27,274][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint20.pt (epoch 20 @ 40 updates, score 24.185) (writing took 1.0294950460001928 seconds)
[2025-07-10 22:42:27,274][fairseq_cli.train][INFO] - end of epoch 20 (average epoch stats below)
[2025-07-10 22:42:27,275][train][INFO] - {"epoch": 20, "train_loss": "25.087", "train_nll_loss": "0.067", "train_loss_recon": "0.855", "train_loss_info_nce": "16.499", "train_ppl": "1.05", "train_wps": "2048.4", "train_ups": "0.5", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "40", "train_lr": "1e-06", "train_gnorm": "38.234", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "72"}
[2025-07-10 22:42:27,310][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:42:27,311][fairseq.trainer][INFO] - begin training epoch 21
[2025-07-10 22:42:27,312][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:42:30,053][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 21 @ 42 updates
[2025-07-10 22:42:30,054][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint21.pt
[2025-07-10 22:42:30,436][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint21.pt
[2025-07-10 22:42:30,756][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint21.pt (epoch 21 @ 42 updates, score None) (writing took 0.7026924840001811 seconds)
[2025-07-10 22:42:30,756][fairseq_cli.train][INFO] - end of epoch 21 (average epoch stats below)
[2025-07-10 22:42:30,757][train][INFO] - {"epoch": 21, "train_loss": "24.974", "train_nll_loss": "0.067", "train_loss_recon": "0.854", "train_loss_info_nce": "16.412", "train_ppl": "1.05", "train_wps": "2350.6", "train_ups": "0.57", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "42", "train_lr": "1.05e-06", "train_gnorm": "37.392", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "76"}
[2025-07-10 22:42:30,792][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:42:30,794][fairseq.trainer][INFO] - begin training epoch 22
[2025-07-10 22:42:30,794][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:42:33,553][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 22 @ 44 updates
[2025-07-10 22:42:33,554][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint22.pt
[2025-07-10 22:42:33,927][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint22.pt
[2025-07-10 22:42:34,247][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint22.pt (epoch 22 @ 44 updates, score None) (writing took 0.6940079820005849 seconds)
[2025-07-10 22:42:34,248][fairseq_cli.train][INFO] - end of epoch 22 (average epoch stats below)
[2025-07-10 22:42:34,249][train][INFO] - {"epoch": 22, "train_loss": "24.753", "train_nll_loss": "0.067", "train_loss_recon": "0.85", "train_loss_info_nce": "16.232", "train_ppl": "1.05", "train_wps": "2344.8", "train_ups": "0.57", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "44", "train_lr": "1.1e-06", "train_gnorm": "35.789", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "79"}
[2025-07-10 22:42:34,288][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:42:34,290][fairseq.trainer][INFO] - begin training epoch 23
[2025-07-10 22:42:34,290][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:42:36,986][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 23 @ 46 updates
[2025-07-10 22:42:36,987][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint23.pt
[2025-07-10 22:42:37,366][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint23.pt
[2025-07-10 22:42:37,687][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint23.pt (epoch 23 @ 46 updates, score None) (writing took 0.7004196999996566 seconds)
[2025-07-10 22:42:37,687][fairseq_cli.train][INFO] - end of epoch 23 (average epoch stats below)
[2025-07-10 22:42:37,688][train][INFO] - {"epoch": 23, "train_loss": "24.606", "train_nll_loss": "0.066", "train_loss_recon": "0.848", "train_loss_info_nce": "16.12", "train_ppl": "1.05", "train_wps": "2380.2", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "46", "train_lr": "1.15e-06", "train_gnorm": "34.452", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "83"}
[2025-07-10 22:42:37,724][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:42:37,727][fairseq.trainer][INFO] - begin training epoch 24
[2025-07-10 22:42:37,727][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:42:40,406][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 24 @ 48 updates
[2025-07-10 22:42:40,407][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint24.pt
[2025-07-10 22:42:40,785][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint24.pt
[2025-07-10 22:42:41,118][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint24.pt (epoch 24 @ 48 updates, score None) (writing took 0.7115001459997075 seconds)
[2025-07-10 22:42:41,118][fairseq_cli.train][INFO] - end of epoch 24 (average epoch stats below)
[2025-07-10 22:42:41,119][train][INFO] - {"epoch": 24, "train_loss": "24.453", "train_nll_loss": "0.066", "train_loss_recon": "0.846", "train_loss_info_nce": "15.951", "train_ppl": "1.05", "train_wps": "2386", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "48", "train_lr": "1.2e-06", "train_gnorm": "33.424", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "86"}
[2025-07-10 22:42:41,157][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:42:41,159][fairseq.trainer][INFO] - begin training epoch 25
[2025-07-10 22:42:41,159][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:42:43,849][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:42:44,073][valid][INFO] - {"epoch": 25, "valid_loss": "23.447", "valid_nll_loss": "0.063", "valid_loss_recon": "0.818", "valid_loss_info_nce": "15.266", "valid_ppl": "1.04", "valid_wps": "79113.1", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "50", "valid_best_loss": "23.447"}
[2025-07-10 22:42:44,073][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 25 @ 50 updates
[2025-07-10 22:42:44,074][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint25.pt
[2025-07-10 22:42:44,455][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint25.pt
[2025-07-10 22:42:45,110][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint25.pt (epoch 25 @ 50 updates, score 23.447) (writing took 1.0366053359994112 seconds)
[2025-07-10 22:42:45,110][fairseq_cli.train][INFO] - end of epoch 25 (average epoch stats below)
[2025-07-10 22:42:45,112][train][INFO] - {"epoch": 25, "train_loss": "24.391", "train_nll_loss": "0.066", "train_loss_recon": "0.845", "train_loss_info_nce": "15.911", "train_ppl": "1.05", "train_wps": "2050.5", "train_ups": "0.5", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "50", "train_lr": "1.25e-06", "train_gnorm": "33.067", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "90"}
[2025-07-10 22:42:45,149][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:42:45,151][fairseq.trainer][INFO] - begin training epoch 26
[2025-07-10 22:42:45,151][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:42:47,858][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 26 @ 52 updates
[2025-07-10 22:42:47,858][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint26.pt
[2025-07-10 22:42:48,238][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint26.pt
[2025-07-10 22:42:48,595][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint26.pt (epoch 26 @ 52 updates, score None) (writing took 0.7374789969999256 seconds)
[2025-07-10 22:42:48,595][fairseq_cli.train][INFO] - end of epoch 26 (average epoch stats below)
[2025-07-10 22:42:48,596][train][INFO] - {"epoch": 26, "train_loss": "24.285", "train_nll_loss": "0.065", "train_loss_recon": "0.844", "train_loss_info_nce": "15.816", "train_ppl": "1.05", "train_wps": "2349.1", "train_ups": "0.57", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "52", "train_lr": "1.3e-06", "train_gnorm": "32.191", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "94"}
[2025-07-10 22:42:48,636][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:42:48,638][fairseq.trainer][INFO] - begin training epoch 27
[2025-07-10 22:42:48,638][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:42:51,319][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 27 @ 54 updates
[2025-07-10 22:42:51,319][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint27.pt
[2025-07-10 22:42:51,709][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint27.pt
[2025-07-10 22:42:52,031][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint27.pt (epoch 27 @ 54 updates, score None) (writing took 0.7123251679995519 seconds)
[2025-07-10 22:42:52,031][fairseq_cli.train][INFO] - end of epoch 27 (average epoch stats below)
[2025-07-10 22:42:52,032][train][INFO] - {"epoch": 27, "train_loss": "24.142", "train_nll_loss": "0.065", "train_loss_recon": "0.842", "train_loss_info_nce": "15.706", "train_ppl": "1.05", "train_wps": "2382.4", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "54", "train_lr": "1.35e-06", "train_gnorm": "31.278", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "97"}
[2025-07-10 22:42:52,069][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:42:52,070][fairseq.trainer][INFO] - begin training epoch 28
[2025-07-10 22:42:52,071][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:42:54,776][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 28 @ 56 updates
[2025-07-10 22:42:54,776][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint28.pt
[2025-07-10 22:42:55,154][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint28.pt
[2025-07-10 22:42:55,492][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint28.pt (epoch 28 @ 56 updates, score None) (writing took 0.7160631540000395 seconds)
[2025-07-10 22:42:55,492][fairseq_cli.train][INFO] - end of epoch 28 (average epoch stats below)
[2025-07-10 22:42:55,493][train][INFO] - {"epoch": 28, "train_loss": "23.974", "train_nll_loss": "0.064", "train_loss_recon": "0.839", "train_loss_info_nce": "15.583", "train_ppl": "1.05", "train_wps": "2365.4", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "56", "train_lr": "1.4e-06", "train_gnorm": "30.354", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "101"}
[2025-07-10 22:42:55,531][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:42:55,532][fairseq.trainer][INFO] - begin training epoch 29
[2025-07-10 22:42:55,533][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:42:58,255][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 29 @ 58 updates
[2025-07-10 22:42:58,256][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint29.pt
[2025-07-10 22:42:58,649][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint29.pt
[2025-07-10 22:42:59,041][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint29.pt (epoch 29 @ 58 updates, score None) (writing took 0.7853081889998066 seconds)
[2025-07-10 22:42:59,041][fairseq_cli.train][INFO] - end of epoch 29 (average epoch stats below)
[2025-07-10 22:42:59,042][train][INFO] - {"epoch": 29, "train_loss": "23.822", "train_nll_loss": "0.064", "train_loss_recon": "0.837", "train_loss_info_nce": "15.476", "train_ppl": "1.05", "train_wps": "2306.8", "train_ups": "0.56", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "58", "train_lr": "1.45e-06", "train_gnorm": "29.762", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "104"}
[2025-07-10 22:42:59,080][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:42:59,082][fairseq.trainer][INFO] - begin training epoch 30
[2025-07-10 22:42:59,082][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:43:01,833][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:43:02,054][valid][INFO] - {"epoch": 30, "valid_loss": "22.561", "valid_nll_loss": "0.061", "valid_loss_recon": "0.804", "valid_loss_info_nce": "14.522", "valid_ppl": "1.04", "valid_wps": "81066.2", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "60", "valid_best_loss": "22.561"}
[2025-07-10 22:43:02,055][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 30 @ 60 updates
[2025-07-10 22:43:02,055][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint30.pt
[2025-07-10 22:43:02,437][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint30.pt
[2025-07-10 22:43:03,059][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint30.pt (epoch 30 @ 60 updates, score 22.561) (writing took 1.004327626000304 seconds)
[2025-07-10 22:43:03,060][fairseq_cli.train][INFO] - end of epoch 30 (average epoch stats below)
[2025-07-10 22:43:03,061][train][INFO] - {"epoch": 30, "train_loss": "23.661", "train_nll_loss": "0.064", "train_loss_recon": "0.834", "train_loss_info_nce": "15.309", "train_ppl": "1.05", "train_wps": "2037", "train_ups": "0.5", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "60", "train_lr": "1.5e-06", "train_gnorm": "28.554", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "108"}
[2025-07-10 22:43:03,097][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:43:03,098][fairseq.trainer][INFO] - begin training epoch 31
[2025-07-10 22:43:03,099][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:43:05,798][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 31 @ 62 updates
[2025-07-10 22:43:05,798][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint31.pt
[2025-07-10 22:43:06,192][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint31.pt
[2025-07-10 22:43:06,528][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint31.pt (epoch 31 @ 62 updates, score None) (writing took 0.7302022889998625 seconds)
[2025-07-10 22:43:06,528][fairseq_cli.train][INFO] - end of epoch 31 (average epoch stats below)
[2025-07-10 22:43:06,529][train][INFO] - {"epoch": 31, "train_loss": "23.49", "train_nll_loss": "0.063", "train_loss_recon": "0.831", "train_loss_info_nce": "15.161", "train_ppl": "1.04", "train_wps": "2360.3", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "62", "train_lr": "1.55e-06", "train_gnorm": "27.743", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "112"}
[2025-07-10 22:43:06,567][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:43:06,569][fairseq.trainer][INFO] - begin training epoch 32
[2025-07-10 22:43:06,569][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:43:09,287][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 32 @ 64 updates
[2025-07-10 22:43:09,288][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint32.pt
[2025-07-10 22:43:09,666][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint32.pt
[2025-07-10 22:43:09,985][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint32.pt (epoch 32 @ 64 updates, score None) (writing took 0.6978240700000242 seconds)
[2025-07-10 22:43:09,986][fairseq_cli.train][INFO] - end of epoch 32 (average epoch stats below)
[2025-07-10 22:43:09,987][train][INFO] - {"epoch": 32, "train_loss": "23.333", "train_nll_loss": "0.063", "train_loss_recon": "0.828", "train_loss_info_nce": "15.046", "train_ppl": "1.04", "train_wps": "2367.8", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "64", "train_lr": "1.6e-06", "train_gnorm": "26.35", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "115"}
[2025-07-10 22:43:10,025][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:43:10,028][fairseq.trainer][INFO] - begin training epoch 33
[2025-07-10 22:43:10,028][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:43:12,745][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 33 @ 66 updates
[2025-07-10 22:43:12,746][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint33.pt
[2025-07-10 22:43:13,127][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint33.pt
[2025-07-10 22:43:13,463][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint33.pt (epoch 33 @ 66 updates, score None) (writing took 0.7180283360003159 seconds)
[2025-07-10 22:43:13,464][fairseq_cli.train][INFO] - end of epoch 33 (average epoch stats below)
[2025-07-10 22:43:13,465][train][INFO] - {"epoch": 33, "train_loss": "23.154", "train_nll_loss": "0.062", "train_loss_recon": "0.824", "train_loss_info_nce": "14.881", "train_ppl": "1.04", "train_wps": "2353.7", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "66", "train_lr": "1.65e-06", "train_gnorm": "25.053", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "119"}
[2025-07-10 22:43:13,505][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:43:13,506][fairseq.trainer][INFO] - begin training epoch 34
[2025-07-10 22:43:13,507][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:43:16,186][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 34 @ 68 updates
[2025-07-10 22:43:16,187][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint34.pt
[2025-07-10 22:43:16,576][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint34.pt
[2025-07-10 22:43:16,896][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint34.pt (epoch 34 @ 68 updates, score None) (writing took 0.7090899969998645 seconds)
[2025-07-10 22:43:16,896][fairseq_cli.train][INFO] - end of epoch 34 (average epoch stats below)
[2025-07-10 22:43:16,897][train][INFO] - {"epoch": 34, "train_loss": "23.024", "train_nll_loss": "0.062", "train_loss_recon": "0.822", "train_loss_info_nce": "14.809", "train_ppl": "1.04", "train_wps": "2385.2", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "68", "train_lr": "1.7e-06", "train_gnorm": "24.351", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "122"}
[2025-07-10 22:43:16,934][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:43:16,935][fairseq.trainer][INFO] - begin training epoch 35
[2025-07-10 22:43:16,936][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:43:19,631][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:43:19,853][valid][INFO] - {"epoch": 35, "valid_loss": "21.769", "valid_nll_loss": "0.059", "valid_loss_recon": "0.785", "valid_loss_info_nce": "13.915", "valid_ppl": "1.04", "valid_wps": "81110.3", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "70", "valid_best_loss": "21.769"}
[2025-07-10 22:43:19,854][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 35 @ 70 updates
[2025-07-10 22:43:19,854][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint35.pt
[2025-07-10 22:43:20,244][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint35.pt
[2025-07-10 22:43:21,149][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint35.pt (epoch 35 @ 70 updates, score 21.769) (writing took 1.294755609999811 seconds)
[2025-07-10 22:43:21,149][fairseq_cli.train][INFO] - end of epoch 35 (average epoch stats below)
[2025-07-10 22:43:21,150][train][INFO] - {"epoch": 35, "train_loss": "22.903", "train_nll_loss": "0.062", "train_loss_recon": "0.819", "train_loss_info_nce": "14.703", "train_ppl": "1.04", "train_wps": "1924.7", "train_ups": "0.47", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "70", "train_lr": "1.75e-06", "train_gnorm": "23.796", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "126"}
[2025-07-10 22:43:21,188][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:43:21,190][fairseq.trainer][INFO] - begin training epoch 36
[2025-07-10 22:43:21,190][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:43:23,888][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 36 @ 72 updates
[2025-07-10 22:43:23,888][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint36.pt
[2025-07-10 22:43:24,275][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint36.pt
[2025-07-10 22:43:24,600][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint36.pt (epoch 36 @ 72 updates, score None) (writing took 0.7117830520001007 seconds)
[2025-07-10 22:43:24,600][fairseq_cli.train][INFO] - end of epoch 36 (average epoch stats below)
[2025-07-10 22:43:24,601][train][INFO] - {"epoch": 36, "train_loss": "22.751", "train_nll_loss": "0.061", "train_loss_recon": "0.816", "train_loss_info_nce": "14.576", "train_ppl": "1.04", "train_wps": "2372.2", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "72", "train_lr": "1.8e-06", "train_gnorm": "23.065", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "130"}
[2025-07-10 22:43:24,636][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:43:24,638][fairseq.trainer][INFO] - begin training epoch 37
[2025-07-10 22:43:24,638][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:43:27,289][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 37 @ 74 updates
[2025-07-10 22:43:27,289][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint37.pt
[2025-07-10 22:43:27,662][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint37.pt
[2025-07-10 22:43:28,006][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint37.pt (epoch 37 @ 74 updates, score None) (writing took 0.7168713279997974 seconds)
[2025-07-10 22:43:28,006][fairseq_cli.train][INFO] - end of epoch 37 (average epoch stats below)
[2025-07-10 22:43:28,007][train][INFO] - {"epoch": 37, "train_loss": "22.59", "train_nll_loss": "0.061", "train_loss_recon": "0.812", "train_loss_info_nce": "14.472", "train_ppl": "1.04", "train_wps": "2403.7", "train_ups": "0.59", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "74", "train_lr": "1.85e-06", "train_gnorm": "22.414", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "133"}
[2025-07-10 22:43:28,050][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:43:28,052][fairseq.trainer][INFO] - begin training epoch 38
[2025-07-10 22:43:28,052][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:43:30,726][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 38 @ 76 updates
[2025-07-10 22:43:30,726][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint38.pt
[2025-07-10 22:43:31,109][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint38.pt
[2025-07-10 22:43:31,431][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint38.pt (epoch 38 @ 76 updates, score None) (writing took 0.7056135660004657 seconds)
[2025-07-10 22:43:31,432][fairseq_cli.train][INFO] - end of epoch 38 (average epoch stats below)
[2025-07-10 22:43:31,433][train][INFO] - {"epoch": 38, "train_loss": "22.441", "train_nll_loss": "0.06", "train_loss_recon": "0.808", "train_loss_info_nce": "14.343", "train_ppl": "1.04", "train_wps": "2389.7", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "76", "train_lr": "1.9e-06", "train_gnorm": "21.525", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "136"}
[2025-07-10 22:43:31,468][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:43:31,469][fairseq.trainer][INFO] - begin training epoch 39
[2025-07-10 22:43:31,470][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:43:34,189][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 39 @ 78 updates
[2025-07-10 22:43:34,190][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint39.pt
[2025-07-10 22:43:34,571][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint39.pt
[2025-07-10 22:43:34,938][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint39.pt (epoch 39 @ 78 updates, score None) (writing took 0.7492535399996996 seconds)
[2025-07-10 22:43:34,939][fairseq_cli.train][INFO] - end of epoch 39 (average epoch stats below)
[2025-07-10 22:43:34,940][train][INFO] - {"epoch": 39, "train_loss": "22.275", "train_nll_loss": "0.06", "train_loss_recon": "0.805", "train_loss_info_nce": "14.219", "train_ppl": "1.04", "train_wps": "2334.2", "train_ups": "0.57", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "78", "train_lr": "1.95e-06", "train_gnorm": "20.825", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "140"}
[2025-07-10 22:43:34,980][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:43:34,982][fairseq.trainer][INFO] - begin training epoch 40
[2025-07-10 22:43:34,982][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:43:37,707][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:43:37,930][valid][INFO] - {"epoch": 40, "valid_loss": "20.84", "valid_nll_loss": "0.056", "valid_loss_recon": "0.763", "valid_loss_info_nce": "13.206", "valid_ppl": "1.04", "valid_wps": "79105.7", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "80", "valid_best_loss": "20.84"}
[2025-07-10 22:43:37,930][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 40 @ 80 updates
[2025-07-10 22:43:37,931][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint40.pt
[2025-07-10 22:43:38,313][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint40.pt
[2025-07-10 22:43:38,950][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint40.pt (epoch 40 @ 80 updates, score 20.84) (writing took 1.0196435759999076 seconds)
[2025-07-10 22:43:38,950][fairseq_cli.train][INFO] - end of epoch 40 (average epoch stats below)
[2025-07-10 22:43:38,951][train][INFO] - {"epoch": 40, "train_loss": "22.096", "train_nll_loss": "0.059", "train_loss_recon": "0.8", "train_loss_info_nce": "14.067", "train_ppl": "1.04", "train_wps": "2040.7", "train_ups": "0.5", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "80", "train_lr": "2e-06", "train_gnorm": "20.176", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "144"}
[2025-07-10 22:43:38,990][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:43:38,991][fairseq.trainer][INFO] - begin training epoch 41
[2025-07-10 22:43:38,991][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:43:41,712][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 41 @ 82 updates
[2025-07-10 22:43:41,712][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint41.pt
[2025-07-10 22:43:42,092][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint41.pt
[2025-07-10 22:43:42,442][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint41.pt (epoch 41 @ 82 updates, score None) (writing took 0.7302416819993596 seconds)
[2025-07-10 22:43:42,442][fairseq_cli.train][INFO] - end of epoch 41 (average epoch stats below)
[2025-07-10 22:43:42,443][train][INFO] - {"epoch": 41, "train_loss": "21.97", "train_nll_loss": "0.059", "train_loss_recon": "0.797", "train_loss_info_nce": "14.009", "train_ppl": "1.04", "train_wps": "2344.4", "train_ups": "0.57", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "82", "train_lr": "2.05e-06", "train_gnorm": "19.507", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "147"}
[2025-07-10 22:43:42,480][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:43:42,481][fairseq.trainer][INFO] - begin training epoch 42
[2025-07-10 22:43:42,482][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:43:45,186][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 42 @ 84 updates
[2025-07-10 22:43:45,186][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint42.pt
[2025-07-10 22:43:45,577][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint42.pt
[2025-07-10 22:43:45,872][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint42.pt (epoch 42 @ 84 updates, score None) (writing took 0.6865624530000787 seconds)
[2025-07-10 22:43:45,872][fairseq_cli.train][INFO] - end of epoch 42 (average epoch stats below)
[2025-07-10 22:43:45,873][train][INFO] - {"epoch": 42, "train_loss": "21.838", "train_nll_loss": "0.059", "train_loss_recon": "0.792", "train_loss_info_nce": "13.878", "train_ppl": "1.04", "train_wps": "2386.6", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "84", "train_lr": "2.1e-06", "train_gnorm": "18.778", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "151"}
[2025-07-10 22:43:45,909][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:43:45,911][fairseq.trainer][INFO] - begin training epoch 43
[2025-07-10 22:43:45,911][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:43:48,628][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 43 @ 86 updates
[2025-07-10 22:43:48,629][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint43.pt
[2025-07-10 22:43:49,004][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint43.pt
[2025-07-10 22:43:49,505][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint43.pt (epoch 43 @ 86 updates, score None) (writing took 0.876506235999841 seconds)
[2025-07-10 22:43:49,505][fairseq_cli.train][INFO] - end of epoch 43 (average epoch stats below)
[2025-07-10 22:43:49,506][train][INFO] - {"epoch": 43, "train_loss": "21.667", "train_nll_loss": "0.058", "train_loss_recon": "0.788", "train_loss_info_nce": "13.777", "train_ppl": "1.04", "train_wps": "2253.6", "train_ups": "0.55", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "86", "train_lr": "2.15e-06", "train_gnorm": "18.241", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "155"}
[2025-07-10 22:43:49,542][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:43:49,544][fairseq.trainer][INFO] - begin training epoch 44
[2025-07-10 22:43:49,544][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:43:52,262][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 44 @ 88 updates
[2025-07-10 22:43:52,262][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint44.pt
[2025-07-10 22:43:52,647][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint44.pt
[2025-07-10 22:43:52,962][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint44.pt (epoch 44 @ 88 updates, score None) (writing took 0.700255972999912 seconds)
[2025-07-10 22:43:52,962][fairseq_cli.train][INFO] - end of epoch 44 (average epoch stats below)
[2025-07-10 22:43:52,964][train][INFO] - {"epoch": 44, "train_loss": "21.539", "train_nll_loss": "0.058", "train_loss_recon": "0.784", "train_loss_info_nce": "13.667", "train_ppl": "1.04", "train_wps": "2367.7", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "88", "train_lr": "2.2e-06", "train_gnorm": "17.617", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "158"}
[2025-07-10 22:43:52,997][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:43:52,999][fairseq.trainer][INFO] - begin training epoch 45
[2025-07-10 22:43:52,999][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:43:55,697][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:43:55,923][valid][INFO] - {"epoch": 45, "valid_loss": "20.135", "valid_nll_loss": "0.054", "valid_loss_recon": "0.741", "valid_loss_info_nce": "12.725", "valid_ppl": "1.04", "valid_wps": "80110.4", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "90", "valid_best_loss": "20.135"}
[2025-07-10 22:43:55,923][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 45 @ 90 updates
[2025-07-10 22:43:55,924][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint45.pt
[2025-07-10 22:43:56,312][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint45.pt
[2025-07-10 22:43:56,940][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint45.pt (epoch 45 @ 90 updates, score 20.135) (writing took 1.0163157469996804 seconds)
[2025-07-10 22:43:56,940][fairseq_cli.train][INFO] - end of epoch 45 (average epoch stats below)
[2025-07-10 22:43:56,941][train][INFO] - {"epoch": 45, "train_loss": "21.388", "train_nll_loss": "0.057", "train_loss_recon": "0.78", "train_loss_info_nce": "13.589", "train_ppl": "1.04", "train_wps": "2058.1", "train_ups": "0.5", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "90", "train_lr": "2.25e-06", "train_gnorm": "17.2", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "162"}
[2025-07-10 22:43:56,976][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:43:56,978][fairseq.trainer][INFO] - begin training epoch 46
[2025-07-10 22:43:56,978][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:43:59,675][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 46 @ 92 updates
[2025-07-10 22:43:59,675][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint46.pt
[2025-07-10 22:44:00,069][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint46.pt
[2025-07-10 22:44:00,416][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint46.pt (epoch 46 @ 92 updates, score None) (writing took 0.7416085310005656 seconds)
[2025-07-10 22:44:00,417][fairseq_cli.train][INFO] - end of epoch 46 (average epoch stats below)
[2025-07-10 22:44:00,418][train][INFO] - {"epoch": 46, "train_loss": "21.215", "train_nll_loss": "0.057", "train_loss_recon": "0.774", "train_loss_info_nce": "13.488", "train_ppl": "1.04", "train_wps": "2354.6", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "92", "train_lr": "2.3e-06", "train_gnorm": "16.552", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "165"}
[2025-07-10 22:44:00,452][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:44:00,453][fairseq.trainer][INFO] - begin training epoch 47
[2025-07-10 22:44:00,454][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:44:03,117][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 47 @ 94 updates
[2025-07-10 22:44:03,118][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint47.pt
[2025-07-10 22:44:03,509][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint47.pt
[2025-07-10 22:44:03,836][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint47.pt (epoch 47 @ 94 updates, score None) (writing took 0.7189158279998082 seconds)
[2025-07-10 22:44:03,837][fairseq_cli.train][INFO] - end of epoch 47 (average epoch stats below)
[2025-07-10 22:44:03,838][train][INFO] - {"epoch": 47, "train_loss": "21.081", "train_nll_loss": "0.057", "train_loss_recon": "0.77", "train_loss_info_nce": "13.355", "train_ppl": "1.04", "train_wps": "2393.7", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "94", "train_lr": "2.35e-06", "train_gnorm": "16.126", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "169"}
[2025-07-10 22:44:03,871][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:44:03,873][fairseq.trainer][INFO] - begin training epoch 48
[2025-07-10 22:44:03,873][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:44:06,571][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 48 @ 96 updates
[2025-07-10 22:44:06,571][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint48.pt
[2025-07-10 22:44:06,943][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint48.pt
[2025-07-10 22:44:07,286][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint48.pt (epoch 48 @ 96 updates, score None) (writing took 0.715519919000144 seconds)
[2025-07-10 22:44:07,287][fairseq_cli.train][INFO] - end of epoch 48 (average epoch stats below)
[2025-07-10 22:44:07,288][train][INFO] - {"epoch": 48, "train_loss": "20.922", "train_nll_loss": "0.056", "train_loss_recon": "0.764", "train_loss_info_nce": "13.281", "train_ppl": "1.04", "train_wps": "2372.8", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "96", "train_lr": "2.4e-06", "train_gnorm": "15.611", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "172"}
[2025-07-10 22:44:07,320][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:44:07,322][fairseq.trainer][INFO] - begin training epoch 49
[2025-07-10 22:44:07,322][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:44:10,022][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 49 @ 98 updates
[2025-07-10 22:44:10,022][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint49.pt
[2025-07-10 22:44:10,387][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint49.pt
[2025-07-10 22:44:10,696][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint49.pt (epoch 49 @ 98 updates, score None) (writing took 0.6740210529997057 seconds)
[2025-07-10 22:44:10,696][fairseq_cli.train][INFO] - end of epoch 49 (average epoch stats below)
[2025-07-10 22:44:10,697][train][INFO] - {"epoch": 49, "train_loss": "20.793", "train_nll_loss": "0.056", "train_loss_recon": "0.759", "train_loss_info_nce": "13.191", "train_ppl": "1.04", "train_wps": "2401.2", "train_ups": "0.59", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "98", "train_lr": "2.45e-06", "train_gnorm": "15.123", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "176"}
[2025-07-10 22:44:10,733][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:44:10,735][fairseq.trainer][INFO] - begin training epoch 50
[2025-07-10 22:44:10,735][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:44:13,410][train_inner][INFO] - {"epoch": 50, "update": 50.0, "loss": "24.125", "nll_loss": "0.065", "loss_recon": "0.833", "loss_info_nce": "15.784", "ppl": "1.05", "wps": "2303.5", "ups": "0.57", "wpb": "4092", "bsz": "494.5", "num_updates": "100", "lr": "2.5e-06", "gnorm": "36.845", "clip": "100", "loss_scale": "128", "train_wall": "92", "gb_free": "11.9", "wall": "178"}
[2025-07-10 22:44:13,410][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:44:13,638][valid][INFO] - {"epoch": 50, "valid_loss": "19.373", "valid_nll_loss": "0.052", "valid_loss_recon": "0.714", "valid_loss_info_nce": "12.231", "valid_ppl": "1.04", "valid_wps": "80549.6", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "100", "valid_best_loss": "19.373"}
[2025-07-10 22:44:13,639][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 50 @ 100 updates
[2025-07-10 22:44:13,639][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint50.pt
[2025-07-10 22:44:14,026][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint50.pt
[2025-07-10 22:44:14,875][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint50.pt (epoch 50 @ 100 updates, score 19.373) (writing took 1.2365557530001752 seconds)
[2025-07-10 22:44:14,875][fairseq_cli.train][INFO] - end of epoch 50 (average epoch stats below)
[2025-07-10 22:44:14,876][train][INFO] - {"epoch": 50, "train_loss": "20.638", "train_nll_loss": "0.055", "train_loss_recon": "0.754", "train_loss_info_nce": "13.086", "train_ppl": "1.04", "train_wps": "1958.6", "train_ups": "0.48", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "100", "train_lr": "2.5e-06", "train_gnorm": "14.683", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "180"}
[2025-07-10 22:44:14,912][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:44:14,914][fairseq.trainer][INFO] - begin training epoch 51
[2025-07-10 22:44:14,914][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:44:17,632][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 51 @ 102 updates
[2025-07-10 22:44:17,632][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint51.pt
[2025-07-10 22:44:18,021][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint51.pt
[2025-07-10 22:44:18,340][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint51.pt (epoch 51 @ 102 updates, score None) (writing took 0.708417446000567 seconds)
[2025-07-10 22:44:18,341][fairseq_cli.train][INFO] - end of epoch 51 (average epoch stats below)
[2025-07-10 22:44:18,342][train][INFO] - {"epoch": 51, "train_loss": "20.48", "train_nll_loss": "0.055", "train_loss_recon": "0.748", "train_loss_info_nce": "12.993", "train_ppl": "1.04", "train_wps": "2362.4", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "102", "train_lr": "2.55e-06", "train_gnorm": "14.238", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "183"}
[2025-07-10 22:44:18,381][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:44:18,382][fairseq.trainer][INFO] - begin training epoch 52
[2025-07-10 22:44:18,383][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:44:21,055][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 52 @ 104 updates
[2025-07-10 22:44:21,056][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint52.pt
[2025-07-10 22:44:21,425][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint52.pt
[2025-07-10 22:44:21,754][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint52.pt (epoch 52 @ 104 updates, score None) (writing took 0.6990069799994671 seconds)
[2025-07-10 22:44:21,754][fairseq_cli.train][INFO] - end of epoch 52 (average epoch stats below)
[2025-07-10 22:44:21,756][train][INFO] - {"epoch": 52, "train_loss": "20.353", "train_nll_loss": "0.055", "train_loss_recon": "0.743", "train_loss_info_nce": "12.888", "train_ppl": "1.04", "train_wps": "2398.1", "train_ups": "0.59", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "104", "train_lr": "2.6e-06", "train_gnorm": "13.901", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "187"}
[2025-07-10 22:44:21,797][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:44:21,799][fairseq.trainer][INFO] - begin training epoch 53
[2025-07-10 22:44:21,799][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:44:24,503][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 53 @ 106 updates
[2025-07-10 22:44:24,503][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint53.pt
[2025-07-10 22:44:24,888][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint53.pt
[2025-07-10 22:44:25,205][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint53.pt (epoch 53 @ 106 updates, score None) (writing took 0.7024825420003253 seconds)
[2025-07-10 22:44:25,206][fairseq_cli.train][INFO] - end of epoch 53 (average epoch stats below)
[2025-07-10 22:44:25,207][train][INFO] - {"epoch": 53, "train_loss": "20.191", "train_nll_loss": "0.054", "train_loss_recon": "0.736", "train_loss_info_nce": "12.807", "train_ppl": "1.04", "train_wps": "2372.1", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "106", "train_lr": "2.65e-06", "train_gnorm": "13.457", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "190"}
[2025-07-10 22:44:25,245][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:44:25,246][fairseq.trainer][INFO] - begin training epoch 54
[2025-07-10 22:44:25,247][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:44:27,965][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 54 @ 108 updates
[2025-07-10 22:44:27,966][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint54.pt
[2025-07-10 22:44:28,334][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint54.pt
[2025-07-10 22:44:28,662][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint54.pt (epoch 54 @ 108 updates, score None) (writing took 0.6964366520005569 seconds)
[2025-07-10 22:44:28,662][fairseq_cli.train][INFO] - end of epoch 54 (average epoch stats below)
[2025-07-10 22:44:28,663][train][INFO] - {"epoch": 54, "train_loss": "20.092", "train_nll_loss": "0.054", "train_loss_recon": "0.732", "train_loss_info_nce": "12.764", "train_ppl": "1.04", "train_wps": "2368.5", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "108", "train_lr": "2.7e-06", "train_gnorm": "13.156", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "194"}
[2025-07-10 22:44:28,701][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:44:28,702][fairseq.trainer][INFO] - begin training epoch 55
[2025-07-10 22:44:28,703][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:44:31,411][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:44:31,637][valid][INFO] - {"epoch": 55, "valid_loss": "18.49", "valid_nll_loss": "0.05", "valid_loss_recon": "0.679", "valid_loss_info_nce": "11.699", "valid_ppl": "1.04", "valid_wps": "77760.4", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "110", "valid_best_loss": "18.49"}
[2025-07-10 22:44:31,638][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 55 @ 110 updates
[2025-07-10 22:44:31,638][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint55.pt
[2025-07-10 22:44:32,020][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint55.pt
[2025-07-10 22:44:32,645][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint55.pt (epoch 55 @ 110 updates, score 18.49) (writing took 1.0068105310001556 seconds)
[2025-07-10 22:44:32,645][fairseq_cli.train][INFO] - end of epoch 55 (average epoch stats below)
[2025-07-10 22:44:32,646][train][INFO] - {"epoch": 55, "train_loss": "19.941", "train_nll_loss": "0.054", "train_loss_recon": "0.726", "train_loss_info_nce": "12.671", "train_ppl": "1.04", "train_wps": "2055.3", "train_ups": "0.5", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "110", "train_lr": "2.75e-06", "train_gnorm": "12.824", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "198"}
[2025-07-10 22:44:32,684][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:44:32,687][fairseq.trainer][INFO] - begin training epoch 56
[2025-07-10 22:44:32,687][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:44:35,407][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 56 @ 112 updates
[2025-07-10 22:44:35,408][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint56.pt
[2025-07-10 22:44:35,791][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint56.pt
[2025-07-10 22:44:36,056][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint56.pt (epoch 56 @ 112 updates, score None) (writing took 0.6488346969999839 seconds)
[2025-07-10 22:44:36,056][fairseq_cli.train][INFO] - end of epoch 56 (average epoch stats below)
[2025-07-10 22:44:36,057][train][INFO] - {"epoch": 56, "train_loss": "19.772", "train_nll_loss": "0.053", "train_loss_recon": "0.719", "train_loss_info_nce": "12.581", "train_ppl": "1.04", "train_wps": "2399.8", "train_ups": "0.59", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "112", "train_lr": "2.8e-06", "train_gnorm": "12.457", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "201"}
[2025-07-10 22:44:36,098][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:44:36,100][fairseq.trainer][INFO] - begin training epoch 57
[2025-07-10 22:44:36,101][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:44:38,852][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 57 @ 114 updates
[2025-07-10 22:44:38,852][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint57.pt
[2025-07-10 22:44:39,239][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint57.pt
[2025-07-10 22:44:39,568][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint57.pt (epoch 57 @ 114 updates, score None) (writing took 0.7155464480001683 seconds)
[2025-07-10 22:44:39,568][fairseq_cli.train][INFO] - end of epoch 57 (average epoch stats below)
[2025-07-10 22:44:39,569][train][INFO] - {"epoch": 57, "train_loss": "19.634", "train_nll_loss": "0.053", "train_loss_recon": "0.713", "train_loss_info_nce": "12.478", "train_ppl": "1.04", "train_wps": "2331.2", "train_ups": "0.57", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "114", "train_lr": "2.85e-06", "train_gnorm": "12.082", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "205"}
[2025-07-10 22:44:39,607][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:44:39,609][fairseq.trainer][INFO] - begin training epoch 58
[2025-07-10 22:44:39,609][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:44:42,288][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 58 @ 116 updates
[2025-07-10 22:44:42,288][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint58.pt
[2025-07-10 22:44:42,665][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint58.pt
[2025-07-10 22:44:43,007][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint58.pt (epoch 58 @ 116 updates, score None) (writing took 0.7193184640000254 seconds)
[2025-07-10 22:44:43,008][fairseq_cli.train][INFO] - end of epoch 58 (average epoch stats below)
[2025-07-10 22:44:43,009][train][INFO] - {"epoch": 58, "train_loss": "19.515", "train_nll_loss": "0.052", "train_loss_recon": "0.708", "train_loss_info_nce": "12.388", "train_ppl": "1.04", "train_wps": "2380", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "116", "train_lr": "2.9e-06", "train_gnorm": "11.892", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "208"}
[2025-07-10 22:44:43,048][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:44:43,050][fairseq.trainer][INFO] - begin training epoch 59
[2025-07-10 22:44:43,050][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:44:45,764][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 59 @ 118 updates
[2025-07-10 22:44:45,764][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint59.pt
[2025-07-10 22:44:46,153][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint59.pt
[2025-07-10 22:44:46,477][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint59.pt (epoch 59 @ 118 updates, score None) (writing took 0.7129646390003472 seconds)
[2025-07-10 22:44:46,477][fairseq_cli.train][INFO] - end of epoch 59 (average epoch stats below)
[2025-07-10 22:44:46,478][train][INFO] - {"epoch": 59, "train_loss": "19.36", "train_nll_loss": "0.052", "train_loss_recon": "0.701", "train_loss_info_nce": "12.318", "train_ppl": "1.04", "train_wps": "2359.6", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "118", "train_lr": "2.95e-06", "train_gnorm": "11.539", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "212"}
[2025-07-10 22:44:46,516][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:44:46,518][fairseq.trainer][INFO] - begin training epoch 60
[2025-07-10 22:44:46,518][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:44:49,219][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:44:49,445][valid][INFO] - {"epoch": 60, "valid_loss": "17.748", "valid_nll_loss": "0.048", "valid_loss_recon": "0.645", "valid_loss_info_nce": "11.296", "valid_ppl": "1.03", "valid_wps": "80447.5", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "120", "valid_best_loss": "17.748"}
[2025-07-10 22:44:49,446][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 60 @ 120 updates
[2025-07-10 22:44:49,446][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint60.pt
[2025-07-10 22:44:49,836][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint60.pt
[2025-07-10 22:44:50,471][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint60.pt (epoch 60 @ 120 updates, score 17.748) (writing took 1.0253158129999065 seconds)
[2025-07-10 22:44:50,471][fairseq_cli.train][INFO] - end of epoch 60 (average epoch stats below)
[2025-07-10 22:44:50,472][train][INFO] - {"epoch": 60, "train_loss": "19.203", "train_nll_loss": "0.052", "train_loss_recon": "0.695", "train_loss_info_nce": "12.243", "train_ppl": "1.04", "train_wps": "2049.5", "train_ups": "0.5", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "120", "train_lr": "3e-06", "train_gnorm": "11.228", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "216"}
[2025-07-10 22:44:50,507][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:44:50,508][fairseq.trainer][INFO] - begin training epoch 61
[2025-07-10 22:44:50,509][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:44:53,197][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 61 @ 122 updates
[2025-07-10 22:44:53,197][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint61.pt
[2025-07-10 22:44:53,596][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint61.pt
[2025-07-10 22:44:53,921][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint61.pt (epoch 61 @ 122 updates, score None) (writing took 0.7241495569996914 seconds)
[2025-07-10 22:44:53,921][fairseq_cli.train][INFO] - end of epoch 61 (average epoch stats below)
[2025-07-10 22:44:53,922][train][INFO] - {"epoch": 61, "train_loss": "19.081", "train_nll_loss": "0.051", "train_loss_recon": "0.688", "train_loss_info_nce": "12.175", "train_ppl": "1.04", "train_wps": "2372.9", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "122", "train_lr": "3.05e-06", "train_gnorm": "11.013", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "219"}
[2025-07-10 22:44:53,959][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:44:53,961][fairseq.trainer][INFO] - begin training epoch 62
[2025-07-10 22:44:53,961][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:44:56,649][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 62 @ 124 updates
[2025-07-10 22:44:56,650][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint62.pt
[2025-07-10 22:44:57,047][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint62.pt
[2025-07-10 22:44:57,367][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint62.pt (epoch 62 @ 124 updates, score None) (writing took 0.7177681189996292 seconds)
[2025-07-10 22:44:57,367][fairseq_cli.train][INFO] - end of epoch 62 (average epoch stats below)
[2025-07-10 22:44:57,368][train][INFO] - {"epoch": 62, "train_loss": "18.961", "train_nll_loss": "0.051", "train_loss_recon": "0.684", "train_loss_info_nce": "12.1", "train_ppl": "1.04", "train_wps": "2375.6", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "124", "train_lr": "3.1e-06", "train_gnorm": "10.767", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "222"}
[2025-07-10 22:44:57,401][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:44:57,403][fairseq.trainer][INFO] - begin training epoch 63
[2025-07-10 22:44:57,403][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:45:00,101][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 63 @ 126 updates
[2025-07-10 22:45:00,101][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint63.pt
[2025-07-10 22:45:00,486][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint63.pt
[2025-07-10 22:45:00,863][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint63.pt (epoch 63 @ 126 updates, score None) (writing took 0.7616857309994884 seconds)
[2025-07-10 22:45:00,863][fairseq_cli.train][INFO] - end of epoch 63 (average epoch stats below)
[2025-07-10 22:45:00,864][train][INFO] - {"epoch": 63, "train_loss": "18.788", "train_nll_loss": "0.051", "train_loss_recon": "0.675", "train_loss_info_nce": "12.018", "train_ppl": "1.04", "train_wps": "2342.1", "train_ups": "0.57", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "126", "train_lr": "3.15e-06", "train_gnorm": "10.48", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "226"}
[2025-07-10 22:45:00,902][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:45:00,904][fairseq.trainer][INFO] - begin training epoch 64
[2025-07-10 22:45:00,904][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:45:03,650][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 64 @ 128 updates
[2025-07-10 22:45:03,651][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint64.pt
[2025-07-10 22:45:04,049][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint64.pt
[2025-07-10 22:45:04,381][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint64.pt (epoch 64 @ 128 updates, score None) (writing took 0.7306620670005941 seconds)
[2025-07-10 22:45:04,381][fairseq_cli.train][INFO] - end of epoch 64 (average epoch stats below)
[2025-07-10 22:45:04,382][train][INFO] - {"epoch": 64, "train_loss": "18.653", "train_nll_loss": "0.05", "train_loss_recon": "0.669", "train_loss_info_nce": "11.952", "train_ppl": "1.04", "train_wps": "2326.8", "train_ups": "0.57", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "128", "train_lr": "3.2e-06", "train_gnorm": "10.357", "train_clip": "100", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "229"}
[2025-07-10 22:45:04,418][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:45:04,419][fairseq.trainer][INFO] - begin training epoch 65
[2025-07-10 22:45:04,420][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:45:07,066][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:45:07,295][valid][INFO] - {"epoch": 65, "valid_loss": "17.08", "valid_nll_loss": "0.046", "valid_loss_recon": "0.613", "valid_loss_info_nce": "10.946", "valid_ppl": "1.03", "valid_wps": "80620.6", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "130", "valid_best_loss": "17.08"}
[2025-07-10 22:45:07,295][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 65 @ 130 updates
[2025-07-10 22:45:07,296][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint65.pt
[2025-07-10 22:45:07,695][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint65.pt
[2025-07-10 22:45:08,360][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint65.pt (epoch 65 @ 130 updates, score 17.08) (writing took 1.0643070099995384 seconds)
[2025-07-10 22:45:08,360][fairseq_cli.train][INFO] - end of epoch 65 (average epoch stats below)
[2025-07-10 22:45:08,361][train][INFO] - {"epoch": 65, "train_loss": "18.545", "train_nll_loss": "0.05", "train_loss_recon": "0.664", "train_loss_info_nce": "11.882", "train_ppl": "1.04", "train_wps": "2057.5", "train_ups": "0.5", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "130", "train_lr": "3.25e-06", "train_gnorm": "10.026", "train_clip": "50", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "233"}
[2025-07-10 22:45:08,397][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:45:08,398][fairseq.trainer][INFO] - begin training epoch 66
[2025-07-10 22:45:08,398][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:45:11,076][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 66 @ 132 updates
[2025-07-10 22:45:11,077][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint66.pt
[2025-07-10 22:45:11,467][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint66.pt
[2025-07-10 22:45:11,792][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint66.pt (epoch 66 @ 132 updates, score None) (writing took 0.7159283170003619 seconds)
[2025-07-10 22:45:11,792][fairseq_cli.train][INFO] - end of epoch 66 (average epoch stats below)
[2025-07-10 22:45:11,793][train][INFO] - {"epoch": 66, "train_loss": "18.383", "train_nll_loss": "0.049", "train_loss_recon": "0.657", "train_loss_info_nce": "11.795", "train_ppl": "1.03", "train_wps": "2385", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "132", "train_lr": "3.3e-06", "train_gnorm": "9.828", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "237"}
[2025-07-10 22:45:11,827][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:45:11,829][fairseq.trainer][INFO] - begin training epoch 67
[2025-07-10 22:45:11,829][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:45:14,509][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 67 @ 134 updates
[2025-07-10 22:45:14,510][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint67.pt
[2025-07-10 22:45:14,879][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint67.pt
[2025-07-10 22:45:15,230][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint67.pt (epoch 67 @ 134 updates, score None) (writing took 0.7206556330002059 seconds)
[2025-07-10 22:45:15,230][fairseq_cli.train][INFO] - end of epoch 67 (average epoch stats below)
[2025-07-10 22:45:15,231][train][INFO] - {"epoch": 67, "train_loss": "18.246", "train_nll_loss": "0.049", "train_loss_recon": "0.651", "train_loss_info_nce": "11.747", "train_ppl": "1.03", "train_wps": "2381.5", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "134", "train_lr": "3.35e-06", "train_gnorm": "9.552", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "240"}
[2025-07-10 22:45:15,267][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:45:15,269][fairseq.trainer][INFO] - begin training epoch 68
[2025-07-10 22:45:15,269][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:45:17,958][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 68 @ 136 updates
[2025-07-10 22:45:17,958][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint68.pt
[2025-07-10 22:45:18,348][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint68.pt
[2025-07-10 22:45:18,677][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint68.pt (epoch 68 @ 136 updates, score None) (writing took 0.7189256640003805 seconds)
[2025-07-10 22:45:18,677][fairseq_cli.train][INFO] - end of epoch 68 (average epoch stats below)
[2025-07-10 22:45:18,678][train][INFO] - {"epoch": 68, "train_loss": "18.121", "train_nll_loss": "0.049", "train_loss_recon": "0.645", "train_loss_info_nce": "11.668", "train_ppl": "1.03", "train_wps": "2374.9", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "136", "train_lr": "3.4e-06", "train_gnorm": "9.361", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "244"}
[2025-07-10 22:45:18,712][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:45:18,714][fairseq.trainer][INFO] - begin training epoch 69
[2025-07-10 22:45:18,714][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:45:21,407][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 69 @ 138 updates
[2025-07-10 22:45:21,408][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint69.pt
[2025-07-10 22:45:21,779][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint69.pt
[2025-07-10 22:45:22,117][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint69.pt (epoch 69 @ 138 updates, score None) (writing took 0.7098033239999495 seconds)
[2025-07-10 22:45:22,117][fairseq_cli.train][INFO] - end of epoch 69 (average epoch stats below)
[2025-07-10 22:45:22,118][train][INFO] - {"epoch": 69, "train_loss": "18.005", "train_nll_loss": "0.048", "train_loss_recon": "0.64", "train_loss_info_nce": "11.608", "train_ppl": "1.03", "train_wps": "2379.8", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "138", "train_lr": "3.45e-06", "train_gnorm": "9.175", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "247"}
[2025-07-10 22:45:22,150][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:45:22,152][fairseq.trainer][INFO] - begin training epoch 70
[2025-07-10 22:45:22,152][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:45:24,873][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:45:25,098][valid][INFO] - {"epoch": 70, "valid_loss": "16.393", "valid_nll_loss": "0.044", "valid_loss_recon": "0.579", "valid_loss_info_nce": "10.602", "valid_ppl": "1.03", "valid_wps": "79816.8", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "140", "valid_best_loss": "16.393"}
[2025-07-10 22:45:25,098][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 70 @ 140 updates
[2025-07-10 22:45:25,099][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint70.pt
[2025-07-10 22:45:25,491][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint70.pt
[2025-07-10 22:45:26,125][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint70.pt (epoch 70 @ 140 updates, score 16.393) (writing took 1.0264543989997037 seconds)
[2025-07-10 22:45:26,125][fairseq_cli.train][INFO] - end of epoch 70 (average epoch stats below)
[2025-07-10 22:45:26,126][train][INFO] - {"epoch": 70, "train_loss": "17.861", "train_nll_loss": "0.048", "train_loss_recon": "0.631", "train_loss_info_nce": "11.519", "train_ppl": "1.03", "train_wps": "2042.6", "train_ups": "0.5", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "140", "train_lr": "3.5e-06", "train_gnorm": "9.097", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "251"}
[2025-07-10 22:45:26,160][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:45:26,162][fairseq.trainer][INFO] - begin training epoch 71
[2025-07-10 22:45:26,162][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:45:28,896][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 71 @ 142 updates
[2025-07-10 22:45:28,896][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint71.pt
[2025-07-10 22:45:29,269][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint71.pt
[2025-07-10 22:45:29,770][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint71.pt (epoch 71 @ 142 updates, score None) (writing took 0.8739392669995141 seconds)
[2025-07-10 22:45:29,770][fairseq_cli.train][INFO] - end of epoch 71 (average epoch stats below)
[2025-07-10 22:45:29,771][train][INFO] - {"epoch": 71, "train_loss": "17.717", "train_nll_loss": "0.048", "train_loss_recon": "0.625", "train_loss_info_nce": "11.466", "train_ppl": "1.03", "train_wps": "2245.6", "train_ups": "0.55", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "142", "train_lr": "3.55e-06", "train_gnorm": "8.757", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "255"}
[2025-07-10 22:45:29,808][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:45:29,810][fairseq.trainer][INFO] - begin training epoch 72
[2025-07-10 22:45:29,810][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:45:32,524][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 72 @ 144 updates
[2025-07-10 22:45:32,525][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint72.pt
[2025-07-10 22:45:32,922][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint72.pt
[2025-07-10 22:45:33,252][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint72.pt (epoch 72 @ 144 updates, score None) (writing took 0.7275717680004163 seconds)
[2025-07-10 22:45:33,252][fairseq_cli.train][INFO] - end of epoch 72 (average epoch stats below)
[2025-07-10 22:45:33,253][train][INFO] - {"epoch": 72, "train_loss": "17.583", "train_nll_loss": "0.047", "train_loss_recon": "0.619", "train_loss_info_nce": "11.386", "train_ppl": "1.03", "train_wps": "2351.1", "train_ups": "0.57", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "144", "train_lr": "3.6e-06", "train_gnorm": "8.52", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "258"}
[2025-07-10 22:45:33,287][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:45:33,289][fairseq.trainer][INFO] - begin training epoch 73
[2025-07-10 22:45:33,289][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:45:35,978][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 73 @ 146 updates
[2025-07-10 22:45:35,979][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint73.pt
[2025-07-10 22:45:36,352][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint73.pt
[2025-07-10 22:45:36,692][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint73.pt (epoch 73 @ 146 updates, score None) (writing took 0.713947659999576 seconds)
[2025-07-10 22:45:36,692][fairseq_cli.train][INFO] - end of epoch 73 (average epoch stats below)
[2025-07-10 22:45:36,694][train][INFO] - {"epoch": 73, "train_loss": "17.475", "train_nll_loss": "0.047", "train_loss_recon": "0.613", "train_loss_info_nce": "11.347", "train_ppl": "1.03", "train_wps": "2379.7", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "146", "train_lr": "3.65e-06", "train_gnorm": "8.381", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "262"}
[2025-07-10 22:45:36,734][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:45:36,736][fairseq.trainer][INFO] - begin training epoch 74
[2025-07-10 22:45:36,736][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:45:39,492][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 74 @ 148 updates
[2025-07-10 22:45:39,492][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint74.pt
[2025-07-10 22:45:39,895][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint74.pt
[2025-07-10 22:45:40,240][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint74.pt (epoch 74 @ 148 updates, score None) (writing took 0.7478225890008616 seconds)
[2025-07-10 22:45:40,240][fairseq_cli.train][INFO] - end of epoch 74 (average epoch stats below)
[2025-07-10 22:45:40,241][train][INFO] - {"epoch": 74, "train_loss": "17.33", "train_nll_loss": "0.047", "train_loss_recon": "0.606", "train_loss_info_nce": "11.256", "train_ppl": "1.03", "train_wps": "2307.6", "train_ups": "0.56", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "148", "train_lr": "3.7e-06", "train_gnorm": "8.118", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "265"}
[2025-07-10 22:45:40,277][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:45:40,279][fairseq.trainer][INFO] - begin training epoch 75
[2025-07-10 22:45:40,279][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:45:42,996][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:45:43,217][valid][INFO] - {"epoch": 75, "valid_loss": "15.777", "valid_nll_loss": "0.042", "valid_loss_recon": "0.548", "valid_loss_info_nce": "10.293", "valid_ppl": "1.03", "valid_wps": "80594", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "150", "valid_best_loss": "15.777"}
[2025-07-10 22:45:43,217][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 75 @ 150 updates
[2025-07-10 22:45:43,218][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint75.pt
[2025-07-10 22:45:43,620][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint75.pt
[2025-07-10 22:45:44,288][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint75.pt (epoch 75 @ 150 updates, score 15.777) (writing took 1.0710066199999346 seconds)
[2025-07-10 22:45:44,288][fairseq_cli.train][INFO] - end of epoch 75 (average epoch stats below)
[2025-07-10 22:45:44,290][train][INFO] - {"epoch": 75, "train_loss": "17.253", "train_nll_loss": "0.046", "train_loss_recon": "0.604", "train_loss_info_nce": "11.225", "train_ppl": "1.03", "train_wps": "2022.1", "train_ups": "0.49", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "150", "train_lr": "3.75e-06", "train_gnorm": "8.099", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "269"}
[2025-07-10 22:45:44,323][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:45:44,325][fairseq.trainer][INFO] - begin training epoch 76
[2025-07-10 22:45:44,325][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:45:47,062][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 76 @ 152 updates
[2025-07-10 22:45:47,062][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint76.pt
[2025-07-10 22:45:47,457][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint76.pt
[2025-07-10 22:45:47,790][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint76.pt (epoch 76 @ 152 updates, score None) (writing took 0.7281001640003524 seconds)
[2025-07-10 22:45:47,790][fairseq_cli.train][INFO] - end of epoch 76 (average epoch stats below)
[2025-07-10 22:45:47,791][train][INFO] - {"epoch": 76, "train_loss": "17.094", "train_nll_loss": "0.046", "train_loss_recon": "0.594", "train_loss_info_nce": "11.147", "train_ppl": "1.03", "train_wps": "2337.8", "train_ups": "0.57", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "152", "train_lr": "3.8e-06", "train_gnorm": "7.785", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "273"}
[2025-07-10 22:45:47,824][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:45:47,826][fairseq.trainer][INFO] - begin training epoch 77
[2025-07-10 22:45:47,826][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:45:50,468][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 77 @ 154 updates
[2025-07-10 22:45:50,468][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint77.pt
[2025-07-10 22:45:50,862][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint77.pt
[2025-07-10 22:45:51,242][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint77.pt (epoch 77 @ 154 updates, score None) (writing took 0.7739099059999717 seconds)
[2025-07-10 22:45:51,242][fairseq_cli.train][INFO] - end of epoch 77 (average epoch stats below)
[2025-07-10 22:45:51,243][train][INFO] - {"epoch": 77, "train_loss": "17.021", "train_nll_loss": "0.046", "train_loss_recon": "0.59", "train_loss_info_nce": "11.066", "train_ppl": "1.03", "train_wps": "2371.8", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "154", "train_lr": "3.85e-06", "train_gnorm": "7.644", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "276"}
[2025-07-10 22:45:51,280][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:45:51,282][fairseq.trainer][INFO] - begin training epoch 78
[2025-07-10 22:45:51,282][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:45:54,038][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 78 @ 156 updates
[2025-07-10 22:45:54,038][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint78.pt
[2025-07-10 22:45:54,420][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint78.pt
[2025-07-10 22:45:54,822][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint78.pt (epoch 78 @ 156 updates, score None) (writing took 0.7841118259993891 seconds)
[2025-07-10 22:45:54,822][fairseq_cli.train][INFO] - end of epoch 78 (average epoch stats below)
[2025-07-10 22:45:54,823][train][INFO] - {"epoch": 78, "train_loss": "16.876", "train_nll_loss": "0.045", "train_loss_recon": "0.584", "train_loss_info_nce": "11.015", "train_ppl": "1.03", "train_wps": "2286.4", "train_ups": "0.56", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "156", "train_lr": "3.9e-06", "train_gnorm": "7.493", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "280"}
[2025-07-10 22:45:54,859][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:45:54,861][fairseq.trainer][INFO] - begin training epoch 79
[2025-07-10 22:45:54,861][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:45:57,570][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 79 @ 158 updates
[2025-07-10 22:45:57,571][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint79.pt
[2025-07-10 22:45:57,962][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint79.pt
[2025-07-10 22:45:58,310][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint79.pt (epoch 79 @ 158 updates, score None) (writing took 0.7398915789999592 seconds)
[2025-07-10 22:45:58,311][fairseq_cli.train][INFO] - end of epoch 79 (average epoch stats below)
[2025-07-10 22:45:58,312][train][INFO] - {"epoch": 79, "train_loss": "16.767", "train_nll_loss": "0.045", "train_loss_recon": "0.578", "train_loss_info_nce": "10.944", "train_ppl": "1.03", "train_wps": "2346.9", "train_ups": "0.57", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "158", "train_lr": "3.95e-06", "train_gnorm": "7.523", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "283"}
[2025-07-10 22:45:58,345][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:45:58,347][fairseq.trainer][INFO] - begin training epoch 80
[2025-07-10 22:45:58,347][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:46:01,048][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:46:01,269][valid][INFO] - {"epoch": 80, "valid_loss": "15.178", "valid_nll_loss": "0.041", "valid_loss_recon": "0.518", "valid_loss_info_nce": "9.996", "valid_ppl": "1.03", "valid_wps": "80044.3", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "160", "valid_best_loss": "15.178"}
[2025-07-10 22:46:01,270][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 80 @ 160 updates
[2025-07-10 22:46:01,270][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint80.pt
[2025-07-10 22:46:01,660][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint80.pt
[2025-07-10 22:46:02,317][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint80.pt (epoch 80 @ 160 updates, score 15.178) (writing took 1.0474601510004504 seconds)
[2025-07-10 22:46:02,317][fairseq_cli.train][INFO] - end of epoch 80 (average epoch stats below)
[2025-07-10 22:46:02,318][train][INFO] - {"epoch": 80, "train_loss": "16.672", "train_nll_loss": "0.045", "train_loss_recon": "0.575", "train_loss_info_nce": "10.925", "train_ppl": "1.03", "train_wps": "2043", "train_ups": "0.5", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "160", "train_lr": "4e-06", "train_gnorm": "7.15", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "287"}
[2025-07-10 22:46:02,354][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:46:02,356][fairseq.trainer][INFO] - begin training epoch 81
[2025-07-10 22:46:02,356][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:46:05,065][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 81 @ 162 updates
[2025-07-10 22:46:05,065][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint81.pt
[2025-07-10 22:46:05,453][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint81.pt
[2025-07-10 22:46:05,778][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint81.pt (epoch 81 @ 162 updates, score None) (writing took 0.7130742749995989 seconds)
[2025-07-10 22:46:05,778][fairseq_cli.train][INFO] - end of epoch 81 (average epoch stats below)
[2025-07-10 22:46:05,779][train][INFO] - {"epoch": 81, "train_loss": "16.53", "train_nll_loss": "0.044", "train_loss_recon": "0.568", "train_loss_info_nce": "10.845", "train_ppl": "1.03", "train_wps": "2365.6", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "162", "train_lr": "4.05e-06", "train_gnorm": "7.229", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "291"}
[2025-07-10 22:46:05,814][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:46:05,816][fairseq.trainer][INFO] - begin training epoch 82
[2025-07-10 22:46:05,816][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:46:08,526][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 82 @ 164 updates
[2025-07-10 22:46:08,527][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint82.pt
[2025-07-10 22:46:08,898][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint82.pt
[2025-07-10 22:46:09,240][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint82.pt (epoch 82 @ 164 updates, score None) (writing took 0.7139922050000678 seconds)
[2025-07-10 22:46:09,240][fairseq_cli.train][INFO] - end of epoch 82 (average epoch stats below)
[2025-07-10 22:46:09,241][train][INFO] - {"epoch": 82, "train_loss": "16.455", "train_nll_loss": "0.044", "train_loss_recon": "0.565", "train_loss_info_nce": "10.801", "train_ppl": "1.03", "train_wps": "2364.4", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "164", "train_lr": "4.1e-06", "train_gnorm": "6.862", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "294"}
[2025-07-10 22:46:09,278][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:46:09,280][fairseq.trainer][INFO] - begin training epoch 83
[2025-07-10 22:46:09,280][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:46:11,988][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 83 @ 166 updates
[2025-07-10 22:46:11,988][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint83.pt
[2025-07-10 22:46:12,379][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint83.pt
[2025-07-10 22:46:12,710][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint83.pt (epoch 83 @ 166 updates, score None) (writing took 0.7224346310003966 seconds)
[2025-07-10 22:46:12,711][fairseq_cli.train][INFO] - end of epoch 83 (average epoch stats below)
[2025-07-10 22:46:12,712][train][INFO] - {"epoch": 83, "train_loss": "16.349", "train_nll_loss": "0.044", "train_loss_recon": "0.559", "train_loss_info_nce": "10.752", "train_ppl": "1.03", "train_wps": "2358.9", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "166", "train_lr": "4.15e-06", "train_gnorm": "7.182", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "298"}
[2025-07-10 22:46:12,747][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:46:12,749][fairseq.trainer][INFO] - begin training epoch 84
[2025-07-10 22:46:12,749][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:46:15,502][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 84 @ 168 updates
[2025-07-10 22:46:15,502][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint84.pt
[2025-07-10 22:46:15,874][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint84.pt
[2025-07-10 22:46:16,113][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint84.pt (epoch 84 @ 168 updates, score None) (writing took 0.6113479099994947 seconds)
[2025-07-10 22:46:16,113][fairseq_cli.train][INFO] - end of epoch 84 (average epoch stats below)
[2025-07-10 22:46:16,115][train][INFO] - {"epoch": 84, "train_loss": "16.217", "train_nll_loss": "0.044", "train_loss_recon": "0.552", "train_loss_info_nce": "10.681", "train_ppl": "1.03", "train_wps": "2406", "train_ups": "0.59", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "168", "train_lr": "4.2e-06", "train_gnorm": "6.66", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "301"}
[2025-07-10 22:46:16,148][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:46:16,150][fairseq.trainer][INFO] - begin training epoch 85
[2025-07-10 22:46:16,150][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:46:18,815][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:46:19,044][valid][INFO] - {"epoch": 85, "valid_loss": "14.594", "valid_nll_loss": "0.039", "valid_loss_recon": "0.488", "valid_loss_info_nce": "9.71", "valid_ppl": "1.03", "valid_wps": "80940.5", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "170", "valid_best_loss": "14.594"}
[2025-07-10 22:46:19,045][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 85 @ 170 updates
[2025-07-10 22:46:19,045][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint85.pt
[2025-07-10 22:46:19,445][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint85.pt
[2025-07-10 22:46:20,157][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint85.pt (epoch 85 @ 170 updates, score 14.594) (writing took 1.1122531529999833 seconds)
[2025-07-10 22:46:20,158][fairseq_cli.train][INFO] - end of epoch 85 (average epoch stats below)
[2025-07-10 22:46:20,159][train][INFO] - {"epoch": 85, "train_loss": "16.17", "train_nll_loss": "0.043", "train_loss_recon": "0.551", "train_loss_info_nce": "10.655", "train_ppl": "1.03", "train_wps": "2024.3", "train_ups": "0.49", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "170", "train_lr": "4.25e-06", "train_gnorm": "6.546", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "12", "train_wall": "305"}
[2025-07-10 22:46:20,194][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:46:20,195][fairseq.trainer][INFO] - begin training epoch 86
[2025-07-10 22:46:20,196][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:46:22,941][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 86 @ 172 updates
[2025-07-10 22:46:22,941][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint86.pt
[2025-07-10 22:46:23,309][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint86.pt
[2025-07-10 22:46:23,743][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint86.pt (epoch 86 @ 172 updates, score None) (writing took 0.8019489320004141 seconds)
[2025-07-10 22:46:23,743][fairseq_cli.train][INFO] - end of epoch 86 (average epoch stats below)
[2025-07-10 22:46:23,744][train][INFO] - {"epoch": 86, "train_loss": "16.057", "train_nll_loss": "0.043", "train_loss_recon": "0.545", "train_loss_info_nce": "10.61", "train_ppl": "1.03", "train_wps": "2283.1", "train_ups": "0.56", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "172", "train_lr": "4.3e-06", "train_gnorm": "6.574", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "309"}
[2025-07-10 22:46:23,782][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:46:23,783][fairseq.trainer][INFO] - begin training epoch 87
[2025-07-10 22:46:23,783][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:46:26,517][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 87 @ 174 updates
[2025-07-10 22:46:26,518][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint87.pt
[2025-07-10 22:46:26,921][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint87.pt
[2025-07-10 22:46:27,257][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint87.pt (epoch 87 @ 174 updates, score None) (writing took 0.739794647999588 seconds)
[2025-07-10 22:46:27,257][fairseq_cli.train][INFO] - end of epoch 87 (average epoch stats below)
[2025-07-10 22:46:27,258][train][INFO] - {"epoch": 87, "train_loss": "15.917", "train_nll_loss": "0.043", "train_loss_recon": "0.538", "train_loss_info_nce": "10.537", "train_ppl": "1.03", "train_wps": "2329.7", "train_ups": "0.57", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "174", "train_lr": "4.35e-06", "train_gnorm": "6.194", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "312"}
[2025-07-10 22:46:27,293][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:46:27,295][fairseq.trainer][INFO] - begin training epoch 88
[2025-07-10 22:46:27,295][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:46:29,982][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 88 @ 176 updates
[2025-07-10 22:46:29,983][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint88.pt
[2025-07-10 22:46:30,356][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint88.pt
[2025-07-10 22:46:30,700][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint88.pt (epoch 88 @ 176 updates, score None) (writing took 0.7179089949995614 seconds)
[2025-07-10 22:46:30,700][fairseq_cli.train][INFO] - end of epoch 88 (average epoch stats below)
[2025-07-10 22:46:30,702][train][INFO] - {"epoch": 88, "train_loss": "15.865", "train_nll_loss": "0.043", "train_loss_recon": "0.535", "train_loss_info_nce": "10.487", "train_ppl": "1.03", "train_wps": "2377.6", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "176", "train_lr": "4.4e-06", "train_gnorm": "6.085", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "316"}
[2025-07-10 22:46:30,741][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:46:30,742][fairseq.trainer][INFO] - begin training epoch 89
[2025-07-10 22:46:30,743][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:46:33,480][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 89 @ 178 updates
[2025-07-10 22:46:33,480][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint89.pt
[2025-07-10 22:46:33,874][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint89.pt
[2025-07-10 22:46:34,205][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint89.pt (epoch 89 @ 178 updates, score None) (writing took 0.7251820379997298 seconds)
[2025-07-10 22:46:34,205][fairseq_cli.train][INFO] - end of epoch 89 (average epoch stats below)
[2025-07-10 22:46:34,206][train][INFO] - {"epoch": 89, "train_loss": "15.767", "train_nll_loss": "0.042", "train_loss_recon": "0.53", "train_loss_info_nce": "10.448", "train_ppl": "1.03", "train_wps": "2335.9", "train_ups": "0.57", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "178", "train_lr": "4.45e-06", "train_gnorm": "6.084", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "319"}
[2025-07-10 22:46:34,243][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:46:34,244][fairseq.trainer][INFO] - begin training epoch 90
[2025-07-10 22:46:34,245][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:46:36,966][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:46:37,194][valid][INFO] - {"epoch": 90, "valid_loss": "14.239", "valid_nll_loss": "0.038", "valid_loss_recon": "0.472", "valid_loss_info_nce": "9.523", "valid_ppl": "1.03", "valid_wps": "80700.1", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "180", "valid_best_loss": "14.239"}
[2025-07-10 22:46:37,195][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 90 @ 180 updates
[2025-07-10 22:46:37,195][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint90.pt
[2025-07-10 22:46:37,576][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint90.pt
[2025-07-10 22:46:38,414][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint90.pt (epoch 90 @ 180 updates, score 14.239) (writing took 1.2195963549993394 seconds)
[2025-07-10 22:46:38,414][fairseq_cli.train][INFO] - end of epoch 90 (average epoch stats below)
[2025-07-10 22:46:38,416][train][INFO] - {"epoch": 90, "train_loss": "15.665", "train_nll_loss": "0.042", "train_loss_recon": "0.526", "train_loss_info_nce": "10.398", "train_ppl": "1.03", "train_wps": "1944.9", "train_ups": "0.48", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "180", "train_lr": "4.5e-06", "train_gnorm": "5.744", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "323"}
[2025-07-10 22:46:38,456][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:46:38,458][fairseq.trainer][INFO] - begin training epoch 91
[2025-07-10 22:46:38,458][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:46:41,180][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 91 @ 182 updates
[2025-07-10 22:46:41,181][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint91.pt
[2025-07-10 22:46:41,585][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint91.pt
[2025-07-10 22:46:41,895][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint91.pt (epoch 91 @ 182 updates, score None) (writing took 0.715032457000234 seconds)
[2025-07-10 22:46:41,896][fairseq_cli.train][INFO] - end of epoch 91 (average epoch stats below)
[2025-07-10 22:46:41,897][train][INFO] - {"epoch": 91, "train_loss": "15.566", "train_nll_loss": "0.042", "train_loss_recon": "0.521", "train_loss_info_nce": "10.344", "train_ppl": "1.03", "train_wps": "2351.7", "train_ups": "0.57", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "182", "train_lr": "4.55e-06", "train_gnorm": "5.494", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "327"}
[2025-07-10 22:46:41,933][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:46:41,936][fairseq.trainer][INFO] - begin training epoch 92
[2025-07-10 22:46:41,936][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:46:44,701][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 92 @ 184 updates
[2025-07-10 22:46:44,701][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint92.pt
[2025-07-10 22:46:45,100][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint92.pt
[2025-07-10 22:46:45,435][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint92.pt (epoch 92 @ 184 updates, score None) (writing took 0.7342212430003201 seconds)
[2025-07-10 22:46:45,435][fairseq_cli.train][INFO] - end of epoch 92 (average epoch stats below)
[2025-07-10 22:46:45,436][train][INFO] - {"epoch": 92, "train_loss": "15.488", "train_nll_loss": "0.042", "train_loss_recon": "0.517", "train_loss_info_nce": "10.304", "train_ppl": "1.03", "train_wps": "2312.9", "train_ups": "0.57", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "184", "train_lr": "4.6e-06", "train_gnorm": "5.305", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "330"}
[2025-07-10 22:46:45,468][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:46:45,470][fairseq.trainer][INFO] - begin training epoch 93
[2025-07-10 22:46:45,470][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:46:48,181][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 93 @ 186 updates
[2025-07-10 22:46:48,181][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint93.pt
[2025-07-10 22:46:48,572][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint93.pt
[2025-07-10 22:46:48,933][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint93.pt (epoch 93 @ 186 updates, score None) (writing took 0.7522460559994215 seconds)
[2025-07-10 22:46:48,933][fairseq_cli.train][INFO] - end of epoch 93 (average epoch stats below)
[2025-07-10 22:46:48,934][train][INFO] - {"epoch": 93, "train_loss": "15.4", "train_nll_loss": "0.041", "train_loss_recon": "0.513", "train_loss_info_nce": "10.257", "train_ppl": "1.03", "train_wps": "2340.2", "train_ups": "0.57", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "186", "train_lr": "4.65e-06", "train_gnorm": "5.242", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "334"}
[2025-07-10 22:46:48,966][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:46:48,968][fairseq.trainer][INFO] - begin training epoch 94
[2025-07-10 22:46:48,968][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:46:51,699][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 94 @ 188 updates
[2025-07-10 22:46:51,699][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint94.pt
[2025-07-10 22:46:52,086][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint94.pt
[2025-07-10 22:46:52,417][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint94.pt (epoch 94 @ 188 updates, score None) (writing took 0.7181595730007757 seconds)
[2025-07-10 22:46:52,417][fairseq_cli.train][INFO] - end of epoch 94 (average epoch stats below)
[2025-07-10 22:46:52,418][train][INFO] - {"epoch": 94, "train_loss": "15.326", "train_nll_loss": "0.041", "train_loss_recon": "0.509", "train_loss_info_nce": "10.219", "train_ppl": "1.03", "train_wps": "2349.8", "train_ups": "0.57", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "188", "train_lr": "4.7e-06", "train_gnorm": "5.115", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "337"}
[2025-07-10 22:46:52,456][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:46:52,457][fairseq.trainer][INFO] - begin training epoch 95
[2025-07-10 22:46:52,458][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:46:55,193][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:46:55,413][valid][INFO] - {"epoch": 95, "valid_loss": "13.87", "valid_nll_loss": "0.037", "valid_loss_recon": "0.453", "valid_loss_info_nce": "9.34", "valid_ppl": "1.03", "valid_wps": "79064.7", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "190", "valid_best_loss": "13.87"}
[2025-07-10 22:46:55,414][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 95 @ 190 updates
[2025-07-10 22:46:55,415][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint95.pt
[2025-07-10 22:46:55,798][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint95.pt
[2025-07-10 22:46:56,469][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint95.pt (epoch 95 @ 190 updates, score 13.87) (writing took 1.0546397789994444 seconds)
[2025-07-10 22:46:56,469][fairseq_cli.train][INFO] - end of epoch 95 (average epoch stats below)
[2025-07-10 22:46:56,470][train][INFO] - {"epoch": 95, "train_loss": "15.25", "train_nll_loss": "0.041", "train_loss_recon": "0.505", "train_loss_info_nce": "10.189", "train_ppl": "1.03", "train_wps": "2020.3", "train_ups": "0.49", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "190", "train_lr": "4.75e-06", "train_gnorm": "5.251", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "342"}
[2025-07-10 22:46:56,507][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:46:56,509][fairseq.trainer][INFO] - begin training epoch 96
[2025-07-10 22:46:56,509][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:46:59,231][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 96 @ 192 updates
[2025-07-10 22:46:59,231][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint96.pt
[2025-07-10 22:46:59,612][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint96.pt
[2025-07-10 22:46:59,965][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint96.pt (epoch 96 @ 192 updates, score None) (writing took 0.7337912159991902 seconds)
[2025-07-10 22:46:59,965][fairseq_cli.train][INFO] - end of epoch 96 (average epoch stats below)
[2025-07-10 22:46:59,966][train][INFO] - {"epoch": 96, "train_loss": "15.148", "train_nll_loss": "0.041", "train_loss_recon": "0.502", "train_loss_info_nce": "10.125", "train_ppl": "1.03", "train_wps": "2341.6", "train_ups": "0.57", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "192", "train_lr": "4.8e-06", "train_gnorm": "5.659", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "345"}
[2025-07-10 22:47:00,005][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:47:00,007][fairseq.trainer][INFO] - begin training epoch 97
[2025-07-10 22:47:00,007][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:47:02,734][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 97 @ 194 updates
[2025-07-10 22:47:02,734][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint97.pt
[2025-07-10 22:47:03,102][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint97.pt
[2025-07-10 22:47:03,451][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint97.pt (epoch 97 @ 194 updates, score None) (writing took 0.7175445630000468 seconds)
[2025-07-10 22:47:03,452][fairseq_cli.train][INFO] - end of epoch 97 (average epoch stats below)
[2025-07-10 22:47:03,453][train][INFO] - {"epoch": 97, "train_loss": "15.095", "train_nll_loss": "0.041", "train_loss_recon": "0.499", "train_loss_info_nce": "10.099", "train_ppl": "1.03", "train_wps": "2347.8", "train_ups": "0.57", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "194", "train_lr": "4.85e-06", "train_gnorm": "4.919", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "348"}
[2025-07-10 22:47:03,492][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:47:03,494][fairseq.trainer][INFO] - begin training epoch 98
[2025-07-10 22:47:03,494][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:47:06,180][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 98 @ 196 updates
[2025-07-10 22:47:06,180][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint98.pt
[2025-07-10 22:47:06,569][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint98.pt
[2025-07-10 22:47:06,900][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint98.pt (epoch 98 @ 196 updates, score None) (writing took 0.7201927639998758 seconds)
[2025-07-10 22:47:06,901][fairseq_cli.train][INFO] - end of epoch 98 (average epoch stats below)
[2025-07-10 22:47:06,901][train][INFO] - {"epoch": 98, "train_loss": "15.029", "train_nll_loss": "0.04", "train_loss_recon": "0.496", "train_loss_info_nce": "10.071", "train_ppl": "1.03", "train_wps": "2373.5", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "196", "train_lr": "4.9e-06", "train_gnorm": "4.734", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "352"}
[2025-07-10 22:47:06,939][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:47:06,940][fairseq.trainer][INFO] - begin training epoch 99
[2025-07-10 22:47:06,941][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:47:09,605][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 99 @ 198 updates
[2025-07-10 22:47:09,606][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint99.pt
[2025-07-10 22:47:09,986][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint99.pt
[2025-07-10 22:47:10,354][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint99.pt (epoch 99 @ 198 updates, score None) (writing took 0.7490312449999692 seconds)
[2025-07-10 22:47:10,355][fairseq_cli.train][INFO] - end of epoch 99 (average epoch stats below)
[2025-07-10 22:47:10,355][train][INFO] - {"epoch": 99, "train_loss": "14.991", "train_nll_loss": "0.04", "train_loss_recon": "0.493", "train_loss_info_nce": "10.028", "train_ppl": "1.03", "train_wps": "2370", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "198", "train_lr": "4.95e-06", "train_gnorm": "5.767", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "355"}
[2025-07-10 22:47:10,393][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:47:10,395][fairseq.trainer][INFO] - begin training epoch 100
[2025-07-10 22:47:10,395][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:47:13,165][train_inner][INFO] - {"epoch": 100, "update": 100.0, "loss": "17.362", "nll_loss": "0.047", "loss_recon": "0.607", "loss_info_nce": "11.284", "ppl": "1.03", "wps": "2276.4", "ups": "0.56", "wpb": "4092", "bsz": "494.5", "num_updates": "200", "lr": "5e-06", "gnorm": "8.47", "clip": "29", "loss_scale": "128", "train_wall": "91", "gb_free": "11.9", "wall": "358"}
[2025-07-10 22:47:13,165][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:47:13,384][valid][INFO] - {"epoch": 100, "valid_loss": "13.514", "valid_nll_loss": "0.036", "valid_loss_recon": "0.433", "valid_loss_info_nce": "9.189", "valid_ppl": "1.03", "valid_wps": "79971.2", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "200", "valid_best_loss": "13.514"}
[2025-07-10 22:47:13,385][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 100 @ 200 updates
[2025-07-10 22:47:13,385][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint100.pt
[2025-07-10 22:47:13,768][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint100.pt
[2025-07-10 22:47:14,416][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint100.pt (epoch 100 @ 200 updates, score 13.514) (writing took 1.0314119869999558 seconds)
[2025-07-10 22:47:14,417][fairseq_cli.train][INFO] - end of epoch 100 (average epoch stats below)
[2025-07-10 22:47:14,418][train][INFO] - {"epoch": 100, "train_loss": "14.863", "train_nll_loss": "0.04", "train_loss_recon": "0.488", "train_loss_info_nce": "9.985", "train_ppl": "1.03", "train_wps": "2015.2", "train_ups": "0.49", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "200", "train_lr": "5e-06", "train_gnorm": "4.964", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "359"}
[2025-07-10 22:47:14,454][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:47:14,456][fairseq.trainer][INFO] - begin training epoch 101
[2025-07-10 22:47:14,456][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:47:17,186][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 101 @ 202 updates
[2025-07-10 22:47:17,186][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint101.pt
[2025-07-10 22:47:17,556][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint101.pt
[2025-07-10 22:47:17,901][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint101.pt (epoch 101 @ 202 updates, score None) (writing took 0.7151221119993352 seconds)
[2025-07-10 22:47:17,901][fairseq_cli.train][INFO] - end of epoch 101 (average epoch stats below)
[2025-07-10 22:47:17,902][train][INFO] - {"epoch": 101, "train_loss": "14.833", "train_nll_loss": "0.04", "train_loss_recon": "0.487", "train_loss_info_nce": "9.955", "train_ppl": "1.03", "train_wps": "2349.4", "train_ups": "0.57", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "202", "train_lr": "5.05e-06", "train_gnorm": "5.063", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "363"}
[2025-07-10 22:47:17,936][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:47:17,937][fairseq.trainer][INFO] - begin training epoch 102
[2025-07-10 22:47:17,937][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:47:20,672][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 102 @ 204 updates
[2025-07-10 22:47:20,672][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint102.pt
[2025-07-10 22:47:21,027][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint102.pt
[2025-07-10 22:47:21,355][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint102.pt (epoch 102 @ 204 updates, score None) (writing took 0.6828203699997175 seconds)
[2025-07-10 22:47:21,355][fairseq_cli.train][INFO] - end of epoch 102 (average epoch stats below)
[2025-07-10 22:47:21,356][train][INFO] - {"epoch": 102, "train_loss": "14.73", "train_nll_loss": "0.04", "train_loss_recon": "0.481", "train_loss_info_nce": "9.916", "train_ppl": "1.03", "train_wps": "2370.1", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "204", "train_lr": "5.1e-06", "train_gnorm": "5.125", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "366"}
[2025-07-10 22:47:21,392][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:47:21,394][fairseq.trainer][INFO] - begin training epoch 103
[2025-07-10 22:47:21,394][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:47:24,102][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 103 @ 206 updates
[2025-07-10 22:47:24,103][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint103.pt
[2025-07-10 22:47:24,458][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint103.pt
[2025-07-10 22:47:24,792][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint103.pt (epoch 103 @ 206 updates, score None) (writing took 0.6896815449999849 seconds)
[2025-07-10 22:47:24,792][fairseq_cli.train][INFO] - end of epoch 103 (average epoch stats below)
[2025-07-10 22:47:24,794][train][INFO] - {"epoch": 103, "train_loss": "14.69", "train_nll_loss": "0.039", "train_loss_recon": "0.481", "train_loss_info_nce": "9.891", "train_ppl": "1.03", "train_wps": "2381.5", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "206", "train_lr": "5.15e-06", "train_gnorm": "6.517", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "370"}
[2025-07-10 22:47:24,828][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:47:24,830][fairseq.trainer][INFO] - begin training epoch 104
[2025-07-10 22:47:24,830][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:47:27,532][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 104 @ 208 updates
[2025-07-10 22:47:27,533][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint104.pt
[2025-07-10 22:47:27,895][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint104.pt
[2025-07-10 22:47:28,225][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint104.pt (epoch 104 @ 208 updates, score None) (writing took 0.6928535910001301 seconds)
[2025-07-10 22:47:28,225][fairseq_cli.train][INFO] - end of epoch 104 (average epoch stats below)
[2025-07-10 22:47:28,227][train][INFO] - {"epoch": 104, "train_loss": "14.609", "train_nll_loss": "0.039", "train_loss_recon": "0.477", "train_loss_info_nce": "9.86", "train_ppl": "1.03", "train_wps": "2384.8", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "208", "train_lr": "5.2e-06", "train_gnorm": "6.099", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "373"}
[2025-07-10 22:47:28,262][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:47:28,263][fairseq.trainer][INFO] - begin training epoch 105
[2025-07-10 22:47:28,264][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:47:30,942][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:47:31,171][valid][INFO] - {"epoch": 105, "valid_loss": "13.266", "valid_nll_loss": "0.036", "valid_loss_recon": "0.422", "valid_loss_info_nce": "9.046", "valid_ppl": "1.03", "valid_wps": "80505.1", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "210", "valid_best_loss": "13.266"}
[2025-07-10 22:47:31,172][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 105 @ 210 updates
[2025-07-10 22:47:31,172][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint105.pt
[2025-07-10 22:47:31,535][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint105.pt
[2025-07-10 22:47:32,417][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint105.pt (epoch 105 @ 210 updates, score 13.266) (writing took 1.2453058330002023 seconds)
[2025-07-10 22:47:32,417][fairseq_cli.train][INFO] - end of epoch 105 (average epoch stats below)
[2025-07-10 22:47:32,418][train][INFO] - {"epoch": 105, "train_loss": "14.595", "train_nll_loss": "0.039", "train_loss_recon": "0.475", "train_loss_info_nce": "9.841", "train_ppl": "1.03", "train_wps": "1952.8", "train_ups": "0.48", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "210", "train_lr": "5.25e-06", "train_gnorm": "4.798", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "377"}
[2025-07-10 22:47:32,455][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:47:32,457][fairseq.trainer][INFO] - begin training epoch 106
[2025-07-10 22:47:32,457][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:47:35,127][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 106 @ 212 updates
[2025-07-10 22:47:35,127][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint106.pt
[2025-07-10 22:47:35,498][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint106.pt
[2025-07-10 22:47:35,837][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint106.pt (epoch 106 @ 212 updates, score None) (writing took 0.7103845790006744 seconds)
[2025-07-10 22:47:35,838][fairseq_cli.train][INFO] - end of epoch 106 (average epoch stats below)
[2025-07-10 22:47:35,838][train][INFO] - {"epoch": 106, "train_loss": "14.508", "train_nll_loss": "0.039", "train_loss_recon": "0.471", "train_loss_info_nce": "9.787", "train_ppl": "1.03", "train_wps": "2393.6", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "212", "train_lr": "5.3e-06", "train_gnorm": "3.845", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "381"}
[2025-07-10 22:47:35,871][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:47:35,873][fairseq.trainer][INFO] - begin training epoch 107
[2025-07-10 22:47:35,873][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:47:38,584][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 107 @ 214 updates
[2025-07-10 22:47:38,584][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint107.pt
[2025-07-10 22:47:38,964][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint107.pt
[2025-07-10 22:47:39,294][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint107.pt (epoch 107 @ 214 updates, score None) (writing took 0.7097628720002831 seconds)
[2025-07-10 22:47:39,294][fairseq_cli.train][INFO] - end of epoch 107 (average epoch stats below)
[2025-07-10 22:47:39,295][train][INFO] - {"epoch": 107, "train_loss": "14.425", "train_nll_loss": "0.039", "train_loss_recon": "0.466", "train_loss_info_nce": "9.757", "train_ppl": "1.03", "train_wps": "2368.3", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "214", "train_lr": "5.35e-06", "train_gnorm": "4.597", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "384"}
[2025-07-10 22:47:39,335][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:47:39,337][fairseq.trainer][INFO] - begin training epoch 108
[2025-07-10 22:47:39,337][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:47:42,001][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 108 @ 216 updates
[2025-07-10 22:47:42,001][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint108.pt
[2025-07-10 22:47:42,359][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint108.pt
[2025-07-10 22:47:42,702][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint108.pt (epoch 108 @ 216 updates, score None) (writing took 0.7008656790003442 seconds)
[2025-07-10 22:47:42,702][fairseq_cli.train][INFO] - end of epoch 108 (average epoch stats below)
[2025-07-10 22:47:42,703][train][INFO] - {"epoch": 108, "train_loss": "14.395", "train_nll_loss": "0.039", "train_loss_recon": "0.465", "train_loss_info_nce": "9.723", "train_ppl": "1.03", "train_wps": "2402.4", "train_ups": "0.59", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "216", "train_lr": "5.4e-06", "train_gnorm": "4.344", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "388"}
[2025-07-10 22:47:42,747][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:47:42,749][fairseq.trainer][INFO] - begin training epoch 109
[2025-07-10 22:47:42,749][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:47:45,412][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 109 @ 218 updates
[2025-07-10 22:47:45,413][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint109.pt
[2025-07-10 22:47:45,803][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint109.pt
[2025-07-10 22:47:46,134][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint109.pt (epoch 109 @ 218 updates, score None) (writing took 0.7216777640005603 seconds)
[2025-07-10 22:47:46,134][fairseq_cli.train][INFO] - end of epoch 109 (average epoch stats below)
[2025-07-10 22:47:46,135][train][INFO] - {"epoch": 109, "train_loss": "14.369", "train_nll_loss": "0.039", "train_loss_recon": "0.464", "train_loss_info_nce": "9.699", "train_ppl": "1.03", "train_wps": "2385.2", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "218", "train_lr": "5.45e-06", "train_gnorm": "5.534", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "391"}
[2025-07-10 22:47:46,174][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:47:46,175][fairseq.trainer][INFO] - begin training epoch 110
[2025-07-10 22:47:46,176][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:47:48,863][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:47:49,090][valid][INFO] - {"epoch": 110, "valid_loss": "13.01", "valid_nll_loss": "0.035", "valid_loss_recon": "0.407", "valid_loss_info_nce": "8.943", "valid_ppl": "1.02", "valid_wps": "79367.1", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "220", "valid_best_loss": "13.01"}
[2025-07-10 22:47:49,090][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 110 @ 220 updates
[2025-07-10 22:47:49,091][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint110.pt
[2025-07-10 22:47:49,483][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint110.pt
[2025-07-10 22:47:50,138][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint110.pt (epoch 110 @ 220 updates, score 13.01) (writing took 1.0476867819998006 seconds)
[2025-07-10 22:47:50,138][fairseq_cli.train][INFO] - end of epoch 110 (average epoch stats below)
[2025-07-10 22:47:50,139][train][INFO] - {"epoch": 110, "train_loss": "14.276", "train_nll_loss": "0.038", "train_loss_recon": "0.46", "train_loss_info_nce": "9.671", "train_ppl": "1.03", "train_wps": "2044.5", "train_ups": "0.5", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "220", "train_lr": "5.5e-06", "train_gnorm": "5.058", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "395"}
[2025-07-10 22:47:50,172][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:47:50,174][fairseq.trainer][INFO] - begin training epoch 111
[2025-07-10 22:47:50,174][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:47:52,879][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 111 @ 222 updates
[2025-07-10 22:47:52,880][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint111.pt
[2025-07-10 22:47:53,270][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint111.pt
[2025-07-10 22:47:53,597][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint111.pt (epoch 111 @ 222 updates, score None) (writing took 0.717868331000318 seconds)
[2025-07-10 22:47:53,597][fairseq_cli.train][INFO] - end of epoch 111 (average epoch stats below)
[2025-07-10 22:47:53,598][train][INFO] - {"epoch": 111, "train_loss": "14.223", "train_nll_loss": "0.038", "train_loss_recon": "0.458", "train_loss_info_nce": "9.638", "train_ppl": "1.03", "train_wps": "2366.7", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "222", "train_lr": "5.55e-06", "train_gnorm": "4.69", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "399"}
[2025-07-10 22:47:53,633][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:47:53,634][fairseq.trainer][INFO] - begin training epoch 112
[2025-07-10 22:47:53,635][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:47:56,369][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 112 @ 224 updates
[2025-07-10 22:47:56,369][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint112.pt
[2025-07-10 22:47:56,744][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint112.pt
[2025-07-10 22:47:57,053][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint112.pt (epoch 112 @ 224 updates, score None) (writing took 0.6838872469998023 seconds)
[2025-07-10 22:47:57,053][fairseq_cli.train][INFO] - end of epoch 112 (average epoch stats below)
[2025-07-10 22:47:57,054][train][INFO] - {"epoch": 112, "train_loss": "14.193", "train_nll_loss": "0.038", "train_loss_recon": "0.456", "train_loss_info_nce": "9.621", "train_ppl": "1.03", "train_wps": "2368.7", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "224", "train_lr": "5.6e-06", "train_gnorm": "4.549", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "402"}
[2025-07-10 22:47:57,089][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:47:57,091][fairseq.trainer][INFO] - begin training epoch 113
[2025-07-10 22:47:57,091][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:47:59,791][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 113 @ 226 updates
[2025-07-10 22:47:59,791][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint113.pt
[2025-07-10 22:48:00,158][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint113.pt
[2025-07-10 22:48:00,641][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint113.pt (epoch 113 @ 226 updates, score None) (writing took 0.8504847840004004 seconds)
[2025-07-10 22:48:00,642][fairseq_cli.train][INFO] - end of epoch 113 (average epoch stats below)
[2025-07-10 22:48:00,643][train][INFO] - {"epoch": 113, "train_loss": "14.128", "train_nll_loss": "0.038", "train_loss_recon": "0.453", "train_loss_info_nce": "9.597", "train_ppl": "1.03", "train_wps": "2281.3", "train_ups": "0.56", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "226", "train_lr": "5.65e-06", "train_gnorm": "4.303", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "406"}
[2025-07-10 22:48:00,678][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:48:00,680][fairseq.trainer][INFO] - begin training epoch 114
[2025-07-10 22:48:00,680][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:48:03,409][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 114 @ 228 updates
[2025-07-10 22:48:03,409][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint114.pt
[2025-07-10 22:48:03,771][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint114.pt
[2025-07-10 22:48:04,339][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint114.pt (epoch 114 @ 228 updates, score None) (writing took 0.9295978150003066 seconds)
[2025-07-10 22:48:04,339][fairseq_cli.train][INFO] - end of epoch 114 (average epoch stats below)
[2025-07-10 22:48:04,340][train][INFO] - {"epoch": 114, "train_loss": "14.096", "train_nll_loss": "0.038", "train_loss_recon": "0.452", "train_loss_info_nce": "9.576", "train_ppl": "1.03", "train_wps": "2214.1", "train_ups": "0.54", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "228", "train_lr": "5.7e-06", "train_gnorm": "3.949", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "409"}
[2025-07-10 22:48:04,374][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:48:04,375][fairseq.trainer][INFO] - begin training epoch 115
[2025-07-10 22:48:04,376][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:48:07,086][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:48:07,314][valid][INFO] - {"epoch": 115, "valid_loss": "12.796", "valid_nll_loss": "0.034", "valid_loss_recon": "0.398", "valid_loss_info_nce": "8.813", "valid_ppl": "1.02", "valid_wps": "79608.8", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "230", "valid_best_loss": "12.796"}
[2025-07-10 22:48:07,315][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 115 @ 230 updates
[2025-07-10 22:48:07,316][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint115.pt
[2025-07-10 22:48:07,715][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint115.pt
[2025-07-10 22:48:08,390][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint115.pt (epoch 115 @ 230 updates, score 12.796) (writing took 1.0745414270004403 seconds)
[2025-07-10 22:48:08,390][fairseq_cli.train][INFO] - end of epoch 115 (average epoch stats below)
[2025-07-10 22:48:08,391][train][INFO] - {"epoch": 115, "train_loss": "14.06", "train_nll_loss": "0.038", "train_loss_recon": "0.45", "train_loss_info_nce": "9.547", "train_ppl": "1.03", "train_wps": "2020.6", "train_ups": "0.49", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "230", "train_lr": "5.75e-06", "train_gnorm": "3.408", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "413"}
[2025-07-10 22:48:08,425][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:48:08,427][fairseq.trainer][INFO] - begin training epoch 116
[2025-07-10 22:48:08,427][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:48:11,178][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 116 @ 232 updates
[2025-07-10 22:48:11,178][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint116.pt
[2025-07-10 22:48:11,556][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint116.pt
[2025-07-10 22:48:11,908][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint116.pt (epoch 116 @ 232 updates, score None) (writing took 0.7301282959997479 seconds)
[2025-07-10 22:48:11,908][fairseq_cli.train][INFO] - end of epoch 116 (average epoch stats below)
[2025-07-10 22:48:11,909][train][INFO] - {"epoch": 116, "train_loss": "13.992", "train_nll_loss": "0.038", "train_loss_recon": "0.446", "train_loss_info_nce": "9.517", "train_ppl": "1.03", "train_wps": "2326.8", "train_ups": "0.57", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "232", "train_lr": "5.8e-06", "train_gnorm": "3.213", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "417"}
[2025-07-10 22:48:11,946][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:48:11,948][fairseq.trainer][INFO] - begin training epoch 117
[2025-07-10 22:48:11,949][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:48:14,646][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 117 @ 234 updates
[2025-07-10 22:48:14,647][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint117.pt
[2025-07-10 22:48:15,038][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint117.pt
[2025-07-10 22:48:15,369][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint117.pt (epoch 117 @ 234 updates, score None) (writing took 0.7224230609999722 seconds)
[2025-07-10 22:48:15,369][fairseq_cli.train][INFO] - end of epoch 117 (average epoch stats below)
[2025-07-10 22:48:15,370][train][INFO] - {"epoch": 117, "train_loss": "13.963", "train_nll_loss": "0.038", "train_loss_recon": "0.445", "train_loss_info_nce": "9.499", "train_ppl": "1.03", "train_wps": "2365.7", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "234", "train_lr": "5.85e-06", "train_gnorm": "2.797", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "420"}
[2025-07-10 22:48:15,404][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:48:15,406][fairseq.trainer][INFO] - begin training epoch 118
[2025-07-10 22:48:15,406][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:48:18,108][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 118 @ 236 updates
[2025-07-10 22:48:18,108][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint118.pt
[2025-07-10 22:48:18,481][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint118.pt
[2025-07-10 22:48:18,832][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint118.pt (epoch 118 @ 236 updates, score None) (writing took 0.7244266649995552 seconds)
[2025-07-10 22:48:18,832][fairseq_cli.train][INFO] - end of epoch 118 (average epoch stats below)
[2025-07-10 22:48:18,834][train][INFO] - {"epoch": 118, "train_loss": "13.911", "train_nll_loss": "0.037", "train_loss_recon": "0.443", "train_loss_info_nce": "9.471", "train_ppl": "1.03", "train_wps": "2363.7", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "236", "train_lr": "5.9e-06", "train_gnorm": "3.223", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "424"}
[2025-07-10 22:48:18,870][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:48:18,872][fairseq.trainer][INFO] - begin training epoch 119
[2025-07-10 22:48:18,872][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:48:21,544][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 119 @ 238 updates
[2025-07-10 22:48:21,544][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint119.pt
[2025-07-10 22:48:21,940][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint119.pt
[2025-07-10 22:48:22,391][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint119.pt (epoch 119 @ 238 updates, score None) (writing took 0.8474408160000166 seconds)
[2025-07-10 22:48:22,391][fairseq_cli.train][INFO] - end of epoch 119 (average epoch stats below)
[2025-07-10 22:48:22,392][train][INFO] - {"epoch": 119, "train_loss": "13.91", "train_nll_loss": "0.037", "train_loss_recon": "0.444", "train_loss_info_nce": "9.483", "train_ppl": "1.03", "train_wps": "2300.2", "train_ups": "0.56", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "238", "train_lr": "5.95e-06", "train_gnorm": "4.441", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "427"}
[2025-07-10 22:48:22,429][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:48:22,430][fairseq.trainer][INFO] - begin training epoch 120
[2025-07-10 22:48:22,430][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:48:25,183][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:48:25,416][valid][INFO] - {"epoch": 120, "valid_loss": "12.668", "valid_nll_loss": "0.034", "valid_loss_recon": "0.392", "valid_loss_info_nce": "8.753", "valid_ppl": "1.02", "valid_wps": "80434.2", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "240", "valid_best_loss": "12.668"}
[2025-07-10 22:48:25,417][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 120 @ 240 updates
[2025-07-10 22:48:25,417][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint120.pt
[2025-07-10 22:48:25,810][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint120.pt
[2025-07-10 22:48:26,739][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint120.pt (epoch 120 @ 240 updates, score 12.668) (writing took 1.3217594269999609 seconds)
[2025-07-10 22:48:26,739][fairseq_cli.train][INFO] - end of epoch 120 (average epoch stats below)
[2025-07-10 22:48:26,740][train][INFO] - {"epoch": 120, "train_loss": "13.839", "train_nll_loss": "0.037", "train_loss_recon": "0.44", "train_loss_info_nce": "9.444", "train_ppl": "1.03", "train_wps": "1882.9", "train_ups": "0.46", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "240", "train_lr": "6e-06", "train_gnorm": "4.927", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "432"}
[2025-07-10 22:48:26,776][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:48:26,778][fairseq.trainer][INFO] - begin training epoch 121
[2025-07-10 22:48:26,778][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:48:29,469][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 121 @ 242 updates
[2025-07-10 22:48:29,469][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint121.pt
[2025-07-10 22:48:29,870][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint121.pt
[2025-07-10 22:48:30,212][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint121.pt (epoch 121 @ 242 updates, score None) (writing took 0.7431396839992885 seconds)
[2025-07-10 22:48:30,212][fairseq_cli.train][INFO] - end of epoch 121 (average epoch stats below)
[2025-07-10 22:48:30,213][train][INFO] - {"epoch": 121, "train_loss": "13.807", "train_nll_loss": "0.037", "train_loss_recon": "0.438", "train_loss_info_nce": "9.415", "train_ppl": "1.03", "train_wps": "2356.7", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "242", "train_lr": "6.05e-06", "train_gnorm": "4.655", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "435"}
[2025-07-10 22:48:30,245][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:48:30,247][fairseq.trainer][INFO] - begin training epoch 122
[2025-07-10 22:48:30,247][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:48:32,905][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 122 @ 244 updates
[2025-07-10 22:48:32,905][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint122.pt
[2025-07-10 22:48:33,303][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint122.pt
[2025-07-10 22:48:33,638][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint122.pt (epoch 122 @ 244 updates, score None) (writing took 0.7332430290007323 seconds)
[2025-07-10 22:48:33,638][fairseq_cli.train][INFO] - end of epoch 122 (average epoch stats below)
[2025-07-10 22:48:33,639][train][INFO] - {"epoch": 122, "train_loss": "13.789", "train_nll_loss": "0.037", "train_loss_recon": "0.438", "train_loss_info_nce": "9.407", "train_ppl": "1.03", "train_wps": "2389.6", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "244", "train_lr": "6.1e-06", "train_gnorm": "4.557", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "439"}
[2025-07-10 22:48:33,672][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:48:33,673][fairseq.trainer][INFO] - begin training epoch 123
[2025-07-10 22:48:33,673][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:48:36,350][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 123 @ 246 updates
[2025-07-10 22:48:36,350][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint123.pt
[2025-07-10 22:48:36,731][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint123.pt
[2025-07-10 22:48:37,080][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint123.pt (epoch 123 @ 246 updates, score None) (writing took 0.7298394609997558 seconds)
[2025-07-10 22:48:37,080][fairseq_cli.train][INFO] - end of epoch 123 (average epoch stats below)
[2025-07-10 22:48:37,081][train][INFO] - {"epoch": 123, "train_loss": "13.733", "train_nll_loss": "0.037", "train_loss_recon": "0.435", "train_loss_info_nce": "9.373", "train_ppl": "1.03", "train_wps": "2378.7", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "246", "train_lr": "6.15e-06", "train_gnorm": "3.893", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "442"}
[2025-07-10 22:48:37,119][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:48:37,121][fairseq.trainer][INFO] - begin training epoch 124
[2025-07-10 22:48:37,121][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:48:39,861][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 124 @ 248 updates
[2025-07-10 22:48:39,861][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint124.pt
[2025-07-10 22:48:40,246][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint124.pt
[2025-07-10 22:48:40,607][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint124.pt (epoch 124 @ 248 updates, score None) (writing took 0.7456811110005219 seconds)
[2025-07-10 22:48:40,607][fairseq_cli.train][INFO] - end of epoch 124 (average epoch stats below)
[2025-07-10 22:48:40,608][train][INFO] - {"epoch": 124, "train_loss": "13.695", "train_nll_loss": "0.037", "train_loss_recon": "0.433", "train_loss_info_nce": "9.356", "train_ppl": "1.03", "train_wps": "2321.3", "train_ups": "0.57", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "248", "train_lr": "6.2e-06", "train_gnorm": "4.193", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "446"}
[2025-07-10 22:48:40,647][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:48:40,649][fairseq.trainer][INFO] - begin training epoch 125
[2025-07-10 22:48:40,649][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:48:43,380][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:48:43,609][valid][INFO] - {"epoch": 125, "valid_loss": "12.455", "valid_nll_loss": "0.033", "valid_loss_recon": "0.378", "valid_loss_info_nce": "8.674", "valid_ppl": "1.02", "valid_wps": "78779.5", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "250", "valid_best_loss": "12.455"}
[2025-07-10 22:48:43,609][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 125 @ 250 updates
[2025-07-10 22:48:43,610][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint125.pt
[2025-07-10 22:48:44,006][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint125.pt
[2025-07-10 22:48:44,701][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint125.pt (epoch 125 @ 250 updates, score 12.455) (writing took 1.0918361200001527 seconds)
[2025-07-10 22:48:44,701][fairseq_cli.train][INFO] - end of epoch 125 (average epoch stats below)
[2025-07-10 22:48:44,702][train][INFO] - {"epoch": 125, "train_loss": "13.656", "train_nll_loss": "0.037", "train_loss_recon": "0.431", "train_loss_info_nce": "9.341", "train_ppl": "1.03", "train_wps": "1999.3", "train_ups": "0.49", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "250", "train_lr": "6.25e-06", "train_gnorm": "3.775", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "450"}
[2025-07-10 22:48:44,739][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:48:44,740][fairseq.trainer][INFO] - begin training epoch 126
[2025-07-10 22:48:44,741][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:48:47,404][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 126 @ 252 updates
[2025-07-10 22:48:47,404][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint126.pt
[2025-07-10 22:48:47,793][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint126.pt
[2025-07-10 22:48:48,112][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint126.pt (epoch 126 @ 252 updates, score None) (writing took 0.7083619429995451 seconds)
[2025-07-10 22:48:48,113][fairseq_cli.train][INFO] - end of epoch 126 (average epoch stats below)
[2025-07-10 22:48:48,114][train][INFO] - {"epoch": 126, "train_loss": "13.642", "train_nll_loss": "0.037", "train_loss_recon": "0.431", "train_loss_info_nce": "9.334", "train_ppl": "1.03", "train_wps": "2399.9", "train_ups": "0.59", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "252", "train_lr": "6.3e-06", "train_gnorm": "2.906", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "453"}
[2025-07-10 22:48:48,149][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:48:48,151][fairseq.trainer][INFO] - begin training epoch 127
[2025-07-10 22:48:48,151][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:48:50,855][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 127 @ 254 updates
[2025-07-10 22:48:50,855][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint127.pt
[2025-07-10 22:48:51,234][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint127.pt
[2025-07-10 22:48:51,732][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint127.pt (epoch 127 @ 254 updates, score None) (writing took 0.876599090000127 seconds)
[2025-07-10 22:48:51,732][fairseq_cli.train][INFO] - end of epoch 127 (average epoch stats below)
[2025-07-10 22:48:51,733][train][INFO] - {"epoch": 127, "train_loss": "13.604", "train_nll_loss": "0.037", "train_loss_recon": "0.43", "train_loss_info_nce": "9.307", "train_ppl": "1.03", "train_wps": "2261.9", "train_ups": "0.55", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "254", "train_lr": "6.35e-06", "train_gnorm": "4.687", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "457"}
[2025-07-10 22:48:51,772][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:48:51,774][fairseq.trainer][INFO] - begin training epoch 128
[2025-07-10 22:48:51,774][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:48:54,489][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 128 @ 256 updates
[2025-07-10 22:48:54,489][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint128.pt
[2025-07-10 22:48:54,886][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint128.pt
[2025-07-10 22:48:55,326][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint128.pt (epoch 128 @ 256 updates, score None) (writing took 0.8372902740002246 seconds)
[2025-07-10 22:48:55,326][fairseq_cli.train][INFO] - end of epoch 128 (average epoch stats below)
[2025-07-10 22:48:55,327][train][INFO] - {"epoch": 128, "train_loss": "13.588", "train_nll_loss": "0.037", "train_loss_recon": "0.429", "train_loss_info_nce": "9.297", "train_ppl": "1.03", "train_wps": "2277.7", "train_ups": "0.56", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "256", "train_lr": "6.4e-06", "train_gnorm": "4.002", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "460"}
[2025-07-10 22:48:55,366][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:48:55,368][fairseq.trainer][INFO] - begin training epoch 129
[2025-07-10 22:48:55,368][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:48:58,057][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 129 @ 258 updates
[2025-07-10 22:48:58,057][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint129.pt
[2025-07-10 22:48:58,431][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint129.pt
[2025-07-10 22:48:58,966][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint129.pt (epoch 129 @ 258 updates, score None) (writing took 0.909459520999917 seconds)
[2025-07-10 22:48:58,966][fairseq_cli.train][INFO] - end of epoch 129 (average epoch stats below)
[2025-07-10 22:48:58,968][train][INFO] - {"epoch": 129, "train_loss": "13.552", "train_nll_loss": "0.036", "train_loss_recon": "0.427", "train_loss_info_nce": "9.289", "train_ppl": "1.03", "train_wps": "2248.8", "train_ups": "0.55", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "258", "train_lr": "6.45e-06", "train_gnorm": "3.512", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "464"}
[2025-07-10 22:48:59,001][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:48:59,003][fairseq.trainer][INFO] - begin training epoch 130
[2025-07-10 22:48:59,003][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:49:01,733][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:49:01,974][valid][INFO] - {"epoch": 130, "valid_loss": "12.386", "valid_nll_loss": "0.033", "valid_loss_recon": "0.377", "valid_loss_info_nce": "8.616", "valid_ppl": "1.02", "valid_wps": "78766.7", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "260", "valid_best_loss": "12.386"}
[2025-07-10 22:49:01,975][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 130 @ 260 updates
[2025-07-10 22:49:01,975][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint130.pt
[2025-07-10 22:49:02,370][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint130.pt
[2025-07-10 22:49:03,042][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint130.pt (epoch 130 @ 260 updates, score 12.386) (writing took 1.0674880309998116 seconds)
[2025-07-10 22:49:03,043][fairseq_cli.train][INFO] - end of epoch 130 (average epoch stats below)
[2025-07-10 22:49:03,044][train][INFO] - {"epoch": 130, "train_loss": "13.511", "train_nll_loss": "0.036", "train_loss_recon": "0.425", "train_loss_info_nce": "9.267", "train_ppl": "1.03", "train_wps": "2008.3", "train_ups": "0.49", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "260", "train_lr": "6.5e-06", "train_gnorm": "2.969", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "468"}
[2025-07-10 22:49:03,079][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:49:03,080][fairseq.trainer][INFO] - begin training epoch 131
[2025-07-10 22:49:03,081][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:49:05,781][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 131 @ 262 updates
[2025-07-10 22:49:05,781][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint131.pt
[2025-07-10 22:49:06,166][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint131.pt
[2025-07-10 22:49:06,507][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint131.pt (epoch 131 @ 262 updates, score None) (writing took 0.7261473919998025 seconds)
[2025-07-10 22:49:06,507][fairseq_cli.train][INFO] - end of epoch 131 (average epoch stats below)
[2025-07-10 22:49:06,508][train][INFO] - {"epoch": 131, "train_loss": "13.476", "train_nll_loss": "0.036", "train_loss_recon": "0.423", "train_loss_info_nce": "9.24", "train_ppl": "1.03", "train_wps": "2362.9", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "262", "train_lr": "6.55e-06", "train_gnorm": "2.959", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "472"}
[2025-07-10 22:49:06,546][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:49:06,547][fairseq.trainer][INFO] - begin training epoch 132
[2025-07-10 22:49:06,548][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:49:09,257][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 132 @ 264 updates
[2025-07-10 22:49:09,258][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint132.pt
[2025-07-10 22:49:09,659][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint132.pt
[2025-07-10 22:49:10,000][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint132.pt (epoch 132 @ 264 updates, score None) (writing took 0.7423391089996585 seconds)
[2025-07-10 22:49:10,000][fairseq_cli.train][INFO] - end of epoch 132 (average epoch stats below)
[2025-07-10 22:49:10,001][train][INFO] - {"epoch": 132, "train_loss": "13.463", "train_nll_loss": "0.036", "train_loss_recon": "0.423", "train_loss_info_nce": "9.227", "train_ppl": "1.03", "train_wps": "2343.7", "train_ups": "0.57", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "264", "train_lr": "6.6e-06", "train_gnorm": "2.64", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "475"}
[2025-07-10 22:49:10,039][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:49:10,041][fairseq.trainer][INFO] - begin training epoch 133
[2025-07-10 22:49:10,041][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:49:12,747][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 133 @ 266 updates
[2025-07-10 22:49:12,747][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint133.pt
[2025-07-10 22:49:13,133][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint133.pt
[2025-07-10 22:49:13,550][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint133.pt (epoch 133 @ 266 updates, score None) (writing took 0.8034990099995412 seconds)
[2025-07-10 22:49:13,551][fairseq_cli.train][INFO] - end of epoch 133 (average epoch stats below)
[2025-07-10 22:49:13,551][train][INFO] - {"epoch": 133, "train_loss": "13.455", "train_nll_loss": "0.036", "train_loss_recon": "0.422", "train_loss_info_nce": "9.219", "train_ppl": "1.03", "train_wps": "2305.5", "train_ups": "0.56", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "266", "train_lr": "6.65e-06", "train_gnorm": "2.693", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "479"}
[2025-07-10 22:49:13,590][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:49:13,591][fairseq.trainer][INFO] - begin training epoch 134
[2025-07-10 22:49:13,592][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:49:16,364][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 134 @ 268 updates
[2025-07-10 22:49:16,364][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint134.pt
[2025-07-10 22:49:16,753][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint134.pt
[2025-07-10 22:49:17,113][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint134.pt (epoch 134 @ 268 updates, score None) (writing took 0.7490522130001409 seconds)
[2025-07-10 22:49:17,113][fairseq_cli.train][INFO] - end of epoch 134 (average epoch stats below)
[2025-07-10 22:49:17,114][train][INFO] - {"epoch": 134, "train_loss": "13.416", "train_nll_loss": "0.036", "train_loss_recon": "0.421", "train_loss_info_nce": "9.204", "train_ppl": "1.03", "train_wps": "2297.7", "train_ups": "0.56", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "268", "train_lr": "6.7e-06", "train_gnorm": "2.415", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "482"}
[2025-07-10 22:49:17,149][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:49:17,151][fairseq.trainer][INFO] - begin training epoch 135
[2025-07-10 22:49:17,151][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:49:19,865][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:49:20,087][valid][INFO] - {"epoch": 135, "valid_loss": "12.232", "valid_nll_loss": "0.033", "valid_loss_recon": "0.366", "valid_loss_info_nce": "8.572", "valid_ppl": "1.02", "valid_wps": "80559.3", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "270", "valid_best_loss": "12.232"}
[2025-07-10 22:49:20,088][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 135 @ 270 updates
[2025-07-10 22:49:20,089][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint135.pt
[2025-07-10 22:49:20,480][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint135.pt
[2025-07-10 22:49:21,167][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint135.pt (epoch 135 @ 270 updates, score 12.232) (writing took 1.0788620550001724 seconds)
[2025-07-10 22:49:21,167][fairseq_cli.train][INFO] - end of epoch 135 (average epoch stats below)
[2025-07-10 22:49:21,168][train][INFO] - {"epoch": 135, "train_loss": "13.385", "train_nll_loss": "0.036", "train_loss_recon": "0.42", "train_loss_info_nce": "9.191", "train_ppl": "1.03", "train_wps": "2019.3", "train_ups": "0.49", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "270", "train_lr": "6.75e-06", "train_gnorm": "2.031", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "486"}
[2025-07-10 22:49:21,203][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:49:21,205][fairseq.trainer][INFO] - begin training epoch 136
[2025-07-10 22:49:21,205][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:49:23,935][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 136 @ 272 updates
[2025-07-10 22:49:23,935][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint136.pt
[2025-07-10 22:49:24,329][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint136.pt
[2025-07-10 22:49:24,687][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint136.pt (epoch 136 @ 272 updates, score None) (writing took 0.7521679729998141 seconds)
[2025-07-10 22:49:24,687][fairseq_cli.train][INFO] - end of epoch 136 (average epoch stats below)
[2025-07-10 22:49:24,688][train][INFO] - {"epoch": 136, "train_loss": "13.36", "train_nll_loss": "0.036", "train_loss_recon": "0.418", "train_loss_info_nce": "9.174", "train_ppl": "1.03", "train_wps": "2325.7", "train_ups": "0.57", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "272", "train_lr": "6.8e-06", "train_gnorm": "1.95", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "490"}
[2025-07-10 22:49:24,721][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:49:24,723][fairseq.trainer][INFO] - begin training epoch 137
[2025-07-10 22:49:24,723][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:49:27,409][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 137 @ 274 updates
[2025-07-10 22:49:27,409][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint137.pt
[2025-07-10 22:49:27,804][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint137.pt
[2025-07-10 22:49:28,137][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint137.pt (epoch 137 @ 274 updates, score None) (writing took 0.7277783339995949 seconds)
[2025-07-10 22:49:28,137][fairseq_cli.train][INFO] - end of epoch 137 (average epoch stats below)
[2025-07-10 22:49:28,138][train][INFO] - {"epoch": 137, "train_loss": "13.366", "train_nll_loss": "0.036", "train_loss_recon": "0.419", "train_loss_info_nce": "9.172", "train_ppl": "1.03", "train_wps": "2372.9", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "274", "train_lr": "6.85e-06", "train_gnorm": "2.538", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "493"}
[2025-07-10 22:49:28,171][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:49:28,173][fairseq.trainer][INFO] - begin training epoch 138
[2025-07-10 22:49:28,173][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:49:30,903][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 138 @ 276 updates
[2025-07-10 22:49:30,903][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint138.pt
[2025-07-10 22:49:31,285][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint138.pt
[2025-07-10 22:49:31,637][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint138.pt (epoch 138 @ 276 updates, score None) (writing took 0.734404199999517 seconds)
[2025-07-10 22:49:31,637][fairseq_cli.train][INFO] - end of epoch 138 (average epoch stats below)
[2025-07-10 22:49:31,644][train][INFO] - {"epoch": 138, "train_loss": "13.331", "train_nll_loss": "0.036", "train_loss_recon": "0.418", "train_loss_info_nce": "9.151", "train_ppl": "1.03", "train_wps": "2338.5", "train_ups": "0.57", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "276", "train_lr": "6.9e-06", "train_gnorm": "2.782", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "497"}
[2025-07-10 22:49:31,679][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:49:31,681][fairseq.trainer][INFO] - begin training epoch 139
[2025-07-10 22:49:31,681][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:49:34,424][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 139 @ 278 updates
[2025-07-10 22:49:34,425][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint139.pt
[2025-07-10 22:49:34,816][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint139.pt
[2025-07-10 22:49:35,156][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint139.pt (epoch 139 @ 278 updates, score None) (writing took 0.7315380350000851 seconds)
[2025-07-10 22:49:35,156][fairseq_cli.train][INFO] - end of epoch 139 (average epoch stats below)
[2025-07-10 22:49:35,157][train][INFO] - {"epoch": 139, "train_loss": "13.303", "train_nll_loss": "0.036", "train_loss_recon": "0.416", "train_loss_info_nce": "9.143", "train_ppl": "1.03", "train_wps": "2330", "train_ups": "0.57", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "278", "train_lr": "6.95e-06", "train_gnorm": "2.989", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "500"}
[2025-07-10 22:49:35,196][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:49:35,198][fairseq.trainer][INFO] - begin training epoch 140
[2025-07-10 22:49:35,198][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:49:37,907][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:49:38,135][valid][INFO] - {"epoch": 140, "valid_loss": "12.18", "valid_nll_loss": "0.033", "valid_loss_recon": "0.365", "valid_loss_info_nce": "8.535", "valid_ppl": "1.02", "valid_wps": "78861.6", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "280", "valid_best_loss": "12.18"}
[2025-07-10 22:49:38,136][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 140 @ 280 updates
[2025-07-10 22:49:38,137][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint140.pt
[2025-07-10 22:49:38,533][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint140.pt
[2025-07-10 22:49:39,234][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint140.pt (epoch 140 @ 280 updates, score 12.18) (writing took 1.0981415969999944 seconds)
[2025-07-10 22:49:39,234][fairseq_cli.train][INFO] - end of epoch 140 (average epoch stats below)
[2025-07-10 22:49:39,235][train][INFO] - {"epoch": 140, "train_loss": "13.281", "train_nll_loss": "0.036", "train_loss_recon": "0.415", "train_loss_info_nce": "9.129", "train_ppl": "1.03", "train_wps": "2007.2", "train_ups": "0.49", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "280", "train_lr": "7e-06", "train_gnorm": "2.251", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "504"}
[2025-07-10 22:49:39,271][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:49:39,273][fairseq.trainer][INFO] - begin training epoch 141
[2025-07-10 22:49:39,273][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:49:41,990][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 141 @ 282 updates
[2025-07-10 22:49:41,990][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint141.pt
[2025-07-10 22:49:42,381][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint141.pt
[2025-07-10 22:49:42,872][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint141.pt (epoch 141 @ 282 updates, score None) (writing took 0.8821516120005981 seconds)
[2025-07-10 22:49:42,872][fairseq_cli.train][INFO] - end of epoch 141 (average epoch stats below)
[2025-07-10 22:49:42,873][train][INFO] - {"epoch": 141, "train_loss": "13.265", "train_nll_loss": "0.036", "train_loss_recon": "0.415", "train_loss_info_nce": "9.111", "train_ppl": "1.03", "train_wps": "2250.4", "train_ups": "0.55", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "282", "train_lr": "7.05e-06", "train_gnorm": "2.126", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "508"}
[2025-07-10 22:49:42,908][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:49:42,910][fairseq.trainer][INFO] - begin training epoch 142
[2025-07-10 22:49:42,910][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:49:45,653][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 142 @ 284 updates
[2025-07-10 22:49:45,653][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint142.pt
[2025-07-10 22:49:46,026][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint142.pt
[2025-07-10 22:49:46,628][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint142.pt (epoch 142 @ 284 updates, score None) (writing took 0.9755975959997158 seconds)
[2025-07-10 22:49:46,629][fairseq_cli.train][INFO] - end of epoch 142 (average epoch stats below)
[2025-07-10 22:49:46,629][train][INFO] - {"epoch": 142, "train_loss": "13.234", "train_nll_loss": "0.036", "train_loss_recon": "0.413", "train_loss_info_nce": "9.106", "train_ppl": "1.02", "train_wps": "2179.1", "train_ups": "0.53", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "284", "train_lr": "7.1e-06", "train_gnorm": "2.427", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "512"}
[2025-07-10 22:49:46,670][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:49:46,671][fairseq.trainer][INFO] - begin training epoch 143
[2025-07-10 22:49:46,672][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:49:49,364][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 143 @ 286 updates
[2025-07-10 22:49:49,364][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint143.pt
[2025-07-10 22:49:49,761][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint143.pt
[2025-07-10 22:49:50,128][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint143.pt (epoch 143 @ 286 updates, score None) (writing took 0.7636961009993684 seconds)
[2025-07-10 22:49:50,128][fairseq_cli.train][INFO] - end of epoch 143 (average epoch stats below)
[2025-07-10 22:49:50,129][train][INFO] - {"epoch": 143, "train_loss": "13.221", "train_nll_loss": "0.036", "train_loss_recon": "0.412", "train_loss_info_nce": "9.095", "train_ppl": "1.02", "train_wps": "2339.3", "train_ups": "0.57", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "286", "train_lr": "7.15e-06", "train_gnorm": "2.438", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "515"}
[2025-07-10 22:49:50,166][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:49:50,167][fairseq.trainer][INFO] - begin training epoch 144
[2025-07-10 22:49:50,168][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:49:52,856][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 144 @ 288 updates
[2025-07-10 22:49:52,856][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint144.pt
[2025-07-10 22:49:53,230][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint144.pt
[2025-07-10 22:49:53,583][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint144.pt (epoch 144 @ 288 updates, score None) (writing took 0.727325865000239 seconds)
[2025-07-10 22:49:53,584][fairseq_cli.train][INFO] - end of epoch 144 (average epoch stats below)
[2025-07-10 22:49:53,585][train][INFO] - {"epoch": 144, "train_loss": "13.226", "train_nll_loss": "0.036", "train_loss_recon": "0.412", "train_loss_info_nce": "9.092", "train_ppl": "1.02", "train_wps": "2368.9", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "288", "train_lr": "7.2e-06", "train_gnorm": "2.63", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "519"}
[2025-07-10 22:49:53,619][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:49:53,621][fairseq.trainer][INFO] - begin training epoch 145
[2025-07-10 22:49:53,621][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:49:56,296][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:49:56,519][valid][INFO] - {"epoch": 145, "valid_loss": "12.092", "valid_nll_loss": "0.033", "valid_loss_recon": "0.358", "valid_loss_info_nce": "8.508", "valid_ppl": "1.02", "valid_wps": "80174.2", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "290", "valid_best_loss": "12.092"}
[2025-07-10 22:49:56,520][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 145 @ 290 updates
[2025-07-10 22:49:56,520][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint145.pt
[2025-07-10 22:49:56,914][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint145.pt
[2025-07-10 22:49:57,600][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint145.pt (epoch 145 @ 290 updates, score 12.092) (writing took 1.0804188409993003 seconds)
[2025-07-10 22:49:57,601][fairseq_cli.train][INFO] - end of epoch 145 (average epoch stats below)
[2025-07-10 22:49:57,602][train][INFO] - {"epoch": 145, "train_loss": "13.18", "train_nll_loss": "0.035", "train_loss_recon": "0.411", "train_loss_info_nce": "9.069", "train_ppl": "1.02", "train_wps": "2037.8", "train_ups": "0.5", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "290", "train_lr": "7.25e-06", "train_gnorm": "4.303", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "523"}
[2025-07-10 22:49:57,637][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:49:57,638][fairseq.trainer][INFO] - begin training epoch 146
[2025-07-10 22:49:57,639][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:50:00,339][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 146 @ 292 updates
[2025-07-10 22:50:00,340][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint146.pt
[2025-07-10 22:50:00,716][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint146.pt
[2025-07-10 22:50:01,069][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint146.pt (epoch 146 @ 292 updates, score None) (writing took 0.7300627580007131 seconds)
[2025-07-10 22:50:01,069][fairseq_cli.train][INFO] - end of epoch 146 (average epoch stats below)
[2025-07-10 22:50:01,070][train][INFO] - {"epoch": 146, "train_loss": "13.169", "train_nll_loss": "0.035", "train_loss_recon": "0.41", "train_loss_info_nce": "9.062", "train_ppl": "1.02", "train_wps": "2360.2", "train_ups": "0.58", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "292", "train_lr": "7.3e-06", "train_gnorm": "2.255", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "526"}
[2025-07-10 22:50:01,109][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:50:01,111][fairseq.trainer][INFO] - begin training epoch 147
[2025-07-10 22:50:01,111][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:50:03,821][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 147 @ 294 updates
[2025-07-10 22:50:03,821][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint147.pt
[2025-07-10 22:50:04,214][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint147.pt
[2025-07-10 22:50:04,626][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint147.pt (epoch 147 @ 294 updates, score None) (writing took 0.8045480660002795 seconds)
[2025-07-10 22:50:04,626][fairseq_cli.train][INFO] - end of epoch 147 (average epoch stats below)
[2025-07-10 22:50:04,627][train][INFO] - {"epoch": 147, "train_loss": "13.145", "train_nll_loss": "0.035", "train_loss_recon": "0.409", "train_loss_info_nce": "9.052", "train_ppl": "1.02", "train_wps": "2301.7", "train_ups": "0.56", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "294", "train_lr": "7.35e-06", "train_gnorm": "3.714", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "530"}
[2025-07-10 22:50:04,664][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:50:04,666][fairseq.trainer][INFO] - begin training epoch 148
[2025-07-10 22:50:04,666][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:50:07,412][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 148 @ 296 updates
[2025-07-10 22:50:07,413][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint148.pt
[2025-07-10 22:50:07,784][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint148.pt
[2025-07-10 22:50:08,449][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint148.pt (epoch 148 @ 296 updates, score None) (writing took 1.0365486800001236 seconds)
[2025-07-10 22:50:08,449][fairseq_cli.train][INFO] - end of epoch 148 (average epoch stats below)
[2025-07-10 22:50:08,450][train][INFO] - {"epoch": 148, "train_loss": "13.136", "train_nll_loss": "0.035", "train_loss_recon": "0.409", "train_loss_info_nce": "9.05", "train_ppl": "1.02", "train_wps": "2141.1", "train_ups": "0.52", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "296", "train_lr": "7.4e-06", "train_gnorm": "4.65", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "533"}
[2025-07-10 22:50:08,494][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:50:08,496][fairseq.trainer][INFO] - begin training epoch 149
[2025-07-10 22:50:08,496][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:50:11,168][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 149 @ 298 updates
[2025-07-10 22:50:11,168][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint149.pt
[2025-07-10 22:50:11,563][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint149.pt
[2025-07-10 22:50:12,158][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint149.pt (epoch 149 @ 298 updates, score None) (writing took 0.9900765100001081 seconds)
[2025-07-10 22:50:12,158][fairseq_cli.train][INFO] - end of epoch 149 (average epoch stats below)
[2025-07-10 22:50:12,159][train][INFO] - {"epoch": 149, "train_loss": "13.119", "train_nll_loss": "0.035", "train_loss_recon": "0.408", "train_loss_info_nce": "9.035", "train_ppl": "1.02", "train_wps": "2207.1", "train_ups": "0.54", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "298", "train_lr": "7.45e-06", "train_gnorm": "4.673", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "537"}
[2025-07-10 22:50:12,193][fairseq.data.iterators][INFO] - grouped total_num_itrs = 2
[2025-07-10 22:50:12,196][fairseq.trainer][INFO] - begin training epoch 150
[2025-07-10 22:50:12,196][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-10 22:50:14,887][train_inner][INFO] - {"epoch": 150, "update": 150.0, "loss": "13.759", "nll_loss": "0.037", "loss_recon": "0.437", "loss_info_nce": "9.388", "ppl": "1.03", "wps": "2251.8", "ups": "0.55", "wpb": "4092", "bsz": "494.5", "num_updates": "300", "lr": "7.5e-06", "gnorm": "3.799", "clip": "0", "loss_scale": "128", "train_wall": "91", "gb_free": "11.9", "wall": "540"}
[2025-07-10 22:50:14,888][fairseq_cli.train][INFO] - Stopping training due to num_updates: 300 >= max_update: 300
[2025-07-10 22:50:14,888][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-10 22:50:15,118][valid][INFO] - {"epoch": 150, "valid_loss": "12.097", "valid_nll_loss": "0.033", "valid_loss_recon": "0.362", "valid_loss_info_nce": "8.478", "valid_ppl": "1.02", "valid_wps": "77085.6", "valid_wpb": "372", "valid_bsz": "26.5", "valid_num_updates": "300", "valid_best_loss": "12.092"}
[2025-07-10 22:50:15,119][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 150 @ 300 updates
[2025-07-10 22:50:15,120][fairseq.trainer][INFO] - Saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint150.pt
[2025-07-10 22:50:15,520][fairseq.trainer][INFO] - Finished saving checkpoint to /home/incantator/Documents/mbari-mae/outputs/mae_ast_batch_sweep_20250710_211226/config_14_4enc_1dec_high_freq/checkpoints/checkpoint150.pt
[2025-07-10 22:50:15,854][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint150.pt (epoch 150 @ 300 updates, score 12.097) (writing took 0.7348434000005 seconds)
[2025-07-10 22:50:15,854][fairseq_cli.train][INFO] - end of epoch 150 (average epoch stats below)
[2025-07-10 22:50:15,855][train][INFO] - {"epoch": 150, "train_loss": "13.095", "train_nll_loss": "0.035", "train_loss_recon": "0.407", "train_loss_info_nce": "9.023", "train_ppl": "1.02", "train_wps": "2214.7", "train_ups": "0.54", "train_wpb": "4092", "train_bsz": "494.5", "train_num_updates": "300", "train_lr": "7.5e-06", "train_gnorm": "5.87", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "2", "train_gb_free": "11.9", "train_wall": "541"}
[2025-07-10 22:50:15,855][fairseq_cli.train][INFO] - done training in 540.5 seconds
