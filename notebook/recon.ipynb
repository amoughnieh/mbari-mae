{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-01T05:50:34.507183Z",
     "start_time": "2025-07-01T05:50:30.724669Z"
    }
   },
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import torch\n",
    "import fairseq\n",
    "from omegaconf import OmegaConf\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import mae_ast.tasks.mae_ast_pretraining\n",
    "\n",
    "\n",
    "\n",
    "def print_info(message):\n",
    "    print(f\"[*] {message}\")\n",
    "\n",
    "print_info(\"All libraries imported successfully.\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali\\miniconda3\\envs\\TF3_7\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] All libraries imported successfully.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T05:50:34.619343Z",
     "start_time": "2025-07-01T05:50:34.588185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "config_dir = \"config/pretrain\"\n",
    "config_name = \"mae_ast - recon\"\n",
    "data_dir = r\"D:\\MBARI 2KHz\\training\\input_dir\"  # The folder with your train.tsv and valid.tsv\n",
    "output_dir = r\"D:\\MBARI 2KHz\\training\\250701 4en 2de\\output_model\"  # The folder where checkpoints are saved\n",
    "checkpoint_file = \"checkpoint_last.pt\"  # The specific checkpoint you want to load\n",
    "\n",
    "index_to_visualize = 1327\n",
    "\n",
    "no_enc = 4\n",
    "no_dec = 2\n",
    "im_sample = 'valid'\n",
    "\n",
    "overrides = {\n",
    "    'task': {\n",
    "        'data': data_dir\n",
    "    },\n",
    "    'dataset': {\n",
    "        'valid_subset': im_sample\n",
    "    },\n",
    "    'hydra': {\n",
    "        'run': {\n",
    "            'dir': output_dir\n",
    "        }\n",
    "    },\n",
    "    'model': {\n",
    "        'encoder_layers': no_enc,\n",
    "        'decoder_layers': no_dec\n",
    "    }\n",
    "}\n",
    "\n",
    "config_path = os.path.join(config_dir, f\"{config_name}.yaml\")\n",
    "cfg = OmegaConf.load(config_path)\n",
    "cfg.merge_with(overrides)\n",
    "\n",
    "print_info(\"Configuration loaded and ready.\")\n",
    "print_info(f\"Will visualize sample #{index_to_visualize} from the validation set.\")"
   ],
   "id": "47766cc021fd1fb7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Configuration loaded and ready.\n",
      "[*] Will visualize sample #1327 from the validation set.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T05:50:36.204483Z",
     "start_time": "2025-07-01T05:50:34.653184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "print_info(\"Setting up the Fairseq task and building the model...\")\n",
    "task = fairseq.tasks.setup_task(cfg.task)\n",
    "model = task.build_model(cfg.model)\n",
    "\n",
    "checkpoint_path = os.path.join(output_dir, \"checkpoints\", checkpoint_file)\n",
    "print_info(f\"Loading checkpoint from: {checkpoint_path}\")\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    model.to(device)\n",
    "    print_info(\"Model moved to GPU.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print_info(\"CUDA not available. Using CPU.\")\n",
    "\n",
    "print_info(f\"Loading dataset for split: {cfg.dataset.valid_subset}\")\n",
    "task.load_dataset(cfg.dataset.valid_subset)\n",
    "dataset = task.dataset(cfg.dataset.valid_subset)\n",
    "print_info(f\"Dataset loaded with {len(dataset)} samples.\")"
   ],
   "id": "d2c2f229cab071d9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 15:50:34 | INFO | mae_ast.tasks.mae_ast_pretraining | current directory is C:\\Users\\Ali\\OneDrive - Georgia Institute of Technology\\25-5 Summer\\CS 7643 - Deep Learning\\_Project\\MAE-AST-Public\n",
      "2025-07-01 15:50:34 | INFO | mae_ast.tasks.mae_ast_pretraining | MAEPretrainingTask Config {'_name': 'mae_ast_pretraining', 'data': 'D:\\\\MBARI 2KHz\\\\training\\\\input_dir', 'sample_rate': 2000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 40000, 'min_sample_size': 5000, 'random_crop': True, 'pad_audio': False, 'feature_type': 'fbank', 'feature_rate': 100, 'feature_dim': 128, 'deltas': False, 'mask_spans': False, 'mask_type': random_mask}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Setting up the Fairseq task and building the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 15:50:34 | INFO | mae_ast.models.mae_ast | MAEModel Config: {'_name': 'mae_ast', 'ast_kernel_size_chan': 16, 'ast_kernel_size_time': 16, 'ast_kernel_stride_chan': 16, 'ast_kernel_stride_time': 16, 'encoder_layers': 4, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_norm_first': False, 'feature_grad_mult': 0.1, 'use_post_enc_proj': False, 'decoder_embed_dim': 768, 'decoder_layers': 2, 'decoder_layerdrop': 0.0, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'random_mask_prob': 0.75, 'mask_length': 10, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'checkpoint_activations': False, 'max_token_length': 48000, 'enc_sine_pos': True, 'enc_conv_pos': False, 'dec_sine_pos': True, 'dec_conv_pos': False}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Loading checkpoint from: D:\\MBARI 2KHz\\training\\250701 4en 2de\\output_model\\checkpoints\\checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 15:50:36 | INFO | mae_ast.data.mae_ast_dataset | max_keep=None, min_keep=5000, loaded 10374, skipped 0 short and 0 long, longest-loaded=20000, shortest-loaded=10000\n",
      "2025-07-01 15:50:36 | INFO | mae_ast.data.mae_ast_dataset | pad_audio=False, random_crop=True, normalize=False, max_sample_size=40000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Model moved to GPU.\n",
      "[*] Loading dataset for split: valid\n",
      "[*] Dataset loaded with 10374 samples.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T05:50:36.396491Z",
     "start_time": "2025-07-01T05:50:36.237482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "print_info(f\"Extracting preprocessed sample at index {index_to_visualize}...\")\n",
    "sample = dataset[index_to_visualize]\n",
    "spectrogram_tensor = sample['source']\n",
    "\n",
    "print_info(\"Creating tensors for model input...\")\n",
    "\n",
    "padding_mask = torch.zeros(1, spectrogram_tensor.shape[0], dtype=torch.bool)\n",
    "\n",
    "batch_tensor = spectrogram_tensor.unsqueeze(0).to(device)\n",
    "padding_mask = padding_mask.to(device)\n",
    "\n",
    "print(\"Inputs are ready.\")\n",
    "print(f\"Shape of input tensor: {batch_tensor.shape}\")\n",
    "print(f\"Shape of padding mask: {padding_mask.shape}\")"
   ],
   "id": "3600cb47eec2303a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Extracting preprocessed sample at index 1327...\n",
      "[*] Creating tensors for model input...\n",
      "Inputs are ready.\n",
      "Shape of input tensor: torch.Size([1, 1001, 128])\n",
      "Shape of padding mask: torch.Size([1, 1001])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T05:50:45.462683Z",
     "start_time": "2025-07-01T05:50:40.309683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print_info(\"Running model forward pass...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_output = model.forward(source=batch_tensor, padding_mask=padding_mask)\n",
    "\n",
    "print_info(\"--- Inspection Results ---\")\n",
    "print(f\"The model returned an object of type: {type(model_output)}\")\n",
    "\n",
    "if isinstance(model_output, (list, tuple)):\n",
    "    print(f\"It contains {len(model_output)} items.\")\n",
    "    for i, item in enumerate(model_output):\n",
    "        if hasattr(item, 'shape'):\n",
    "            print(f\"  - Item #{i} has shape: {item.shape}\")\n",
    "elif hasattr(model_output, 'shape'):\n",
    "     print(f\"The output has shape: {model_output.shape}\")"
   ],
   "id": "466bfaec665ea99b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Running model forward pass...\n",
      "[*] --- Inspection Results ---\n",
      "The model returned an object of type: <class 'dict'>\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T05:50:45.913683Z",
     "start_time": "2025-07-01T05:50:45.899683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print_info(\"Keys in the output dictionary:\")\n",
    "print(list(model_output.keys()))\n",
    "\n",
    "print_info(\"\\nShapes of tensors in the output dictionary:\")\n",
    "for key, value in model_output.items():\n",
    "    if isinstance(value, torch.Tensor) and hasattr(value, 'shape'):\n",
    "        print(f\"  - Key '{key}' has a tensor with shape: {value.shape}\")"
   ],
   "id": "977941b8cfb6e3b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Keys in the output dictionary:\n",
      "['logit_m_list_recon', 'logit_m_list_class', 'target_m_list', 'padding_mask', 'mask_indices']\n",
      "[*] \n",
      "Shapes of tensors in the output dictionary:\n",
      "  - Key 'logit_m_list_recon' has a tensor with shape: torch.Size([1, 372, 256])\n",
      "  - Key 'logit_m_list_class' has a tensor with shape: torch.Size([1, 372, 256])\n",
      "  - Key 'target_m_list' has a tensor with shape: torch.Size([1, 372, 256])\n",
      "  - Key 'padding_mask' has a tensor with shape: torch.Size([1, 496])\n",
      "  - Key 'mask_indices' has a tensor with shape: torch.Size([1, 496])\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T05:50:46.300682Z",
     "start_time": "2025-07-01T05:50:46.085684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Final Cell: Correctly Oriented Image Reconstruction\n",
    "\n",
    "# --- Imports for this cell ---\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Get a Sample and Run Inference ---\n",
    "print_info(f\"Extracting preprocessed sample at index {index_to_visualize}...\")\n",
    "sample = dataset[index_to_visualize]\n",
    "spectrogram_tensor = sample['source']\n",
    "\n",
    "# Prepare tensors for the model\n",
    "batch_tensor = spectrogram_tensor.unsqueeze(0).to(device)\n",
    "padding_mask = torch.zeros(1, spectrogram_tensor.shape[0], dtype=torch.bool).to(device)\n",
    "\n",
    "print_info(\"Running model forward pass...\")\n",
    "with torch.no_grad():\n",
    "    model_output = model.forward(source=batch_tensor, padding_mask=padding_mask, mask=True)\n",
    "\n",
    "# --- 2. Extract Tensors from the Model Output ---\n",
    "print_info(\"Extracting tensors from the output dictionary...\")\n",
    "reconstructed_patches = model_output['logit_m_list_recon'].squeeze(0)\n",
    "mask_indices = model_output['mask_indices'].squeeze(0)\n",
    "\n",
    "# --- 3. Reassemble the Full Images ---\n",
    "print_info(\"Reassembling full spectrograms...\")\n",
    "\n",
    "# Get original patches and parameters\n",
    "all_patches = model.unfold(batch_tensor.unsqueeze(1)).squeeze(0).transpose(0, 1)\n",
    "p_c, p_t = cfg.model.ast_kernel_size_chan, cfg.model.ast_kernel_size_time\n",
    "h, w = spectrogram_tensor.shape[0], spectrogram_tensor.shape[1]\n",
    "n_h, n_w = h // p_c, w // p_t\n",
    "folder = torch.nn.Fold(output_size=(h, w), kernel_size=(p_c, p_t), stride=(p_c, p_t))\n",
    "\n",
    "# Reassemble the masked image\n",
    "masked_input_patches = all_patches.clone()\n",
    "masked_input_patches[mask_indices] = torch.min(all_patches)\n",
    "masked_image_data = masked_input_patches.transpose(0, 1).reshape(1, p_c * p_t, n_h * n_w)\n",
    "masked_image_tensor = folder(masked_image_data).squeeze(0)\n",
    "\n",
    "# Reassemble the globally normalized reconstruction\n",
    "recons_global_patches = all_patches.clone()\n",
    "recons_global_patches[mask_indices] = reconstructed_patches\n",
    "recons_global_data = recons_global_patches.transpose(0, 1).reshape(1, p_c * p_t, n_h * n_w)\n",
    "recons_global_tensor = folder(recons_global_data).squeeze(0)\n",
    "\n",
    "# Reassemble the per-patch normalized reconstruction\n",
    "'''recons_per_patch_patches = all_patches.clone()\n",
    "normalized_recons_patches = torch.zeros_like(reconstructed_patches)\n",
    "for i in range(reconstructed_patches.shape[0]):\n",
    "    patch = reconstructed_patches[i]\n",
    "    min_p, max_p = torch.min(patch), torch.max(patch)\n",
    "    if max_p > min_p:\n",
    "        normalized_recons_patches[i] = (patch - min_p) / (max_p - min_p)\n",
    "    else:\n",
    "        normalized_recons_patches[i] = torch.ones_like(patch) * 0.5\n",
    "recons_per_patch_patches[mask_indices] = normalized_recons_patches\n",
    "recons_per_patch_data = recons_per_patch_patches.transpose(0, 1).reshape(1, p_c * p_t, n_h * n_w)\n",
    "recons_per_patch_tensor = folder(recons_per_patch_data).squeeze(0)'''\n",
    "\n",
    "# --- 4. Corrected Helper Function to Save Images ---\n",
    "def save_tensor_as_image(data_tensor, filename):\n",
    "    \"\"\"Normalizes a tensor and saves it as a correctly oriented grayscale PNG.\"\"\"\n",
    "    # Convert to NumPy and TRANSPOSE to make Time the X-axis (wide image)\n",
    "    data_array = data_tensor.cpu().numpy().transpose()\n",
    "\n",
    "    # Normalize the data to the 0-255 range for image saving\n",
    "    min_val, max_val = np.min(data_array), np.max(data_array)\n",
    "    if max_val > min_val:\n",
    "        normalized_data = (data_array - min_val) / (max_val - min_val)\n",
    "    else:\n",
    "        normalized_data = np.zeros_like(data_array)\n",
    "\n",
    "    image_data = (normalized_data * 255).astype(np.uint8)\n",
    "    img = Image.fromarray(image_data, 'L')\n",
    "    img.save(filename)\n",
    "    print_info(f\"Successfully saved image to {filename}\")\n",
    "\n",
    "# --- 5. Save All Comparison Images ---\n",
    "print_info(\"Saving all comparison images with correct orientation...\")\n",
    "\n",
    "save_tensor_as_image(spectrogram_tensor[:, :cfg.task.feature_dim], f\"{index_to_visualize}-{im_sample}-{no_enc}enc{no_dec}dec-original.png\")\n",
    "#save_tensor_as_image(masked_image_tensor.squeeze(0)[:, :cfg.task.feature_dim], f\"{index_to_visualize}-masked.png\")\n",
    "save_tensor_as_image(recons_global_tensor.squeeze(0)[:, :cfg.task.feature_dim], f\"{index_to_visualize}-{im_sample}-{no_enc}enc{no_dec}dec-recon.png\")\n",
    "#save_tensor_as_image(recons_per_patch_tensor.squeeze(0)[:, :cfg.task.feature_dim], \"comparison_04_recon_per_patch_norm.png\")"
   ],
   "id": "e8a232bd0397149f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Extracting preprocessed sample at index 1327...\n",
      "[*] Running model forward pass...\n",
      "[*] Extracting tensors from the output dictionary...\n",
      "[*] Reassembling full spectrograms...\n",
      "[*] Saving all comparison images with correct orientation...\n",
      "[*] Successfully saved image to 1327-valid-4enc2dec-original.png\n",
      "[*] Successfully saved image to 1327-valid-4enc2dec-recon.png\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T05:50:48.154685Z",
     "start_time": "2025-07-01T05:50:48.140684Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "bd28a562ff431f8b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
