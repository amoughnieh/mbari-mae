[2025-07-01 20:03:37,189][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'tblog', 'wandb_project': None, 'azureml_logging': False, 'seed': 1337, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': 'C:\\Users\\Ali\\OneDrive - Georgia Institute of Technology\\25-5 Summer\\CS 7643 - Deep Learning\\_Project\\MAE-AST-Public\\mae_ast', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 6, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 1048576, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 5, 'validate_interval_updates': 10000, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1048576, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [8], 'lr': [0.0001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 25000, 'keep_interval_updates': 1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'mae_ast', 'ast_kernel_size_chan': 16, 'ast_kernel_size_time': 16, 'ast_kernel_stride_chan': 16, 'ast_kernel_stride_time': 16, 'encoder_layers': 4, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_norm_first': False, 'feature_grad_mult': 0.1, 'use_post_enc_proj': False, 'decoder_embed_dim': 768, 'decoder_layers': 1, 'decoder_layerdrop': 0.0, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'random_mask_prob': 0.75, 'mask_length': 10, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'checkpoint_activations': False, 'max_token_length': 48000, 'enc_sine_pos': True, 'enc_conv_pos': False, 'dec_sine_pos': True, 'dec_conv_pos': False}, 'task': {'_name': 'mae_ast_pretraining', 'data': 'D:\\MBARI 2KHz\\training\\input_dir', 'sample_rate': 2000, 'normalize': True, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 40000, 'min_sample_size': 5000, 'random_crop': True, 'pad_audio': False, 'feature_type': 'fbank', 'feature_rate': 100, 'feature_dim': 128, 'deltas': False, 'mask_spans': False, 'mask_type': random_mask}, 'criterion': {'_name': 'mae_ast', 'reconstruction_weight': 10.0, 'classification_weight': 1.0}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0001]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 4000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000.0, 'lr': [0.0001]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2025-07-01 20:03:37,189][mae_ast.tasks.mae_ast_pretraining][INFO] - current directory is D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model
[2025-07-01 20:03:37,189][mae_ast.tasks.mae_ast_pretraining][INFO] - MAEPretrainingTask Config {'_name': 'mae_ast_pretraining', 'data': 'D:\\MBARI 2KHz\\training\\input_dir', 'sample_rate': 2000, 'normalize': True, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 40000, 'min_sample_size': 5000, 'random_crop': True, 'pad_audio': False, 'feature_type': 'fbank', 'feature_rate': 100, 'feature_dim': 128, 'deltas': False, 'mask_spans': False, 'mask_type': random_mask}
[2025-07-01 20:03:37,189][mae_ast.models.mae_ast][INFO] - MAEModel Config: {'_name': 'mae_ast', 'ast_kernel_size_chan': 16, 'ast_kernel_size_time': 16, 'ast_kernel_stride_chan': 16, 'ast_kernel_stride_time': 16, 'encoder_layers': 4, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_norm_first': False, 'feature_grad_mult': 0.1, 'use_post_enc_proj': False, 'decoder_embed_dim': 768, 'decoder_layers': 1, 'decoder_layerdrop': 0.0, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'random_mask_prob': 0.75, 'mask_length': 10, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'checkpoint_activations': False, 'max_token_length': 48000, 'enc_sine_pos': True, 'enc_conv_pos': False, 'dec_sine_pos': True, 'dec_conv_pos': False}
[2025-07-01 20:03:37,672][fairseq_cli.train][INFO] - MAE_AST(
  (feature_extractor): Identity()
  (post_extract_proj): Linear(in_features=256, out_features=768, bias=True)
  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (batch_norm): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  (unfold): Unfold(kernel_size=(16, 16), dilation=1, padding=0, stride=(16, 16))
  (dropout_input): Dropout(p=0.1, inplace=False)
  (enc_sine_pos_embed): SinusoidalPositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (dec_sine_pos_embed): SinusoidalPositionalEncoding()
  (decoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (final_proj_reconstruction): Linear(in_features=768, out_features=256, bias=True)
  (final_proj_classification): Linear(in_features=768, out_features=256, bias=True)
)
[2025-07-01 20:03:37,672][fairseq_cli.train][INFO] - task: MAE_AST_Pretraining_Task
[2025-07-01 20:03:37,672][fairseq_cli.train][INFO] - model: MAE_AST
[2025-07-01 20:03:37,672][fairseq_cli.train][INFO] - criterion: MAE_AST_Criterion
[2025-07-01 20:03:37,672][fairseq_cli.train][INFO] - num. shared model params: 36,035,328 (num. trained: 36,035,328)
[2025-07-01 20:03:37,672][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2025-07-01 20:03:37,861][mae_ast.data.mae_ast_dataset][INFO] - max_keep=None, min_keep=5000, loaded 10374, skipped 0 short and 0 long, longest-loaded=20000, shortest-loaded=10000
[2025-07-01 20:03:37,861][mae_ast.data.mae_ast_dataset][INFO] - pad_audio=False, random_crop=True, normalize=True, max_sample_size=40000
[2025-07-01 20:03:38,075][fairseq.trainer][INFO] - detected shared parameter: batch_norm.weight <- batch_norm.bias
[2025-07-01 20:03:38,075][fairseq.utils][INFO] - ***********************CUDA enviroments for all 1 workers***********************
[2025-07-01 20:03:38,075][fairseq.utils][INFO] - rank   0: capabilities =  7.5  ; total memory = 6.000 GB ; name = NVIDIA GeForce RTX 2060                 
[2025-07-01 20:03:38,075][fairseq.utils][INFO] - ***********************CUDA enviroments for all 1 workers***********************
[2025-07-01 20:03:38,075][fairseq_cli.train][INFO] - training on 1 devices (GPUs/TPUs)
[2025-07-01 20:03:38,075][fairseq_cli.train][INFO] - max tokens per device = 1048576 and max sentences per device = None
[2025-07-01 20:03:38,075][fairseq.trainer][INFO] - Preparing to load checkpoint checkpoints\checkpoint_last.pt
[2025-07-01 20:03:38,091][fairseq.trainer][INFO] - No existing checkpoint found checkpoints\checkpoint_last.pt
[2025-07-01 20:03:38,091][fairseq.trainer][INFO] - loading train data for epoch 1
[2025-07-01 20:03:38,132][mae_ast.data.mae_ast_dataset][INFO] - max_keep=None, min_keep=5000, loaded 93367, skipped 0 short and 0 long, longest-loaded=20000, shortest-loaded=10000
[2025-07-01 20:03:38,132][mae_ast.data.mae_ast_dataset][INFO] - pad_audio=False, random_crop=True, normalize=True, max_sample_size=40000
[2025-07-01 20:03:39,060][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-01 20:03:39,060][fairseq.trainer][INFO] - begin training epoch 1
[2025-07-01 20:03:39,060][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-01 20:12:16,359][train_inner][INFO] - {"epoch": 1, "update": 0.826, "loss": "19.307", "nll_loss": "0.052", "loss_recon": "0.711", "loss_info_nce": "12.196", "ppl": "1.04", "wps": "1198.1", "ups": "0.4", "wpb": "2965.8", "bsz": "386.3", "num_updates": "200", "lr": "5e-06", "gnorm": "16.937", "clip": "58.5", "loss_scale": "128", "train_wall": "495", "gb_free": "2.2", "wall": "513"}
[2025-07-01 20:14:00,057][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1 @ 242 updates
[2025-07-01 20:14:00,057][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint1.pt
[2025-07-01 20:14:01,798][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint1.pt
[2025-07-01 20:14:03,525][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint1.pt (epoch 1 @ 242 updates, score None) (writing took 3.4654299999999694 seconds)
[2025-07-01 20:14:03,525][fairseq_cli.train][INFO] - end of epoch 1 (average epoch stats below)
[2025-07-01 20:14:03,541][train][INFO] - {"epoch": 1, "train_loss": "18.349", "train_nll_loss": "0.05", "train_loss_recon": "0.66", "train_loss_info_nce": "11.732", "train_ppl": "1.03", "train_wps": "1177.3", "train_ups": "0.4", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "242", "train_lr": "6.05e-06", "train_gnorm": "14.642", "train_clip": "48.3", "train_loss_scale": "128", "train_train_wall": "598", "train_gb_free": "2.2", "train_wall": "625"}
[2025-07-01 20:14:03,742][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-01 20:14:03,742][fairseq.trainer][INFO] - begin training epoch 2
[2025-07-01 20:14:03,742][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-01 20:20:46,556][train_inner][INFO] - {"epoch": 2, "update": 1.653, "loss": "12.262", "nll_loss": "0.033", "loss_recon": "0.312", "loss_info_nce": "9.143", "ppl": "1.02", "wps": "1157.9", "ups": "0.39", "wpb": "2953.7", "bsz": "386.4", "num_updates": "400", "lr": "1e-05", "gnorm": "2.992", "clip": "0", "loss_scale": "128", "train_wall": "503", "gb_free": "2.2", "wall": "1028"}
[2025-07-01 20:24:18,010][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 2 @ 484 updates
[2025-07-01 20:24:18,010][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint2.pt
[2025-07-01 20:24:19,785][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint2.pt
[2025-07-01 20:24:20,601][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint2.pt (epoch 2 @ 484 updates, score None) (writing took 2.58875009999997 seconds)
[2025-07-01 20:24:20,601][fairseq_cli.train][INFO] - end of epoch 2 (average epoch stats below)
[2025-07-01 20:24:20,627][train][INFO] - {"epoch": 2, "train_loss": "11.63", "train_nll_loss": "0.031", "train_loss_recon": "0.267", "train_loss_info_nce": "8.956", "train_ppl": "1.02", "train_wps": "1159.6", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "484", "train_lr": "1.21e-05", "train_gnorm": "2.74", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "610", "train_gb_free": "2.2", "train_wall": "1243"}
[2025-07-01 20:24:20,870][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-01 20:24:20,870][fairseq.trainer][INFO] - begin training epoch 3
[2025-07-01 20:24:20,885][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-01 20:29:14,857][train_inner][INFO] - {"epoch": 3, "update": 2.479, "loss": "11.056", "nll_loss": "0.03", "loss_recon": "0.232", "loss_info_nce": "8.74", "ppl": "1.02", "wps": "1162.9", "ups": "0.39", "wpb": "2955.5", "bsz": "385.6", "num_updates": "600", "lr": "1.5e-05", "gnorm": "2.472", "clip": "0", "loss_scale": "128", "train_wall": "502", "gb_free": "2.2", "wall": "1537"}
[2025-07-01 20:34:35,794][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 3 @ 726 updates
[2025-07-01 20:34:35,795][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint3.pt
[2025-07-01 20:34:37,478][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint3.pt
[2025-07-01 20:34:38,249][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint3.pt (epoch 3 @ 726 updates, score None) (writing took 2.45508319999999 seconds)
[2025-07-01 20:34:38,249][fairseq_cli.train][INFO] - end of epoch 3 (average epoch stats below)
[2025-07-01 20:34:38,269][train][INFO] - {"epoch": 3, "train_loss": "10.919", "train_nll_loss": "0.029", "train_loss_recon": "0.226", "train_loss_info_nce": "8.661", "train_ppl": "1.02", "train_wps": "1158.6", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "726", "train_lr": "1.815e-05", "train_gnorm": "2.885", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "612", "train_gb_free": "2.2", "train_wall": "1860"}
[2025-07-01 20:34:38,483][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-01 20:34:38,486][fairseq.trainer][INFO] - begin training epoch 4
[2025-07-01 20:34:38,486][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-01 20:38:09,574][train_inner][INFO] - {"epoch": 4, "update": 3.306, "loss": "10.829", "nll_loss": "0.029", "loss_recon": "0.222", "loss_info_nce": "8.612", "ppl": "1.02", "wps": "1105.8", "ups": "0.37", "wpb": "2956.5", "bsz": "385.1", "num_updates": "800", "lr": "2e-05", "gnorm": "4.18", "clip": "7.5", "loss_scale": "128", "train_wall": "529", "gb_free": "2.2", "wall": "2071"}
[2025-07-01 20:45:44,001][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 4 @ 968 updates
[2025-07-01 20:45:44,002][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint4.pt
[2025-07-01 20:45:45,755][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint4.pt
[2025-07-01 20:45:46,526][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint4.pt (epoch 4 @ 968 updates, score None) (writing took 2.525127800000064 seconds)
[2025-07-01 20:45:46,527][fairseq_cli.train][INFO] - end of epoch 4 (average epoch stats below)
[2025-07-01 20:45:46,553][train][INFO] - {"epoch": 4, "train_loss": "10.722", "train_nll_loss": "0.029", "train_loss_recon": "0.216", "train_loss_info_nce": "8.558", "train_ppl": "1.02", "train_wps": "1070.8", "train_ups": "0.36", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "968", "train_lr": "2.42e-05", "train_gnorm": "4.066", "train_clip": "6.6", "train_loss_scale": "128", "train_train_wall": "663", "train_gb_free": "2.2", "train_wall": "2528"}
[2025-07-01 20:45:46,758][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-01 20:45:46,762][fairseq.trainer][INFO] - begin training epoch 5
[2025-07-01 20:45:46,763][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-01 20:47:13,499][train_inner][INFO] - {"epoch": 5, "update": 4.132, "loss": "10.684", "nll_loss": "0.029", "loss_recon": "0.215", "loss_info_nce": "8.538", "ppl": "1.02", "wps": "1085", "ups": "0.37", "wpb": "2950.9", "bsz": "386", "num_updates": "1000", "lr": "2.5e-05", "gnorm": "3.647", "clip": "1.5", "loss_scale": "128", "train_wall": "539", "gb_free": "2.2", "wall": "2615"}
[2025-07-01 20:56:31,497][train_inner][INFO] - {"epoch": 5, "update": 4.959, "loss": "10.547", "nll_loss": "0.028", "loss_recon": "0.206", "loss_info_nce": "8.491", "ppl": "1.02", "wps": "1063.3", "ups": "0.36", "wpb": "2966.7", "bsz": "386.8", "num_updates": "1200", "lr": "3e-05", "gnorm": "5.184", "clip": "16", "loss_scale": "128", "train_wall": "557", "gb_free": "2.2", "wall": "3173"}
[2025-07-01 20:56:55,290][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-01 20:57:39,307][valid][INFO] - {"epoch": 5, "valid_loss": "10.331", "valid_nll_loss": "0.028", "valid_loss_recon": "0.194", "valid_loss_info_nce": "8.392", "valid_ppl": "1.02", "valid_wps": "2634.3", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "1210"}
[2025-07-01 20:57:39,309][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 5 @ 1210 updates
[2025-07-01 20:57:39,310][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint5.pt
[2025-07-01 20:57:41,106][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint5.pt
[2025-07-01 20:57:44,214][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint5.pt (epoch 5 @ 1210 updates, score 10.331) (writing took 4.9055386 seconds)
[2025-07-01 20:57:44,215][fairseq_cli.train][INFO] - end of epoch 5 (average epoch stats below)
[2025-07-01 20:57:44,236][train][INFO] - {"epoch": 5, "train_loss": "10.554", "train_nll_loss": "0.028", "train_loss_recon": "0.206", "train_loss_info_nce": "8.492", "train_ppl": "1.02", "train_wps": "997", "train_ups": "0.34", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "1210", "train_lr": "3.025e-05", "train_gnorm": "5.215", "train_clip": "14.9", "train_loss_scale": "128", "train_train_wall": "666", "train_gb_free": "2.2", "train_wall": "3246"}
[2025-07-01 20:57:44,457][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-01 20:57:44,460][fairseq.trainer][INFO] - begin training epoch 6
[2025-07-01 20:57:44,461][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-01 21:06:38,974][train_inner][INFO] - {"epoch": 6, "update": 5.785, "loss": "10.448", "nll_loss": "0.028", "loss_recon": "0.199", "loss_info_nce": "8.456", "ppl": "1.02", "wps": "973.1", "ups": "0.33", "wpb": "2955.5", "bsz": "385.4", "num_updates": "1400", "lr": "3.5e-05", "gnorm": "7.229", "clip": "21", "loss_scale": "128", "train_wall": "555", "gb_free": "2.2", "wall": "3781"}
[2025-07-01 21:09:01,714][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 6 @ 1452 updates
[2025-07-01 21:09:01,716][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint6.pt
[2025-07-01 21:09:03,681][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint6.pt
[2025-07-01 21:09:04,729][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint6.pt (epoch 6 @ 1452 updates, score None) (writing took 3.0149820999999974 seconds)
[2025-07-01 21:09:04,730][fairseq_cli.train][INFO] - end of epoch 6 (average epoch stats below)
[2025-07-01 21:09:04,749][train][INFO] - {"epoch": 6, "train_loss": "10.439", "train_nll_loss": "0.028", "train_loss_recon": "0.198", "train_loss_info_nce": "8.455", "train_ppl": "1.02", "train_wps": "1051.5", "train_ups": "0.36", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "1452", "train_lr": "3.63e-05", "train_gnorm": "8.617", "train_clip": "26.4", "train_loss_scale": "128", "train_train_wall": "674", "train_gb_free": "2.2", "train_wall": "3927"}
[2025-07-01 21:09:05,149][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-01 21:09:05,156][fairseq.trainer][INFO] - begin training epoch 7
[2025-07-01 21:09:05,156][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-01 21:15:58,454][train_inner][INFO] - {"epoch": 7, "update": 6.612, "loss": "10.352", "nll_loss": "0.028", "loss_recon": "0.193", "loss_info_nce": "8.427", "ppl": "1.02", "wps": "1056.9", "ups": "0.36", "wpb": "2956.5", "bsz": "385.5", "num_updates": "1600", "lr": "4e-05", "gnorm": "8.674", "clip": "23.5", "loss_scale": "128", "train_wall": "551", "gb_free": "2.2", "wall": "4340"}
[2025-07-01 21:20:19,199][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 7 @ 1694 updates
[2025-07-01 21:20:19,200][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint7.pt
[2025-07-01 21:20:21,002][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint7.pt
[2025-07-01 21:20:21,777][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint7.pt (epoch 7 @ 1694 updates, score None) (writing took 2.5778997999996136 seconds)
[2025-07-01 21:20:21,778][fairseq_cli.train][INFO] - end of epoch 7 (average epoch stats below)
[2025-07-01 21:20:21,797][train][INFO] - {"epoch": 7, "train_loss": "10.308", "train_nll_loss": "0.028", "train_loss_recon": "0.191", "train_loss_info_nce": "8.403", "train_ppl": "1.02", "train_wps": "1056.9", "train_ups": "0.36", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "1694", "train_lr": "4.235e-05", "train_gnorm": "6.438", "train_clip": "14", "train_loss_scale": "128", "train_train_wall": "669", "train_gb_free": "2.2", "train_wall": "4604"}
[2025-07-01 21:20:22,005][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-01 21:20:22,008][fairseq.trainer][INFO] - begin training epoch 8
[2025-07-01 21:20:22,009][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-01 21:25:46,527][train_inner][INFO] - {"epoch": 8, "update": 7.438, "loss": "10.245", "nll_loss": "0.028", "loss_recon": "0.187", "loss_info_nce": "8.374", "ppl": "1.02", "wps": "1004.9", "ups": "0.34", "wpb": "2954.6", "bsz": "385.9", "num_updates": "1800", "lr": "4.5e-05", "gnorm": "6.997", "clip": "22", "loss_scale": "128", "train_wall": "582", "gb_free": "2.2", "wall": "4928"}
[2025-07-01 21:32:38,569][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 8 @ 1936 updates
[2025-07-01 21:32:38,570][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint8.pt
[2025-07-01 21:32:40,383][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint8.pt
[2025-07-01 21:32:40,962][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint8.pt (epoch 8 @ 1936 updates, score None) (writing took 2.3917050999998537 seconds)
[2025-07-01 21:32:40,962][fairseq_cli.train][INFO] - end of epoch 8 (average epoch stats below)
[2025-07-01 21:32:40,966][train][INFO] - {"epoch": 8, "train_loss": "10.197", "train_nll_loss": "0.028", "train_loss_recon": "0.184", "train_loss_info_nce": "8.353", "train_ppl": "1.02", "train_wps": "968", "train_ups": "0.33", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "1936", "train_lr": "4.84e-05", "train_gnorm": "8.062", "train_clip": "30.6", "train_loss_scale": "128", "train_train_wall": "732", "train_gb_free": "2.2", "train_wall": "5343"}
[2025-07-01 21:32:41,179][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-01 21:32:41,183][fairseq.trainer][INFO] - begin training epoch 9
[2025-07-01 21:32:41,183][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-01 21:35:38,674][train_inner][INFO] - {"epoch": 9, "update": 8.264, "loss": "10.163", "nll_loss": "0.027", "loss_recon": "0.183", "loss_info_nce": "8.333", "ppl": "1.02", "wps": "997.3", "ups": "0.34", "wpb": "2952.8", "bsz": "386.2", "num_updates": "2000", "lr": "5e-05", "gnorm": "7.503", "clip": "27", "loss_scale": "128", "train_wall": "586", "gb_free": "2.2", "wall": "5521"}
[2025-07-01 21:42:45,225][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 9 @ 2178 updates
[2025-07-01 21:42:45,226][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint9.pt
[2025-07-01 21:42:47,055][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint9.pt
[2025-07-01 21:42:47,681][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint9.pt (epoch 9 @ 2178 updates, score None) (writing took 2.4566215000004377 seconds)
[2025-07-01 21:42:47,682][fairseq_cli.train][INFO] - end of epoch 9 (average epoch stats below)
[2025-07-01 21:42:47,690][train][INFO] - {"epoch": 9, "train_loss": "10.119", "train_nll_loss": "0.027", "train_loss_recon": "0.181", "train_loss_info_nce": "8.307", "train_ppl": "1.02", "train_wps": "1179.4", "train_ups": "0.4", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "2178", "train_lr": "5.445e-05", "train_gnorm": "9.399", "train_clip": "37.6", "train_loss_scale": "256", "train_train_wall": "601", "train_gb_free": "2.2", "train_wall": "5950"}
[2025-07-01 21:42:47,906][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-01 21:42:47,911][fairseq.trainer][INFO] - begin training epoch 10
[2025-07-01 21:42:47,911][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-01 21:43:51,888][train_inner][INFO] - {"epoch": 10, "update": 9.091, "loss": "10.121", "nll_loss": "0.027", "loss_recon": "0.182", "loss_info_nce": "8.306", "ppl": "1.02", "wps": "1198.5", "ups": "0.41", "wpb": "2955.5", "bsz": "385.4", "num_updates": "2200", "lr": "5.5e-05", "gnorm": "11.191", "clip": "48.5", "loss_scale": "256", "train_wall": "487", "gb_free": "2.2", "wall": "6014"}
[2025-07-01 21:53:16,485][train_inner][INFO] - {"epoch": 10, "update": 9.917, "loss": "10.019", "nll_loss": "0.027", "loss_recon": "0.177", "loss_info_nce": "8.248", "ppl": "1.02", "wps": "1050.3", "ups": "0.35", "wpb": "2964.8", "bsz": "386.6", "num_updates": "2400", "lr": "6e-05", "gnorm": "6.696", "clip": "20.5", "loss_scale": "256", "train_wall": "562", "gb_free": "2.2", "wall": "6578"}
[2025-07-01 21:54:09,006][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-01 21:55:02,538][valid][INFO] - {"epoch": 10, "valid_loss": "9.742", "valid_nll_loss": "0.026", "valid_loss_recon": "0.162", "valid_loss_info_nce": "8.125", "valid_ppl": "1.02", "valid_wps": "1544.8", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "2420", "valid_best_loss": "9.742"}
[2025-07-01 21:55:02,540][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 10 @ 2420 updates
[2025-07-01 21:55:02,541][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint10.pt
[2025-07-01 21:55:04,635][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint10.pt
[2025-07-01 21:55:06,944][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint10.pt (epoch 10 @ 2420 updates, score 9.742) (writing took 4.404211000000032 seconds)
[2025-07-01 21:55:06,945][fairseq_cli.train][INFO] - end of epoch 10 (average epoch stats below)
[2025-07-01 21:55:06,971][train][INFO] - {"epoch": 10, "train_loss": "10.03", "train_nll_loss": "0.027", "train_loss_recon": "0.178", "train_loss_info_nce": "8.253", "train_ppl": "1.02", "train_wps": "967.9", "train_ups": "0.33", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "2420", "train_lr": "6.05e-05", "train_gnorm": "7.759", "train_clip": "27.3", "train_loss_scale": "256", "train_train_wall": "676", "train_gb_free": "2.2", "train_wall": "6689"}
[2025-07-01 21:55:07,400][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-01 21:55:07,406][fairseq.trainer][INFO] - begin training epoch 11
[2025-07-01 21:55:07,407][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-01 22:03:23,621][train_inner][INFO] - {"epoch": 11, "update": 10.744, "loss": "9.905", "nll_loss": "0.027", "loss_recon": "0.171", "loss_info_nce": "8.193", "ppl": "1.02", "wps": "974.2", "ups": "0.33", "wpb": "2957.4", "bsz": "384.8", "num_updates": "2600", "lr": "6.5e-05", "gnorm": "7.539", "clip": "22.5", "loss_scale": "256", "train_wall": "544", "gb_free": "2.2", "wall": "7186"}
[2025-07-01 22:05:54,522][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 11 @ 2662 updates
[2025-07-01 22:05:54,523][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint11.pt
[2025-07-01 22:05:56,328][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint11.pt
[2025-07-01 22:05:56,972][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint11.pt (epoch 11 @ 2662 updates, score None) (writing took 2.4502566000001025 seconds)
[2025-07-01 22:05:56,973][fairseq_cli.train][INFO] - end of epoch 11 (average epoch stats below)
[2025-07-01 22:05:56,990][train][INFO] - {"epoch": 11, "train_loss": "9.888", "train_nll_loss": "0.027", "train_loss_recon": "0.17", "train_loss_info_nce": "8.183", "train_ppl": "1.02", "train_wps": "1100.8", "train_ups": "0.37", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "2662", "train_lr": "6.655e-05", "train_gnorm": "8.255", "train_clip": "24.8", "train_loss_scale": "256", "train_train_wall": "642", "train_gb_free": "2.2", "train_wall": "7339"}
[2025-07-01 22:05:57,291][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-01 22:05:57,294][fairseq.trainer][INFO] - begin training epoch 12
[2025-07-01 22:05:57,295][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-01 22:12:03,583][train_inner][INFO] - {"epoch": 12, "update": 11.57, "loss": "9.794", "nll_loss": "0.026", "loss_recon": "0.167", "loss_info_nce": "8.127", "ppl": "1.02", "wps": "1136.8", "ups": "0.38", "wpb": "2955.5", "bsz": "386.1", "num_updates": "2800", "lr": "7e-05", "gnorm": "7.936", "clip": "26", "loss_scale": "256", "train_wall": "514", "gb_free": "2.2", "wall": "7705"}
[2025-07-01 22:16:09,207][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 12 @ 2904 updates
[2025-07-01 22:16:09,208][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint12.pt
[2025-07-01 22:16:10,985][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint12.pt
[2025-07-01 22:16:11,561][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint12.pt (epoch 12 @ 2904 updates, score None) (writing took 2.3536567999999534 seconds)
[2025-07-01 22:16:11,561][fairseq_cli.train][INFO] - end of epoch 12 (average epoch stats below)
[2025-07-01 22:16:11,578][train][INFO] - {"epoch": 12, "train_loss": "9.72", "train_nll_loss": "0.026", "train_loss_recon": "0.164", "train_loss_info_nce": "8.082", "train_ppl": "1.02", "train_wps": "1164.3", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "2904", "train_lr": "7.26e-05", "train_gnorm": "7.048", "train_clip": "22.7", "train_loss_scale": "256", "train_train_wall": "608", "train_gb_free": "2.2", "train_wall": "7953"}
[2025-07-01 22:16:11,781][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-01 22:16:11,785][fairseq.trainer][INFO] - begin training epoch 13
[2025-07-01 22:16:11,785][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-01 22:20:32,212][train_inner][INFO] - {"epoch": 13, "update": 12.397, "loss": "9.603", "nll_loss": "0.026", "loss_recon": "0.159", "loss_info_nce": "8.011", "ppl": "1.02", "wps": "1160.7", "ups": "0.39", "wpb": "2951.8", "bsz": "386.2", "num_updates": "3000", "lr": "7.5e-05", "gnorm": "6.802", "clip": "17.5", "loss_scale": "256", "train_wall": "503", "gb_free": "2.2", "wall": "8214"}
[2025-07-01 22:27:31,848][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 13 @ 3146 updates
[2025-07-01 22:27:31,849][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint13.pt
[2025-07-01 22:27:33,696][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint13.pt
[2025-07-01 22:27:34,288][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint13.pt (epoch 13 @ 3146 updates, score None) (writing took 2.440804500000013 seconds)
[2025-07-01 22:27:34,289][fairseq_cli.train][INFO] - end of epoch 13 (average epoch stats below)
[2025-07-01 22:27:34,296][train][INFO] - {"epoch": 13, "train_loss": "9.49", "train_nll_loss": "0.026", "train_loss_recon": "0.155", "train_loss_info_nce": "7.939", "train_ppl": "1.02", "train_wps": "1048.1", "train_ups": "0.35", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "3146", "train_lr": "7.865e-05", "train_gnorm": "5.719", "train_clip": "11.2", "train_loss_scale": "256", "train_train_wall": "677", "train_gb_free": "2.2", "train_wall": "8636"}
[2025-07-01 22:27:34,492][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-01 22:27:34,496][fairseq.trainer][INFO] - begin training epoch 14
[2025-07-01 22:27:34,496][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-01 22:30:00,632][train_inner][INFO] - {"epoch": 14, "update": 13.223, "loss": "9.436", "nll_loss": "0.025", "loss_recon": "0.153", "loss_info_nce": "7.905", "ppl": "1.02", "wps": "1041.2", "ups": "0.35", "wpb": "2959.3", "bsz": "384.5", "num_updates": "3200", "lr": "8e-05", "gnorm": "5.895", "clip": "14", "loss_scale": "256", "train_wall": "563", "gb_free": "2.2", "wall": "8783"}
[2025-07-01 22:37:54,291][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 14 @ 3388 updates
[2025-07-01 22:37:54,292][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint14.pt
[2025-07-01 22:37:56,012][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint14.pt
[2025-07-01 22:37:56,605][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint14.pt (epoch 14 @ 3388 updates, score None) (writing took 2.3140590000002703 seconds)
[2025-07-01 22:37:56,606][fairseq_cli.train][INFO] - end of epoch 14 (average epoch stats below)
[2025-07-01 22:37:56,614][train][INFO] - {"epoch": 14, "train_loss": "9.329", "train_nll_loss": "0.025", "train_loss_recon": "0.15", "train_loss_info_nce": "7.834", "train_ppl": "1.02", "train_wps": "1149.8", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "3388", "train_lr": "8.47e-05", "train_gnorm": "6.454", "train_clip": "17.4", "train_loss_scale": "256", "train_train_wall": "617", "train_gb_free": "2.2", "train_wall": "9259"}
[2025-07-01 22:37:56,813][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-01 22:37:56,817][fairseq.trainer][INFO] - begin training epoch 15
[2025-07-01 22:37:56,817][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-01 22:38:30,612][train_inner][INFO] - {"epoch": 15, "update": 14.05, "loss": "9.308", "nll_loss": "0.025", "loss_recon": "0.149", "loss_info_nce": "7.818", "ppl": "1.02", "wps": "1158", "ups": "0.39", "wpb": "2952.8", "bsz": "386.2", "num_updates": "3400", "lr": "8.5e-05", "gnorm": "6.051", "clip": "15", "loss_scale": "256", "train_wall": "505", "gb_free": "2.2", "wall": "9293"}
[2025-07-01 22:48:29,253][train_inner][INFO] - {"epoch": 15, "update": 14.876, "loss": "9.172", "nll_loss": "0.025", "loss_recon": "0.145", "loss_info_nce": "7.723", "ppl": "1.02", "wps": "989.9", "ups": "0.33", "wpb": "2963", "bsz": "387.1", "num_updates": "3600", "lr": "9e-05", "gnorm": "5.609", "clip": "10.5", "loss_scale": "256", "train_wall": "597", "gb_free": "2.2", "wall": "9891"}
[2025-07-01 22:49:53,406][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-01 22:50:28,262][valid][INFO] - {"epoch": 15, "valid_loss": "8.763", "valid_nll_loss": "0.024", "valid_loss_recon": "0.131", "valid_loss_info_nce": "7.454", "valid_ppl": "1.02", "valid_wps": "2354.8", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "3630", "valid_best_loss": "8.763"}
[2025-07-01 22:50:28,262][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 15 @ 3630 updates
[2025-07-01 22:50:28,262][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint15.pt
[2025-07-01 22:50:29,979][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint15.pt
[2025-07-01 22:50:31,425][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint15.pt (epoch 15 @ 3630 updates, score 8.763) (writing took 3.164117800000895 seconds)
[2025-07-01 22:50:31,425][fairseq_cli.train][INFO] - end of epoch 15 (average epoch stats below)
[2025-07-01 22:50:31,456][train][INFO] - {"epoch": 15, "train_loss": "9.167", "train_nll_loss": "0.025", "train_loss_recon": "0.145", "train_loss_info_nce": "7.72", "train_ppl": "1.02", "train_wps": "948", "train_ups": "0.32", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "3630", "train_lr": "9.075e-05", "train_gnorm": "5.273", "train_clip": "8.7", "train_loss_scale": "256", "train_train_wall": "712", "train_gb_free": "2.2", "train_wall": "10013"}
[2025-07-01 22:50:31,666][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-01 22:50:31,666][fairseq.trainer][INFO] - begin training epoch 16
[2025-07-01 22:50:31,666][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-01 22:57:40,831][train_inner][INFO] - {"epoch": 16, "update": 15.702, "loss": "9.048", "nll_loss": "0.024", "loss_recon": "0.142", "loss_info_nce": "7.631", "ppl": "1.02", "wps": "1072.7", "ups": "0.36", "wpb": "2958.3", "bsz": "385", "num_updates": "3800", "lr": "9.5e-05", "gnorm": "5.01", "clip": "5.5", "loss_scale": "256", "train_wall": "510", "gb_free": "2.2", "wall": "10443"}
[2025-07-01 23:00:53,830][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 16 @ 3872 updates
[2025-07-01 23:00:53,830][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint16.pt
[2025-07-01 23:00:55,603][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint16.pt
[2025-07-01 23:00:56,218][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint16.pt (epoch 16 @ 3872 updates, score None) (writing took 2.3870881000002555 seconds)
[2025-07-01 23:00:56,218][fairseq_cli.train][INFO] - end of epoch 16 (average epoch stats below)
[2025-07-01 23:00:56,218][train][INFO] - {"epoch": 16, "train_loss": "9.02", "train_nll_loss": "0.024", "train_loss_recon": "0.141", "train_loss_info_nce": "7.606", "train_ppl": "1.02", "train_wps": "1145.3", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "3872", "train_lr": "9.68e-05", "train_gnorm": "5.468", "train_clip": "7", "train_loss_scale": "256", "train_train_wall": "619", "train_gb_free": "2.2", "train_wall": "10638"}
[2025-07-01 23:00:56,440][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-01 23:00:56,444][fairseq.trainer][INFO] - begin training epoch 17
[2025-07-01 23:00:56,444][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-01 23:06:18,619][train_inner][INFO] - {"epoch": 17, "update": 16.529, "loss": "8.93", "nll_loss": "0.024", "loss_recon": "0.14", "loss_info_nce": "7.535", "ppl": "1.02", "wps": "1140.2", "ups": "0.39", "wpb": "2951.8", "bsz": "386.3", "num_updates": "4000", "lr": "0.0001", "gnorm": "5.121", "clip": "8", "loss_scale": "256", "train_wall": "512", "gb_free": "2.2", "wall": "10961"}
[2025-07-01 23:11:13,560][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 17 @ 4114 updates
[2025-07-01 23:11:13,576][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint17.pt
[2025-07-01 23:11:15,422][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint17.pt
[2025-07-01 23:11:15,993][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint17.pt (epoch 17 @ 4114 updates, score None) (writing took 2.433136999999988 seconds)
[2025-07-01 23:11:16,009][fairseq_cli.train][INFO] - end of epoch 17 (average epoch stats below)
[2025-07-01 23:11:16,009][train][INFO] - {"epoch": 17, "train_loss": "8.874", "train_nll_loss": "0.024", "train_loss_recon": "0.138", "train_loss_info_nce": "7.491", "train_ppl": "1.02", "train_wps": "1154.5", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "4114", "train_lr": "9.97522e-05", "train_gnorm": "4.751", "train_clip": "7.9", "train_loss_scale": "512", "train_train_wall": "613", "train_gb_free": "2.2", "train_wall": "11258"}
[2025-07-01 23:11:16,219][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-01 23:11:16,219][fairseq.trainer][INFO] - begin training epoch 18
[2025-07-01 23:11:16,219][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-01 23:14:57,281][train_inner][INFO] - {"epoch": 18, "update": 17.355, "loss": "8.817", "nll_loss": "0.024", "loss_recon": "0.138", "loss_info_nce": "7.439", "ppl": "1.02", "wps": "1139.7", "ups": "0.39", "wpb": "2955.5", "bsz": "385.4", "num_updates": "4200", "lr": "9.95652e-05", "gnorm": "4.765", "clip": "5.5", "loss_scale": "512", "train_wall": "513", "gb_free": "2.2", "wall": "11479"}
[2025-07-01 23:21:18,570][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 18 @ 4356 updates
[2025-07-01 23:21:18,570][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint18.pt
[2025-07-01 23:21:20,461][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint18.pt
[2025-07-01 23:21:21,031][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint18.pt (epoch 18 @ 4356 updates, score None) (writing took 2.4534627000011824 seconds)
[2025-07-01 23:21:21,031][fairseq_cli.train][INFO] - end of epoch 18 (average epoch stats below)
[2025-07-01 23:21:21,031][train][INFO] - {"epoch": 18, "train_loss": "8.728", "train_nll_loss": "0.024", "train_loss_recon": "0.137", "train_loss_info_nce": "7.362", "train_ppl": "1.02", "train_wps": "1182.7", "train_ups": "0.4", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "4356", "train_lr": "9.92261e-05", "train_gnorm": "4.623", "train_clip": "3.3", "train_loss_scale": "512", "train_train_wall": "599", "train_gb_free": "2.2", "train_wall": "11863"}
[2025-07-01 23:21:21,231][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-01 23:21:21,247][fairseq.trainer][INFO] - begin training epoch 19
[2025-07-01 23:21:21,247][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-01 23:23:21,667][train_inner][INFO] - {"epoch": 19, "update": 18.182, "loss": "8.675", "nll_loss": "0.023", "loss_recon": "0.136", "loss_info_nce": "7.316", "ppl": "1.02", "wps": "1171.9", "ups": "0.4", "wpb": "2955.5", "bsz": "385.9", "num_updates": "4400", "lr": "9.91304e-05", "gnorm": "4.372", "clip": "3.5", "loss_scale": "512", "train_wall": "499", "gb_free": "2.2", "wall": "11984"}
[2025-07-01 23:32:35,571][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 19 @ 4598 updates
[2025-07-01 23:32:35,571][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint19.pt
[2025-07-01 23:32:37,444][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint19.pt
[2025-07-01 23:32:38,490][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint19.pt (epoch 19 @ 4598 updates, score None) (writing took 2.914359300000797 seconds)
[2025-07-01 23:32:38,490][fairseq_cli.train][INFO] - end of epoch 19 (average epoch stats below)
[2025-07-01 23:32:38,490][train][INFO] - {"epoch": 19, "train_loss": "8.489", "train_nll_loss": "0.023", "train_loss_recon": "0.135", "train_loss_info_nce": "7.136", "train_ppl": "1.02", "train_wps": "1056.2", "train_ups": "0.36", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "4598", "train_lr": "9.87e-05", "train_gnorm": "4.332", "train_clip": "3.3", "train_loss_scale": "512", "train_train_wall": "670", "train_gb_free": "2.2", "train_wall": "12540"}
[2025-07-01 23:32:38,875][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-01 23:32:38,875][fairseq.trainer][INFO] - begin training epoch 20
[2025-07-01 23:32:38,875][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-01 23:32:46,224][train_inner][INFO] - {"epoch": 20, "update": 19.008, "loss": "8.462", "nll_loss": "0.023", "loss_recon": "0.135", "loss_info_nce": "7.111", "ppl": "1.02", "wps": "1047.4", "ups": "0.35", "wpb": "2956.5", "bsz": "385.1", "num_updates": "4600", "lr": "9.86957e-05", "gnorm": "4.569", "clip": "3.5", "loss_scale": "512", "train_wall": "557", "gb_free": "2.2", "wall": "12548"}
[2025-07-01 23:41:53,072][train_inner][INFO] - {"epoch": 20, "update": 19.835, "loss": "8.126", "nll_loss": "0.022", "loss_recon": "0.132", "loss_info_nce": "6.805", "ppl": "1.02", "wps": "1083.7", "ups": "0.37", "wpb": "2963", "bsz": "387.1", "num_updates": "4800", "lr": "9.82609e-05", "gnorm": "4.498", "clip": "2.5", "loss_scale": "512", "train_wall": "546", "gb_free": "2.2", "wall": "13095"}
[2025-07-01 23:43:32,307][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-01 23:44:08,149][valid][INFO] - {"epoch": 20, "valid_loss": "7.242", "valid_nll_loss": "0.02", "valid_loss_recon": "0.119", "valid_loss_info_nce": "6.055", "valid_ppl": "1.01", "valid_wps": "2292.4", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "4840", "valid_best_loss": "7.242"}
[2025-07-01 23:44:08,149][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 20 @ 4840 updates
[2025-07-01 23:44:08,149][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint20.pt
[2025-07-01 23:44:09,933][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint20.pt
[2025-07-01 23:44:11,729][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint20.pt (epoch 20 @ 4840 updates, score 7.242) (writing took 3.5781712000007246 seconds)
[2025-07-01 23:44:11,730][fairseq_cli.train][INFO] - end of epoch 20 (average epoch stats below)
[2025-07-01 23:44:11,746][train][INFO] - {"epoch": 20, "train_loss": "8.1", "train_nll_loss": "0.022", "train_loss_recon": "0.132", "train_loss_info_nce": "6.78", "train_ppl": "1.02", "train_wps": "1032.2", "train_ups": "0.35", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "4840", "train_lr": "9.81739e-05", "train_gnorm": "4.45", "train_clip": "2.1", "train_loss_scale": "512", "train_train_wall": "650", "train_gb_free": "2.2", "train_wall": "13234"}
[2025-07-01 23:44:11,968][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-01 23:44:11,968][fairseq.trainer][INFO] - begin training epoch 21
[2025-07-01 23:44:11,968][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-01 23:51:21,229][train_inner][INFO] - {"epoch": 21, "update": 20.661, "loss": "7.858", "nll_loss": "0.021", "loss_recon": "0.13", "loss_info_nce": "6.555", "ppl": "1.01", "wps": "1041.1", "ups": "0.35", "wpb": "2957.4", "bsz": "385.1", "num_updates": "5000", "lr": "9.78261e-05", "gnorm": "4.445", "clip": "0.5", "loss_scale": "512", "train_wall": "526", "gb_free": "2.2", "wall": "13663"}
[2025-07-01 23:55:05,173][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 21 @ 5082 updates
[2025-07-01 23:55:05,173][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint21.pt
[2025-07-01 23:55:07,558][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint21.pt
[2025-07-01 23:55:10,537][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint21.pt (epoch 21 @ 5082 updates, score None) (writing took 5.36111130000063 seconds)
[2025-07-01 23:55:10,537][fairseq_cli.train][INFO] - end of epoch 21 (average epoch stats below)
[2025-07-01 23:55:10,653][train][INFO] - {"epoch": 21, "train_loss": "7.786", "train_nll_loss": "0.021", "train_loss_recon": "0.129", "train_loss_info_nce": "6.492", "train_ppl": "1.01", "train_wps": "1086.1", "train_ups": "0.37", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "5082", "train_lr": "9.76478e-05", "train_gnorm": "4.276", "train_clip": "0.4", "train_loss_scale": "512", "train_train_wall": "650", "train_gb_free": "2.2", "train_wall": "13892"}
[2025-07-01 23:55:11,064][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-01 23:55:11,085][fairseq.trainer][INFO] - begin training epoch 22
[2025-07-01 23:55:11,085][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 00:00:18,877][train_inner][INFO] - {"epoch": 22, "update": 21.488, "loss": "7.637", "nll_loss": "0.021", "loss_recon": "0.128", "loss_info_nce": "6.36", "ppl": "1.01", "wps": "1098", "ups": "0.37", "wpb": "2951.8", "bsz": "386.6", "num_updates": "5200", "lr": "9.73913e-05", "gnorm": "4.514", "clip": "7", "loss_scale": "512", "train_wall": "528", "gb_free": "2.2", "wall": "14201"}
[2025-07-02 00:05:25,099][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 22 @ 5324 updates
[2025-07-02 00:05:25,099][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint22.pt
[2025-07-02 00:05:26,940][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint22.pt
[2025-07-02 00:05:27,657][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint22.pt (epoch 22 @ 5324 updates, score None) (writing took 2.555632699999478 seconds)
[2025-07-02 00:05:27,657][fairseq_cli.train][INFO] - end of epoch 22 (average epoch stats below)
[2025-07-02 00:05:27,667][train][INFO] - {"epoch": 22, "train_loss": "7.544", "train_nll_loss": "0.02", "train_loss_recon": "0.127", "train_loss_info_nce": "6.27", "train_ppl": "1.01", "train_wps": "1159.7", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "5324", "train_lr": "9.71217e-05", "train_gnorm": "4.387", "train_clip": "8.3", "train_loss_scale": "512", "train_train_wall": "610", "train_gb_free": "2.2", "train_wall": "14510"}
[2025-07-02 00:05:27,925][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 00:05:27,942][fairseq.trainer][INFO] - begin training epoch 23
[2025-07-02 00:05:27,942][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 00:08:39,129][train_inner][INFO] - {"epoch": 23, "update": 22.314, "loss": "7.452", "nll_loss": "0.02", "loss_recon": "0.126", "loss_info_nce": "6.187", "ppl": "1.01", "wps": "1182.8", "ups": "0.4", "wpb": "2958.3", "bsz": "384.1", "num_updates": "5400", "lr": "9.69565e-05", "gnorm": "3.626", "clip": "3.5", "loss_scale": "512", "train_wall": "494", "gb_free": "2.2", "wall": "14701"}
[2025-07-02 00:15:16,432][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 23 @ 5566 updates
[2025-07-02 00:15:16,432][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint23.pt
[2025-07-02 00:15:18,222][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint23.pt
[2025-07-02 00:15:18,855][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint23.pt (epoch 23 @ 5566 updates, score None) (writing took 2.4314765999988595 seconds)
[2025-07-02 00:15:18,855][fairseq_cli.train][INFO] - end of epoch 23 (average epoch stats below)
[2025-07-02 00:15:18,882][train][INFO] - {"epoch": 23, "train_loss": "7.321", "train_nll_loss": "0.02", "train_loss_recon": "0.125", "train_loss_info_nce": "6.068", "train_ppl": "1.01", "train_wps": "1210.3", "train_ups": "0.41", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "5566", "train_lr": "9.65957e-05", "train_gnorm": "3.127", "train_clip": "0.4", "train_loss_scale": "512", "train_train_wall": "585", "train_gb_free": "2.2", "train_wall": "15101"}
[2025-07-02 00:15:19,134][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 00:15:19,134][fairseq.trainer][INFO] - begin training epoch 24
[2025-07-02 00:15:19,134][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 00:16:43,475][train_inner][INFO] - {"epoch": 24, "update": 23.14, "loss": "7.273", "nll_loss": "0.02", "loss_recon": "0.125", "loss_info_nce": "6.024", "ppl": "1.01", "wps": "1219.3", "ups": "0.41", "wpb": "2952.8", "bsz": "387", "num_updates": "5600", "lr": "9.65217e-05", "gnorm": "2.997", "clip": "0", "loss_scale": "512", "train_wall": "479", "gb_free": "2.2", "wall": "15185"}
[2025-07-02 00:24:37,875][train_inner][INFO] - {"epoch": 24, "update": 23.967, "loss": "7.137", "nll_loss": "0.019", "loss_recon": "0.123", "loss_info_nce": "5.904", "ppl": "1.01", "wps": "1250.4", "ups": "0.42", "wpb": "2965.8", "bsz": "386.3", "num_updates": "5800", "lr": "9.6087e-05", "gnorm": "3.52", "clip": "1.5", "loss_scale": "512", "train_wall": "473", "gb_free": "2.2", "wall": "15660"}
[2025-07-02 00:24:55,058][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 24 @ 5808 updates
[2025-07-02 00:24:55,058][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint24.pt
[2025-07-02 00:24:56,871][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint24.pt
[2025-07-02 00:24:57,544][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint24.pt (epoch 24 @ 5808 updates, score None) (writing took 2.48106960000041 seconds)
[2025-07-02 00:24:57,544][fairseq_cli.train][INFO] - end of epoch 24 (average epoch stats below)
[2025-07-02 00:24:57,544][train][INFO] - {"epoch": 24, "train_loss": "7.143", "train_nll_loss": "0.019", "train_loss_recon": "0.123", "train_loss_info_nce": "5.909", "train_ppl": "1.01", "train_wps": "1236.5", "train_ups": "0.42", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "5808", "train_lr": "9.60696e-05", "train_gnorm": "3.416", "train_clip": "1.2", "train_loss_scale": "512", "train_train_wall": "573", "train_gb_free": "2.2", "train_wall": "15679"}
[2025-07-02 00:24:57,804][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 00:24:57,814][fairseq.trainer][INFO] - begin training epoch 25
[2025-07-02 00:24:57,814][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 00:32:34,880][train_inner][INFO] - {"epoch": 25, "update": 24.793, "loss": "6.996", "nll_loss": "0.019", "loss_recon": "0.122", "loss_info_nce": "5.774", "ppl": "1.01", "wps": "1239.2", "ups": "0.42", "wpb": "2955.5", "bsz": "385.4", "num_updates": "6000", "lr": "9.56522e-05", "gnorm": "2.823", "clip": "0", "loss_scale": "512", "train_wall": "472", "gb_free": "2.2", "wall": "16137"}
[2025-07-02 00:34:31,517][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-02 00:35:03,138][valid][INFO] - {"epoch": 25, "valid_loss": "6.226", "valid_nll_loss": "0.017", "valid_loss_recon": "0.112", "valid_loss_info_nce": "5.106", "valid_ppl": "1.01", "valid_wps": "2602.6", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "6050", "valid_best_loss": "6.226"}
[2025-07-02 00:35:03,138][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 25 @ 6050 updates
[2025-07-02 00:35:03,138][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint25.pt
[2025-07-02 00:35:04,996][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint25.pt
[2025-07-02 00:35:06,106][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint25.pt (epoch 25 @ 6050 updates, score 6.226) (writing took 2.968549500001245 seconds)
[2025-07-02 00:35:06,106][fairseq_cli.train][INFO] - end of epoch 25 (average epoch stats below)
[2025-07-02 00:35:06,123][train][INFO] - {"epoch": 25, "train_loss": "6.981", "train_nll_loss": "0.019", "train_loss_recon": "0.122", "train_loss_info_nce": "5.761", "train_ppl": "1.01", "train_wps": "1175.8", "train_ups": "0.4", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "6050", "train_lr": "9.55435e-05", "train_gnorm": "2.993", "train_clip": "0", "train_loss_scale": "512", "train_train_wall": "571", "train_gb_free": "2.2", "train_wall": "16288"}
[2025-07-02 00:35:06,340][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 00:35:06,340][fairseq.trainer][INFO] - begin training epoch 26
[2025-07-02 00:35:06,340][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 00:41:03,210][train_inner][INFO] - {"epoch": 26, "update": 25.62, "loss": "6.903", "nll_loss": "0.019", "loss_recon": "0.121", "loss_info_nce": "5.689", "ppl": "1.01", "wps": "1163.2", "ups": "0.39", "wpb": "2956.5", "bsz": "385.4", "num_updates": "6200", "lr": "9.52174e-05", "gnorm": "3.072", "clip": "0", "loss_scale": "1024", "train_wall": "470", "gb_free": "2.2", "wall": "16645"}
[2025-07-02 00:44:40,155][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 26 @ 6292 updates
[2025-07-02 00:44:40,155][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint26.pt
[2025-07-02 00:44:42,009][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint26.pt
[2025-07-02 00:44:42,625][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint26.pt (epoch 26 @ 6292 updates, score None) (writing took 2.482805499999813 seconds)
[2025-07-02 00:44:42,625][fairseq_cli.train][INFO] - end of epoch 26 (average epoch stats below)
[2025-07-02 00:44:42,641][train][INFO] - {"epoch": 26, "train_loss": "6.871", "train_nll_loss": "0.019", "train_loss_recon": "0.121", "train_loss_info_nce": "5.662", "train_ppl": "1.01", "train_wps": "1241.2", "train_ups": "0.42", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "6292", "train_lr": "9.50174e-05", "train_gnorm": "2.835", "train_clip": "0", "train_loss_scale": "1024", "train_train_wall": "571", "train_gb_free": "2.2", "train_wall": "16865"}
[2025-07-02 00:44:42,900][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 00:44:42,900][fairseq.trainer][INFO] - begin training epoch 27
[2025-07-02 00:44:42,900][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 00:49:01,398][train_inner][INFO] - {"epoch": 27, "update": 26.446, "loss": "6.81", "nll_loss": "0.018", "loss_recon": "0.12", "loss_info_nce": "5.61", "ppl": "1.01", "wps": "1235.4", "ups": "0.42", "wpb": "2953.7", "bsz": "386.4", "num_updates": "6400", "lr": "9.47826e-05", "gnorm": "2.808", "clip": "0", "loss_scale": "1024", "train_wall": "473", "gb_free": "2.2", "wall": "17123"}
[2025-07-02 00:54:15,636][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 27 @ 6534 updates
[2025-07-02 00:54:15,636][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint27.pt
[2025-07-02 00:54:17,479][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint27.pt
[2025-07-02 00:54:18,080][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint27.pt (epoch 27 @ 6534 updates, score None) (writing took 2.4449759000017366 seconds)
[2025-07-02 00:54:18,080][fairseq_cli.train][INFO] - end of epoch 27 (average epoch stats below)
[2025-07-02 00:54:18,095][train][INFO] - {"epoch": 27, "train_loss": "6.754", "train_nll_loss": "0.018", "train_loss_recon": "0.12", "train_loss_info_nce": "5.556", "train_ppl": "1.01", "train_wps": "1243.5", "train_ups": "0.42", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "6534", "train_lr": "9.44913e-05", "train_gnorm": "2.674", "train_clip": "0", "train_loss_scale": "1024", "train_train_wall": "570", "train_gb_free": "2.2", "train_wall": "17440"}
[2025-07-02 00:54:18,306][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 00:54:18,306][fairseq.trainer][INFO] - begin training epoch 28
[2025-07-02 00:54:18,306][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 00:56:55,467][train_inner][INFO] - {"epoch": 28, "update": 27.273, "loss": "6.707", "nll_loss": "0.018", "loss_recon": "0.119", "loss_info_nce": "5.513", "ppl": "1.01", "wps": "1247.7", "ups": "0.42", "wpb": "2957.4", "bsz": "384.5", "num_updates": "6600", "lr": "9.43478e-05", "gnorm": "2.511", "clip": "0", "loss_scale": "1024", "train_wall": "468", "gb_free": "2.2", "wall": "17597"}
[2025-07-02 01:03:52,056][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 28 @ 6776 updates
[2025-07-02 01:03:52,065][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint28.pt
[2025-07-02 01:03:53,908][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint28.pt
[2025-07-02 01:03:54,693][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint28.pt (epoch 28 @ 6776 updates, score None) (writing took 2.6311872999976913 seconds)
[2025-07-02 01:03:54,693][fairseq_cli.train][INFO] - end of epoch 28 (average epoch stats below)
[2025-07-02 01:03:54,710][train][INFO] - {"epoch": 28, "train_loss": "6.635", "train_nll_loss": "0.018", "train_loss_recon": "0.119", "train_loss_info_nce": "5.448", "train_ppl": "1.01", "train_wps": "1241", "train_ups": "0.42", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "6776", "train_lr": "9.39652e-05", "train_gnorm": "2.703", "train_clip": "0", "train_loss_scale": "1024", "train_train_wall": "570", "train_gb_free": "2.2", "train_wall": "18017"}
[2025-07-02 01:03:54,977][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 01:03:54,994][fairseq.trainer][INFO] - begin training epoch 29
[2025-07-02 01:03:54,994][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 01:04:53,767][train_inner][INFO] - {"epoch": 29, "update": 28.099, "loss": "6.617", "nll_loss": "0.018", "loss_recon": "0.119", "loss_info_nce": "5.431", "ppl": "1.01", "wps": "1235.1", "ups": "0.42", "wpb": "2953.7", "bsz": "386.2", "num_updates": "6800", "lr": "9.3913e-05", "gnorm": "2.766", "clip": "0", "loss_scale": "1024", "train_wall": "472", "gb_free": "2.2", "wall": "18076"}
[2025-07-02 01:12:47,189][train_inner][INFO] - {"epoch": 29, "update": 28.926, "loss": "6.534", "nll_loss": "0.018", "loss_recon": "0.118", "loss_info_nce": "5.358", "ppl": "1.01", "wps": "1252.1", "ups": "0.42", "wpb": "2963.9", "bsz": "386.8", "num_updates": "7000", "lr": "9.34783e-05", "gnorm": "2.546", "clip": "0", "loss_scale": "1024", "train_wall": "472", "gb_free": "2.2", "wall": "18549"}
[2025-07-02 01:13:28,083][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 29 @ 7018 updates
[2025-07-02 01:13:28,083][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint29.pt
[2025-07-02 01:13:29,939][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint29.pt
[2025-07-02 01:13:30,620][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint29.pt (epoch 29 @ 7018 updates, score None) (writing took 2.5323960999994597 seconds)
[2025-07-02 01:13:30,620][fairseq_cli.train][INFO] - end of epoch 29 (average epoch stats below)
[2025-07-02 01:13:30,630][train][INFO] - {"epoch": 29, "train_loss": "6.537", "train_nll_loss": "0.018", "train_loss_recon": "0.118", "train_loss_info_nce": "5.36", "train_ppl": "1.01", "train_wps": "1242.4", "train_ups": "0.42", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "7018", "train_lr": "9.34391e-05", "train_gnorm": "2.489", "train_clip": "0", "train_loss_scale": "1024", "train_train_wall": "570", "train_gb_free": "2.2", "train_wall": "18593"}
[2025-07-02 01:13:30,857][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 01:13:30,857][fairseq.trainer][INFO] - begin training epoch 30
[2025-07-02 01:13:30,857][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 01:20:43,856][train_inner][INFO] - {"epoch": 30, "update": 29.752, "loss": "6.47", "nll_loss": "0.017", "loss_recon": "0.117", "loss_info_nce": "5.299", "ppl": "1.01", "wps": "1240.1", "ups": "0.42", "wpb": "2955.5", "bsz": "385.8", "num_updates": "7200", "lr": "9.30435e-05", "gnorm": "2.641", "clip": "0", "loss_scale": "1024", "train_wall": "471", "gb_free": "2.2", "wall": "19026"}
[2025-07-02 01:23:04,057][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-02 01:23:33,819][valid][INFO] - {"epoch": 30, "valid_loss": "5.804", "valid_nll_loss": "0.016", "valid_loss_recon": "0.106", "valid_loss_info_nce": "4.744", "valid_ppl": "1.01", "valid_wps": "2776.9", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "7260", "valid_best_loss": "5.804"}
[2025-07-02 01:23:33,819][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 30 @ 7260 updates
[2025-07-02 01:23:33,819][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint30.pt
[2025-07-02 01:23:35,622][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint30.pt
[2025-07-02 01:23:36,857][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint30.pt (epoch 30 @ 7260 updates, score 5.804) (writing took 3.0339615999982925 seconds)
[2025-07-02 01:23:36,857][fairseq_cli.train][INFO] - end of epoch 30 (average epoch stats below)
[2025-07-02 01:23:36,865][train][INFO] - {"epoch": 30, "train_loss": "6.458", "train_nll_loss": "0.017", "train_loss_recon": "0.117", "train_loss_info_nce": "5.288", "train_ppl": "1.01", "train_wps": "1180.3", "train_ups": "0.4", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "7260", "train_lr": "9.2913e-05", "train_gnorm": "2.643", "train_clip": "0", "train_loss_scale": "1024", "train_train_wall": "570", "train_gb_free": "2.2", "train_wall": "19199"}
[2025-07-02 01:23:37,057][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 01:23:37,057][fairseq.trainer][INFO] - begin training epoch 31
[2025-07-02 01:23:37,057][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 01:29:09,961][train_inner][INFO] - {"epoch": 31, "update": 30.579, "loss": "6.397", "nll_loss": "0.017", "loss_recon": "0.116", "loss_info_nce": "5.233", "ppl": "1.01", "wps": "1168.7", "ups": "0.4", "wpb": "2957.4", "bsz": "384.6", "num_updates": "7400", "lr": "9.26087e-05", "gnorm": "2.495", "clip": "0", "loss_scale": "1024", "train_wall": "471", "gb_free": "2.2", "wall": "19532"}
[2025-07-02 01:33:11,570][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 31 @ 7502 updates
[2025-07-02 01:33:11,585][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint31.pt
[2025-07-02 01:33:13,443][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint31.pt
[2025-07-02 01:33:14,128][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint31.pt (epoch 31 @ 7502 updates, score None) (writing took 2.5444466000008106 seconds)
[2025-07-02 01:33:14,128][fairseq_cli.train][INFO] - end of epoch 31 (average epoch stats below)
[2025-07-02 01:33:14,128][train][INFO] - {"epoch": 31, "train_loss": "6.369", "train_nll_loss": "0.017", "train_loss_recon": "0.116", "train_loss_info_nce": "5.208", "train_ppl": "1.01", "train_wps": "1239.5", "train_ups": "0.42", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "7502", "train_lr": "9.2387e-05", "train_gnorm": "2.443", "train_clip": "0", "train_loss_scale": "1024", "train_train_wall": "572", "train_gb_free": "2.2", "train_wall": "19776"}
[2025-07-02 01:33:14,368][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 01:33:14,368][fairseq.trainer][INFO] - begin training epoch 32
[2025-07-02 01:33:14,368][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 01:37:08,819][train_inner][INFO] - {"epoch": 32, "update": 31.405, "loss": "6.336", "nll_loss": "0.017", "loss_recon": "0.116", "loss_info_nce": "5.178", "ppl": "1.01", "wps": "1232.5", "ups": "0.42", "wpb": "2950.9", "bsz": "386.4", "num_updates": "7600", "lr": "9.21739e-05", "gnorm": "2.348", "clip": "0", "loss_scale": "1024", "train_wall": "473", "gb_free": "2.2", "wall": "20011"}
[2025-07-02 01:42:48,665][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 32 @ 7744 updates
[2025-07-02 01:42:48,665][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint32.pt
[2025-07-02 01:42:50,498][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint32.pt
[2025-07-02 01:42:51,057][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint32.pt (epoch 32 @ 7744 updates, score None) (writing took 2.391278000002785 seconds)
[2025-07-02 01:42:51,057][fairseq_cli.train][INFO] - end of epoch 32 (average epoch stats below)
[2025-07-02 01:42:51,067][train][INFO] - {"epoch": 32, "train_loss": "6.301", "train_nll_loss": "0.017", "train_loss_recon": "0.115", "train_loss_info_nce": "5.146", "train_ppl": "1.01", "train_wps": "1240.3", "train_ups": "0.42", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "7744", "train_lr": "9.18609e-05", "train_gnorm": "2.255", "train_clip": "0", "train_loss_scale": "1024", "train_train_wall": "571", "train_gb_free": "2.2", "train_wall": "20353"}
[2025-07-02 01:42:51,268][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 01:42:51,283][fairseq.trainer][INFO] - begin training epoch 33
[2025-07-02 01:42:51,283][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 01:45:05,874][train_inner][INFO] - {"epoch": 33, "update": 32.231, "loss": "6.275", "nll_loss": "0.017", "loss_recon": "0.115", "loss_info_nce": "5.126", "ppl": "1.01", "wps": "1240.3", "ups": "0.42", "wpb": "2958.3", "bsz": "385.3", "num_updates": "7800", "lr": "9.17391e-05", "gnorm": "2.209", "clip": "0", "loss_scale": "1024", "train_wall": "471", "gb_free": "2.2", "wall": "20488"}
[2025-07-02 01:52:26,379][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 33 @ 7986 updates
[2025-07-02 01:52:26,379][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint33.pt
[2025-07-02 01:52:28,224][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint33.pt
[2025-07-02 01:52:28,860][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint33.pt (epoch 33 @ 7986 updates, score None) (writing took 2.485991400000785 seconds)
[2025-07-02 01:52:28,860][fairseq_cli.train][INFO] - end of epoch 33 (average epoch stats below)
[2025-07-02 01:52:28,870][train][INFO] - {"epoch": 33, "train_loss": "6.228", "train_nll_loss": "0.017", "train_loss_recon": "0.115", "train_loss_info_nce": "5.082", "train_ppl": "1.01", "train_wps": "1238.4", "train_ups": "0.42", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "7986", "train_lr": "9.13348e-05", "train_gnorm": "2.21", "train_clip": "0", "train_loss_scale": "1024", "train_train_wall": "572", "train_gb_free": "2.2", "train_wall": "20931"}
[2025-07-02 01:52:29,110][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 01:52:29,110][fairseq.trainer][INFO] - begin training epoch 34
[2025-07-02 01:52:29,110][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 01:53:04,672][train_inner][INFO] - {"epoch": 34, "update": 33.058, "loss": "6.22", "nll_loss": "0.017", "loss_recon": "0.115", "loss_info_nce": "5.073", "ppl": "1.01", "wps": "1233.5", "ups": "0.42", "wpb": "2952.8", "bsz": "386.2", "num_updates": "8000", "lr": "9.13043e-05", "gnorm": "2.21", "clip": "0", "loss_scale": "1024", "train_wall": "473", "gb_free": "2.2", "wall": "20967"}
[2025-07-02 02:00:56,644][train_inner][INFO] - {"epoch": 34, "update": 33.884, "loss": "6.179", "nll_loss": "0.017", "loss_recon": "0.114", "loss_info_nce": "5.037", "ppl": "1.01", "wps": "1257.2", "ups": "0.42", "wpb": "2966.7", "bsz": "386", "num_updates": "8200", "lr": "9.08696e-05", "gnorm": "2.299", "clip": "0", "loss_scale": "2048", "train_wall": "471", "gb_free": "2.2", "wall": "21439"}
[2025-07-02 02:02:02,072][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 34 @ 8228 updates
[2025-07-02 02:02:02,072][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint34.pt
[2025-07-02 02:02:03,930][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint34.pt
[2025-07-02 02:02:04,491][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint34.pt (epoch 34 @ 8228 updates, score None) (writing took 2.4180892999975185 seconds)
[2025-07-02 02:02:04,491][fairseq_cli.train][INFO] - end of epoch 34 (average epoch stats below)
[2025-07-02 02:02:04,507][train][INFO] - {"epoch": 34, "train_loss": "6.173", "train_nll_loss": "0.017", "train_loss_recon": "0.114", "train_loss_info_nce": "5.032", "train_ppl": "1.01", "train_wps": "1243.1", "train_ups": "0.42", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "8228", "train_lr": "9.08087e-05", "train_gnorm": "2.204", "train_clip": "0", "train_loss_scale": "2048", "train_train_wall": "570", "train_gb_free": "2.2", "train_wall": "21506"}
[2025-07-02 02:02:04,715][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 02:02:04,731][fairseq.trainer][INFO] - begin training epoch 35
[2025-07-02 02:02:04,731][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 02:09:03,162][train_inner][INFO] - {"epoch": 35, "update": 34.711, "loss": "6.12", "nll_loss": "0.017", "loss_recon": "0.113", "loss_info_nce": "4.987", "ppl": "1.01", "wps": "1213.8", "ups": "0.41", "wpb": "2952.8", "bsz": "386.7", "num_updates": "8400", "lr": "9.04348e-05", "gnorm": "2.088", "clip": "0", "loss_scale": "2048", "train_wall": "481", "gb_free": "2.2", "wall": "21925"}
[2025-07-02 02:11:45,816][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-02 02:12:15,540][valid][INFO] - {"epoch": 35, "valid_loss": "5.601", "valid_nll_loss": "0.015", "valid_loss_recon": "0.105", "valid_loss_info_nce": "4.549", "valid_ppl": "1.01", "valid_wps": "2769.2", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "8470", "valid_best_loss": "5.601"}
[2025-07-02 02:12:15,540][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 35 @ 8470 updates
[2025-07-02 02:12:15,540][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint35.pt
[2025-07-02 02:12:17,345][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint35.pt
[2025-07-02 02:12:18,709][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint35.pt (epoch 35 @ 8470 updates, score 5.601) (writing took 3.169046900002286 seconds)
[2025-07-02 02:12:18,709][fairseq_cli.train][INFO] - end of epoch 35 (average epoch stats below)
[2025-07-02 02:12:18,751][train][INFO] - {"epoch": 35, "train_loss": "6.115", "train_nll_loss": "0.017", "train_loss_recon": "0.113", "train_loss_info_nce": "4.981", "train_ppl": "1.01", "train_wps": "1165", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "8470", "train_lr": "9.02826e-05", "train_gnorm": "2.088", "train_clip": "0", "train_loss_scale": "2048", "train_train_wall": "578", "train_gb_free": "2.2", "train_wall": "22121"}
[2025-07-02 02:12:18,978][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 02:12:18,978][fairseq.trainer][INFO] - begin training epoch 36
[2025-07-02 02:12:18,978][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 02:17:31,589][train_inner][INFO] - {"epoch": 36, "update": 35.537, "loss": "6.084", "nll_loss": "0.016", "loss_recon": "0.113", "loss_info_nce": "4.953", "ppl": "1.01", "wps": "1162.3", "ups": "0.39", "wpb": "2954.6", "bsz": "385.4", "num_updates": "8600", "lr": "9e-05", "gnorm": "2.056", "clip": "0", "loss_scale": "2048", "train_wall": "471", "gb_free": "2.2", "wall": "22434"}
[2025-07-02 02:21:55,312][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 36 @ 8712 updates
[2025-07-02 02:21:55,312][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint36.pt
[2025-07-02 02:21:57,113][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint36.pt
[2025-07-02 02:21:57,806][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint36.pt (epoch 36 @ 8712 updates, score None) (writing took 2.501906900000904 seconds)
[2025-07-02 02:21:57,806][fairseq_cli.train][INFO] - end of epoch 36 (average epoch stats below)
[2025-07-02 02:21:57,822][train][INFO] - {"epoch": 36, "train_loss": "6.063", "train_nll_loss": "0.016", "train_loss_recon": "0.113", "train_loss_info_nce": "4.937", "train_ppl": "1.01", "train_wps": "1235.7", "train_ups": "0.42", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "8712", "train_lr": "8.97565e-05", "train_gnorm": "2.241", "train_clip": "0", "train_loss_scale": "2048", "train_train_wall": "571", "train_gb_free": "2.2", "train_wall": "22700"}
[2025-07-02 02:21:58,011][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 02:21:58,027][fairseq.trainer][INFO] - begin training epoch 37
[2025-07-02 02:21:58,027][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 02:25:27,481][train_inner][INFO] - {"epoch": 37, "update": 36.364, "loss": "6.042", "nll_loss": "0.016", "loss_recon": "0.112", "loss_info_nce": "4.917", "ppl": "1.01", "wps": "1242.5", "ups": "0.42", "wpb": "2956.5", "bsz": "384.8", "num_updates": "8800", "lr": "8.95652e-05", "gnorm": "2.115", "clip": "0", "loss_scale": "2048", "train_wall": "470", "gb_free": "2.2", "wall": "22909"}
[2025-07-02 02:31:31,018][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 37 @ 8954 updates
[2025-07-02 02:31:31,034][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint37.pt
[2025-07-02 02:31:32,819][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint37.pt
[2025-07-02 02:31:33,580][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint37.pt (epoch 37 @ 8954 updates, score None) (writing took 2.5475647000021127 seconds)
[2025-07-02 02:31:33,580][fairseq_cli.train][INFO] - end of epoch 37 (average epoch stats below)
[2025-07-02 02:31:33,596][train][INFO] - {"epoch": 37, "train_loss": "6.013", "train_nll_loss": "0.016", "train_loss_recon": "0.112", "train_loss_info_nce": "4.891", "train_ppl": "1.01", "train_wps": "1242.8", "train_ups": "0.42", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "8954", "train_lr": "8.92304e-05", "train_gnorm": "2.001", "train_clip": "0", "train_loss_scale": "2048", "train_train_wall": "570", "train_gb_free": "2.2", "train_wall": "23275"}
[2025-07-02 02:31:33,821][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 02:31:33,821][fairseq.trainer][INFO] - begin training epoch 38
[2025-07-02 02:31:33,821][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 02:33:25,148][train_inner][INFO] - {"epoch": 38, "update": 37.19, "loss": "5.996", "nll_loss": "0.016", "loss_recon": "0.112", "loss_info_nce": "4.878", "ppl": "1.01", "wps": "1237.5", "ups": "0.42", "wpb": "2955.5", "bsz": "386.2", "num_updates": "9000", "lr": "8.91304e-05", "gnorm": "2.028", "clip": "0", "loss_scale": "2048", "train_wall": "472", "gb_free": "2.2", "wall": "23387"}
[2025-07-02 02:41:07,494][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 38 @ 9196 updates
[2025-07-02 02:41:07,494][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint38.pt
[2025-07-02 02:41:09,195][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint38.pt
[2025-07-02 02:41:09,807][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint38.pt (epoch 38 @ 9196 updates, score None) (writing took 2.311706000000413 seconds)
[2025-07-02 02:41:09,807][fairseq_cli.train][INFO] - end of epoch 38 (average epoch stats below)
[2025-07-02 02:41:09,823][train][INFO] - {"epoch": 38, "train_loss": "5.964", "train_nll_loss": "0.016", "train_loss_recon": "0.112", "train_loss_info_nce": "4.848", "train_ppl": "1.01", "train_wps": "1241.8", "train_ups": "0.42", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "9196", "train_lr": "8.87043e-05", "train_gnorm": "1.986", "train_clip": "0", "train_loss_scale": "2048", "train_train_wall": "570", "train_gb_free": "2.2", "train_wall": "23852"}
[2025-07-02 02:41:10,057][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 02:41:10,067][fairseq.trainer][INFO] - begin training epoch 39
[2025-07-02 02:41:10,067][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 02:41:21,304][train_inner][INFO] - {"epoch": 39, "update": 38.017, "loss": "5.962", "nll_loss": "0.016", "loss_recon": "0.112", "loss_info_nce": "4.845", "ppl": "1.01", "wps": "1241.4", "ups": "0.42", "wpb": "2955.5", "bsz": "385.4", "num_updates": "9200", "lr": "8.86957e-05", "gnorm": "2.026", "clip": "0", "loss_scale": "2048", "train_wall": "471", "gb_free": "2.2", "wall": "23863"}
[2025-07-02 02:49:14,726][train_inner][INFO] - {"epoch": 39, "update": 38.843, "loss": "5.925", "nll_loss": "0.016", "loss_recon": "0.111", "loss_info_nce": "4.813", "ppl": "1.01", "wps": "1252.1", "ups": "0.42", "wpb": "2963.9", "bsz": "386.8", "num_updates": "9400", "lr": "8.82609e-05", "gnorm": "1.965", "clip": "0", "loss_scale": "2048", "train_wall": "472", "gb_free": "2.2", "wall": "24337"}
[2025-07-02 02:50:43,502][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 39 @ 9438 updates
[2025-07-02 02:50:43,502][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint39.pt
[2025-07-02 02:50:45,288][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint39.pt
[2025-07-02 02:50:45,839][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint39.pt (epoch 39 @ 9438 updates, score None) (writing took 2.340519699999277 seconds)
[2025-07-02 02:50:45,839][fairseq_cli.train][INFO] - end of epoch 39 (average epoch stats below)
[2025-07-02 02:50:45,839][train][INFO] - {"epoch": 39, "train_loss": "5.922", "train_nll_loss": "0.016", "train_loss_recon": "0.111", "train_loss_info_nce": "4.811", "train_ppl": "1.01", "train_wps": "1242.2", "train_ups": "0.42", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "9438", "train_lr": "8.81783e-05", "train_gnorm": "1.936", "train_clip": "0", "train_loss_scale": "2048", "train_train_wall": "570", "train_gb_free": "2.2", "train_wall": "24428"}
[2025-07-02 02:50:46,096][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 02:50:46,096][fairseq.trainer][INFO] - begin training epoch 40
[2025-07-02 02:50:46,096][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 02:57:12,396][train_inner][INFO] - {"epoch": 40, "update": 39.669, "loss": "5.888", "nll_loss": "0.016", "loss_recon": "0.11", "loss_info_nce": "4.783", "ppl": "1.01", "wps": "1237.1", "ups": "0.42", "wpb": "2954.6", "bsz": "386.1", "num_updates": "9600", "lr": "8.78261e-05", "gnorm": "1.937", "clip": "0", "loss_scale": "2048", "train_wall": "472", "gb_free": "2.2", "wall": "24814"}
[2025-07-02 03:00:19,793][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-02 03:00:49,485][valid][INFO] - {"epoch": 40, "valid_loss": "5.392", "valid_nll_loss": "0.015", "valid_loss_recon": "0.101", "valid_loss_info_nce": "4.381", "valid_ppl": "1.01", "valid_wps": "2776.8", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "9680", "valid_best_loss": "5.392"}
[2025-07-02 03:00:49,485][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 40 @ 9680 updates
[2025-07-02 03:00:49,493][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint40.pt
[2025-07-02 03:00:51,290][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint40.pt
[2025-07-02 03:00:52,818][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint40.pt (epoch 40 @ 9680 updates, score 5.392) (writing took 3.3321238000025915 seconds)
[2025-07-02 03:00:52,818][fairseq_cli.train][INFO] - end of epoch 40 (average epoch stats below)
[2025-07-02 03:00:52,818][train][INFO] - {"epoch": 40, "train_loss": "5.883", "train_nll_loss": "0.016", "train_loss_recon": "0.111", "train_loss_info_nce": "4.776", "train_ppl": "1.01", "train_wps": "1178.9", "train_ups": "0.4", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "9680", "train_lr": "8.76522e-05", "train_gnorm": "1.951", "train_clip": "0", "train_loss_scale": "2048", "train_train_wall": "570", "train_gb_free": "2.2", "train_wall": "25035"}
[2025-07-02 03:00:53,008][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 03:00:53,008][fairseq.trainer][INFO] - begin training epoch 41
[2025-07-02 03:00:53,008][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 03:05:40,213][train_inner][INFO] - {"epoch": 41, "update": 40.496, "loss": "5.86", "nll_loss": "0.016", "loss_recon": "0.111", "loss_info_nce": "4.752", "ppl": "1.01", "wps": "1163.3", "ups": "0.39", "wpb": "2953.7", "bsz": "386.2", "num_updates": "9800", "lr": "8.73913e-05", "gnorm": "1.849", "clip": "0", "loss_scale": "2048", "train_wall": "472", "gb_free": "2.2", "wall": "25322"}
[2025-07-02 03:10:25,639][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 41 @ 9922 updates
[2025-07-02 03:10:25,639][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint41.pt
[2025-07-02 03:10:27,297][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint41.pt
[2025-07-02 03:10:27,990][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint41.pt (epoch 41 @ 9922 updates, score None) (writing took 2.3546408000001975 seconds)
[2025-07-02 03:10:27,990][fairseq_cli.train][INFO] - end of epoch 41 (average epoch stats below)
[2025-07-02 03:10:27,998][train][INFO] - {"epoch": 41, "train_loss": "5.839", "train_nll_loss": "0.016", "train_loss_recon": "0.11", "train_loss_info_nce": "4.737", "train_ppl": "1.01", "train_wps": "1244.1", "train_ups": "0.42", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "9922", "train_lr": "8.71261e-05", "train_gnorm": "1.929", "train_clip": "0", "train_loss_scale": "2048", "train_train_wall": "569", "train_gb_free": "2.2", "train_wall": "25610"}
[2025-07-02 03:10:28,215][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 03:10:28,215][fairseq.trainer][INFO] - begin training epoch 42
[2025-07-02 03:10:28,215][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 03:13:34,230][train_inner][INFO] - {"epoch": 42, "update": 41.322, "loss": "5.82", "nll_loss": "0.016", "loss_recon": "0.11", "loss_info_nce": "4.721", "ppl": "1.01", "wps": "1248.2", "ups": "0.42", "wpb": "2958.3", "bsz": "384.3", "num_updates": "10000", "lr": "8.69565e-05", "gnorm": "1.906", "clip": "0", "loss_scale": "2048", "train_wall": "468", "gb_free": "2.2", "wall": "25796"}
[2025-07-02 03:13:34,246][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-02 03:14:04,629][valid][INFO] - {"epoch": 42, "valid_loss": "5.331", "valid_nll_loss": "0.014", "valid_loss_recon": "0.099", "valid_loss_info_nce": "4.343", "valid_ppl": "1.01", "valid_wps": "2715.7", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "10000", "valid_best_loss": "5.331"}
[2025-07-02 03:20:31,653][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 42 @ 10164 updates
[2025-07-02 03:20:31,663][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint42.pt
[2025-07-02 03:20:33,483][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint42.pt
[2025-07-02 03:20:34,058][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint42.pt (epoch 42 @ 10164 updates, score None) (writing took 2.402704000000085 seconds)
[2025-07-02 03:20:34,058][fairseq_cli.train][INFO] - end of epoch 42 (average epoch stats below)
[2025-07-02 03:20:34,068][train][INFO] - {"epoch": 42, "train_loss": "5.793", "train_nll_loss": "0.016", "train_loss_recon": "0.11", "train_loss_info_nce": "4.692", "train_ppl": "1.01", "train_wps": "1180.6", "train_ups": "0.4", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "10164", "train_lr": "8.66e-05", "train_gnorm": "1.74", "train_clip": "0", "train_loss_scale": "2048", "train_train_wall": "570", "train_gb_free": "2.2", "train_wall": "26216"}
[2025-07-02 03:20:34,283][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 03:20:34,283][fairseq.trainer][INFO] - begin training epoch 43
[2025-07-02 03:20:34,283][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 03:22:01,106][train_inner][INFO] - {"epoch": 43, "update": 42.149, "loss": "5.784", "nll_loss": "0.016", "loss_recon": "0.11", "loss_info_nce": "4.683", "ppl": "1.01", "wps": "1166.2", "ups": "0.39", "wpb": "2955.5", "bsz": "385.6", "num_updates": "10200", "lr": "8.65217e-05", "gnorm": "1.698", "clip": "0", "loss_scale": "2048", "train_wall": "471", "gb_free": "2.2", "wall": "26303"}
[2025-07-02 03:29:54,502][train_inner][INFO] - {"epoch": 43, "update": 42.975, "loss": "5.747", "nll_loss": "0.016", "loss_recon": "0.109", "loss_info_nce": "4.654", "ppl": "1.01", "wps": "1252.2", "ups": "0.42", "wpb": "2963.9", "bsz": "386.8", "num_updates": "10400", "lr": "8.6087e-05", "gnorm": "1.807", "clip": "0", "loss_scale": "4096", "train_wall": "472", "gb_free": "2.2", "wall": "26776"}
[2025-07-02 03:30:07,257][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 43 @ 10406 updates
[2025-07-02 03:30:07,257][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint43.pt
[2025-07-02 03:30:14,878][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint43.pt
[2025-07-02 03:30:15,536][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint43.pt (epoch 43 @ 10406 updates, score None) (writing took 8.277693800002453 seconds)
[2025-07-02 03:30:15,536][fairseq_cli.train][INFO] - end of epoch 43 (average epoch stats below)
[2025-07-02 03:30:15,552][train][INFO] - {"epoch": 43, "train_loss": "5.75", "train_nll_loss": "0.016", "train_loss_recon": "0.109", "train_loss_info_nce": "4.657", "train_ppl": "1.01", "train_wps": "1230.6", "train_ups": "0.42", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "10406", "train_lr": "8.60739e-05", "train_gnorm": "1.753", "train_clip": "0", "train_loss_scale": "4096", "train_train_wall": "569", "train_gb_free": "2.2", "train_wall": "26797"}
[2025-07-02 03:30:15,779][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 03:30:15,779][fairseq.trainer][INFO] - begin training epoch 44
[2025-07-02 03:30:15,779][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 03:38:03,465][train_inner][INFO] - {"epoch": 44, "update": 43.802, "loss": "5.709", "nll_loss": "0.015", "loss_recon": "0.109", "loss_info_nce": "4.619", "ppl": "1.01", "wps": "1209.3", "ups": "0.41", "wpb": "2956.5", "bsz": "385.4", "num_updates": "10600", "lr": "8.56522e-05", "gnorm": "1.783", "clip": "0", "loss_scale": "4096", "train_wall": "478", "gb_free": "2.2", "wall": "27265"}
[2025-07-02 03:40:19,768][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 44 @ 10648 updates
[2025-07-02 03:40:19,768][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint44.pt
[2025-07-02 03:40:23,891][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint44.pt
[2025-07-02 03:40:24,499][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint44.pt (epoch 44 @ 10648 updates, score None) (writing took 4.722889100001339 seconds)
[2025-07-02 03:40:24,499][fairseq_cli.train][INFO] - end of epoch 44 (average epoch stats below)
[2025-07-02 03:40:24,509][train][INFO] - {"epoch": 44, "train_loss": "5.701", "train_nll_loss": "0.015", "train_loss_recon": "0.109", "train_loss_info_nce": "4.614", "train_ppl": "1.01", "train_wps": "1175", "train_ups": "0.4", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "10648", "train_lr": "8.55478e-05", "train_gnorm": "1.739", "train_clip": "0", "train_loss_scale": "4096", "train_train_wall": "601", "train_gb_free": "2.2", "train_wall": "27406"}
[2025-07-02 03:40:24,718][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 03:40:24,718][fairseq.trainer][INFO] - begin training epoch 45
[2025-07-02 03:40:24,718][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 03:47:36,164][train_inner][INFO] - {"epoch": 45, "update": 44.628, "loss": "5.673", "nll_loss": "0.015", "loss_recon": "0.108", "loss_info_nce": "4.588", "ppl": "1.01", "wps": "1030.8", "ups": "0.35", "wpb": "2951.8", "bsz": "386.2", "num_updates": "10800", "lr": "8.52174e-05", "gnorm": "1.647", "clip": "0", "loss_scale": "4096", "train_wall": "565", "gb_free": "2.2", "wall": "27838"}
[2025-07-02 03:51:27,492][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-02 03:51:57,824][valid][INFO] - {"epoch": 45, "valid_loss": "5.244", "valid_nll_loss": "0.014", "valid_loss_recon": "0.1", "valid_loss_info_nce": "4.245", "valid_ppl": "1.01", "valid_wps": "2723", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "10890", "valid_best_loss": "5.244"}
[2025-07-02 03:51:57,824][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 45 @ 10890 updates
[2025-07-02 03:51:57,824][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint45.pt
[2025-07-02 03:51:59,656][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint45.pt
[2025-07-02 03:52:01,500][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint45.pt (epoch 45 @ 10890 updates, score 5.244) (writing took 3.6701103999985207 seconds)
[2025-07-02 03:52:01,501][fairseq_cli.train][INFO] - end of epoch 45 (average epoch stats below)
[2025-07-02 03:52:01,526][train][INFO] - {"epoch": 45, "train_loss": "5.664", "train_nll_loss": "0.015", "train_loss_recon": "0.108", "train_loss_info_nce": "4.58", "train_ppl": "1.01", "train_wps": "1026.6", "train_ups": "0.35", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "10890", "train_lr": "8.50217e-05", "train_gnorm": "1.648", "train_clip": "0", "train_loss_scale": "4096", "train_train_wall": "660", "train_gb_free": "2.2", "train_wall": "28103"}
[2025-07-02 03:52:01,741][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 03:52:01,741][fairseq.trainer][INFO] - begin training epoch 46
[2025-07-02 03:52:01,741][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 03:56:58,350][train_inner][INFO] - {"epoch": 46, "update": 45.455, "loss": "5.647", "nll_loss": "0.015", "loss_recon": "0.108", "loss_info_nce": "4.565", "ppl": "1.01", "wps": "1050.5", "ups": "0.36", "wpb": "2952.8", "bsz": "386.6", "num_updates": "11000", "lr": "8.47826e-05", "gnorm": "1.606", "clip": "0", "loss_scale": "4096", "train_wall": "525", "gb_free": "2.2", "wall": "28400"}
[2025-07-02 04:02:45,327][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 46 @ 11132 updates
[2025-07-02 04:02:45,327][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint46.pt
[2025-07-02 04:02:47,115][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint46.pt
[2025-07-02 04:02:47,813][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint46.pt (epoch 46 @ 11132 updates, score None) (writing took 2.494800000000396 seconds)
[2025-07-02 04:02:47,813][fairseq_cli.train][INFO] - end of epoch 46 (average epoch stats below)
[2025-07-02 04:02:47,829][train][INFO] - {"epoch": 46, "train_loss": "5.635", "train_nll_loss": "0.015", "train_loss_recon": "0.108", "train_loss_info_nce": "4.553", "train_ppl": "1.01", "train_wps": "1107.1", "train_ups": "0.37", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "11132", "train_lr": "8.44957e-05", "train_gnorm": "1.731", "train_clip": "0", "train_loss_scale": "4096", "train_train_wall": "640", "train_gb_free": "2.2", "train_wall": "28750"}
[2025-07-02 04:02:48,045][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 04:02:48,045][fairseq.trainer][INFO] - begin training epoch 47
[2025-07-02 04:02:48,045][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 04:06:05,365][train_inner][INFO] - {"epoch": 47, "update": 46.281, "loss": "5.625", "nll_loss": "0.015", "loss_recon": "0.108", "loss_info_nce": "4.542", "ppl": "1.01", "wps": "1082", "ups": "0.37", "wpb": "2959.3", "bsz": "384.1", "num_updates": "11200", "lr": "8.43478e-05", "gnorm": "1.722", "clip": "0", "loss_scale": "4096", "train_wall": "541", "gb_free": "2.2", "wall": "28947"}
[2025-07-02 04:13:53,907][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 47 @ 11374 updates
[2025-07-02 04:13:53,907][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint47.pt
[2025-07-02 04:13:55,773][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint47.pt
[2025-07-02 04:13:56,372][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint47.pt (epoch 47 @ 11374 updates, score None) (writing took 2.4658398999999918 seconds)
[2025-07-02 04:13:56,372][fairseq_cli.train][INFO] - end of epoch 47 (average epoch stats below)
[2025-07-02 04:13:56,387][train][INFO] - {"epoch": 47, "train_loss": "5.605", "train_nll_loss": "0.015", "train_loss_recon": "0.108", "train_loss_info_nce": "4.528", "train_ppl": "1.01", "train_wps": "1070.3", "train_ups": "0.36", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "11374", "train_lr": "8.39696e-05", "train_gnorm": "1.652", "train_clip": "0", "train_loss_scale": "4096", "train_train_wall": "663", "train_gb_free": "2.2", "train_wall": "29418"}
[2025-07-02 04:13:56,623][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 04:13:56,623][fairseq.trainer][INFO] - begin training epoch 48
[2025-07-02 04:13:56,623][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 04:15:01,708][train_inner][INFO] - {"epoch": 48, "update": 47.107, "loss": "5.599", "nll_loss": "0.015", "loss_recon": "0.108", "loss_info_nce": "4.524", "ppl": "1.01", "wps": "1101.8", "ups": "0.37", "wpb": "2954.6", "bsz": "386.1", "num_updates": "11400", "lr": "8.3913e-05", "gnorm": "1.694", "clip": "0", "loss_scale": "4096", "train_wall": "530", "gb_free": "2.2", "wall": "29484"}
[2025-07-02 04:23:04,308][train_inner][INFO] - {"epoch": 48, "update": 47.934, "loss": "5.574", "nll_loss": "0.015", "loss_recon": "0.107", "loss_info_nce": "4.504", "ppl": "1.01", "wps": "1228.7", "ups": "0.41", "wpb": "2964.8", "bsz": "387.1", "num_updates": "11600", "lr": "8.34783e-05", "gnorm": "1.46", "clip": "0", "loss_scale": "4096", "train_wall": "481", "gb_free": "2.2", "wall": "29966"}
[2025-07-02 04:23:40,616][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 48 @ 11616 updates
[2025-07-02 04:23:40,631][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint48.pt
[2025-07-02 04:23:42,491][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint48.pt
[2025-07-02 04:23:43,197][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint48.pt (epoch 48 @ 11616 updates, score None) (writing took 2.574857300001895 seconds)
[2025-07-02 04:23:43,197][fairseq_cli.train][INFO] - end of epoch 48 (average epoch stats below)
[2025-07-02 04:23:43,207][train][INFO] - {"epoch": 48, "train_loss": "5.578", "train_nll_loss": "0.015", "train_loss_recon": "0.107", "train_loss_info_nce": "4.504", "train_ppl": "1.01", "train_wps": "1219.4", "train_ups": "0.41", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "11616", "train_lr": "8.34435e-05", "train_gnorm": "1.474", "train_clip": "0", "train_loss_scale": "4096", "train_train_wall": "580", "train_gb_free": "2.2", "train_wall": "30005"}
[2025-07-02 04:23:43,423][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 04:23:43,423][fairseq.trainer][INFO] - begin training epoch 49
[2025-07-02 04:23:43,423][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 04:31:24,308][train_inner][INFO] - {"epoch": 49, "update": 48.76, "loss": "5.561", "nll_loss": "0.015", "loss_recon": "0.107", "loss_info_nce": "4.488", "ppl": "1.01", "wps": "1182.2", "ups": "0.4", "wpb": "2955.5", "bsz": "385.4", "num_updates": "11800", "lr": "8.30435e-05", "gnorm": "1.563", "clip": "0", "loss_scale": "4096", "train_wall": "494", "gb_free": "2.2", "wall": "30466"}
[2025-07-02 04:33:44,002][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 49 @ 11858 updates
[2025-07-02 04:33:44,003][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint49.pt
[2025-07-02 04:33:45,843][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint49.pt
[2025-07-02 04:33:46,469][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint49.pt (epoch 49 @ 11858 updates, score None) (writing took 2.4680680999990727 seconds)
[2025-07-02 04:33:46,469][fairseq_cli.train][INFO] - end of epoch 49 (average epoch stats below)
[2025-07-02 04:33:46,469][train][INFO] - {"epoch": 49, "train_loss": "5.556", "train_nll_loss": "0.015", "train_loss_recon": "0.107", "train_loss_info_nce": "4.485", "train_ppl": "1.01", "train_wps": "1186.1", "train_ups": "0.4", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "11858", "train_lr": "8.29174e-05", "train_gnorm": "1.639", "train_clip": "0", "train_loss_scale": "4096", "train_train_wall": "597", "train_gb_free": "2.2", "train_wall": "30608"}
[2025-07-02 04:33:46,667][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 04:33:46,667][fairseq.trainer][INFO] - begin training epoch 50
[2025-07-02 04:33:46,667][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 04:39:42,020][train_inner][INFO] - {"epoch": 50, "update": 49.587, "loss": "5.54", "nll_loss": "0.015", "loss_recon": "0.107", "loss_info_nce": "4.469", "ppl": "1.01", "wps": "1186.2", "ups": "0.4", "wpb": "2951.8", "bsz": "386.3", "num_updates": "12000", "lr": "8.26087e-05", "gnorm": "1.476", "clip": "0", "loss_scale": "4096", "train_wall": "492", "gb_free": "2.2", "wall": "30964"}
[2025-07-02 04:44:15,790][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-02 04:45:10,286][valid][INFO] - {"epoch": 50, "valid_loss": "5.141", "valid_nll_loss": "0.014", "valid_loss_recon": "0.098", "valid_loss_info_nce": "4.157", "valid_ppl": "1.01", "valid_wps": "1512", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "12100", "valid_best_loss": "5.141"}
[2025-07-02 04:45:10,286][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 50 @ 12100 updates
[2025-07-02 04:45:10,286][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint50.pt
[2025-07-02 04:45:12,359][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint50.pt
[2025-07-02 04:45:14,870][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint50.pt (epoch 50 @ 12100 updates, score 5.141) (writing took 4.595905700000003 seconds)
[2025-07-02 04:45:14,887][fairseq_cli.train][INFO] - end of epoch 50 (average epoch stats below)
[2025-07-02 04:45:14,930][train][INFO] - {"epoch": 50, "train_loss": "5.53", "train_nll_loss": "0.015", "train_loss_recon": "0.107", "train_loss_info_nce": "4.463", "train_ppl": "1.01", "train_wps": "1039.4", "train_ups": "0.35", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "12100", "train_lr": "8.23913e-05", "train_gnorm": "1.356", "train_clip": "0", "train_loss_scale": "4096", "train_train_wall": "626", "train_gb_free": "2.2", "train_wall": "31297"}
[2025-07-02 04:45:15,313][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 04:45:15,313][fairseq.trainer][INFO] - begin training epoch 51
[2025-07-02 04:45:15,313][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 04:50:01,545][train_inner][INFO] - {"epoch": 51, "update": 50.413, "loss": "5.521", "nll_loss": "0.015", "loss_recon": "0.107", "loss_info_nce": "4.455", "ppl": "1.01", "wps": "955.9", "ups": "0.32", "wpb": "2961.1", "bsz": "384.2", "num_updates": "12200", "lr": "8.21739e-05", "gnorm": "1.496", "clip": "0", "loss_scale": "4096", "train_wall": "555", "gb_free": "2.2", "wall": "31583"}
[2025-07-02 04:56:43,365][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 51 @ 12342 updates
[2025-07-02 04:56:43,365][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint51.pt
[2025-07-02 04:56:45,364][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint51.pt
[2025-07-02 04:56:46,651][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint51.pt (epoch 51 @ 12342 updates, score None) (writing took 3.276337399998738 seconds)
[2025-07-02 04:56:46,653][fairseq_cli.train][INFO] - end of epoch 51 (average epoch stats below)
[2025-07-02 04:56:46,672][train][INFO] - {"epoch": 51, "train_loss": "5.511", "train_nll_loss": "0.015", "train_loss_recon": "0.107", "train_loss_info_nce": "4.445", "train_ppl": "1.01", "train_wps": "1034.4", "train_ups": "0.35", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "12342", "train_lr": "8.18652e-05", "train_gnorm": "1.616", "train_clip": "0", "train_loss_scale": "8192", "train_train_wall": "682", "train_gb_free": "2.2", "train_wall": "31989"}
[2025-07-02 04:56:47,069][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 04:56:47,076][fairseq.trainer][INFO] - begin training epoch 52
[2025-07-02 04:56:47,076][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 04:59:36,449][train_inner][INFO] - {"epoch": 52, "update": 51.24, "loss": "5.502", "nll_loss": "0.015", "loss_recon": "0.106", "loss_info_nce": "4.439", "ppl": "1.01", "wps": "1027.5", "ups": "0.35", "wpb": "2953.7", "bsz": "385.6", "num_updates": "12400", "lr": "8.17391e-05", "gnorm": "1.574", "clip": "0", "loss_scale": "8192", "train_wall": "565", "gb_free": "2.2", "wall": "32158"}
[2025-07-02 05:08:17,824][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 52 @ 12584 updates
[2025-07-02 05:08:17,824][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint52.pt
[2025-07-02 05:08:19,626][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint52.pt
[2025-07-02 05:08:20,479][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint52.pt (epoch 52 @ 12584 updates, score None) (writing took 2.6481130000029225 seconds)
[2025-07-02 05:08:20,479][fairseq_cli.train][INFO] - end of epoch 52 (average epoch stats below)
[2025-07-02 05:08:20,495][train][INFO] - {"epoch": 52, "train_loss": "5.487", "train_nll_loss": "0.015", "train_loss_recon": "0.106", "train_loss_info_nce": "4.424", "train_ppl": "1.01", "train_wps": "1031.3", "train_ups": "0.35", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "12584", "train_lr": "8.13391e-05", "train_gnorm": "1.343", "train_clip": "0", "train_loss_scale": "8192", "train_train_wall": "683", "train_gb_free": "2.2", "train_wall": "32682"}
[2025-07-02 05:08:20,721][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 05:08:20,721][fairseq.trainer][INFO] - begin training epoch 53
[2025-07-02 05:08:20,721][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 05:09:04,896][train_inner][INFO] - {"epoch": 53, "update": 52.066, "loss": "5.484", "nll_loss": "0.015", "loss_recon": "0.106", "loss_info_nce": "4.42", "ppl": "1.01", "wps": "1039.5", "ups": "0.35", "wpb": "2954.6", "bsz": "385.9", "num_updates": "12600", "lr": "8.13043e-05", "gnorm": "1.343", "clip": "0", "loss_scale": "8192", "train_wall": "560", "gb_free": "2.2", "wall": "32727"}
[2025-07-02 05:17:20,439][train_inner][INFO] - {"epoch": 53, "update": 52.893, "loss": "5.469", "nll_loss": "0.015", "loss_recon": "0.106", "loss_info_nce": "4.408", "ppl": "1.01", "wps": "1196.2", "ups": "0.4", "wpb": "2963.9", "bsz": "386.8", "num_updates": "12800", "lr": "8.08696e-05", "gnorm": "1.601", "clip": "0", "loss_scale": "8192", "train_wall": "494", "gb_free": "2.2", "wall": "33222"}
[2025-07-02 05:18:21,956][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 53 @ 12826 updates
[2025-07-02 05:18:21,956][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint53.pt
[2025-07-02 05:18:23,731][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint53.pt
[2025-07-02 05:18:24,406][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint53.pt (epoch 53 @ 12826 updates, score None) (writing took 2.4493605000025127 seconds)
[2025-07-02 05:18:24,416][fairseq_cli.train][INFO] - end of epoch 53 (average epoch stats below)
[2025-07-02 05:18:24,416][train][INFO] - {"epoch": 53, "train_loss": "5.468", "train_nll_loss": "0.015", "train_loss_recon": "0.106", "train_loss_info_nce": "4.408", "train_ppl": "1.01", "train_wps": "1184.8", "train_ups": "0.4", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "12826", "train_lr": "8.0813e-05", "train_gnorm": "1.666", "train_clip": "0", "train_loss_scale": "8192", "train_train_wall": "598", "train_gb_free": "2.2", "train_wall": "33286"}
[2025-07-02 05:18:24,616][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 05:18:24,616][fairseq.trainer][INFO] - begin training epoch 54
[2025-07-02 05:18:24,616][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 05:25:27,587][train_inner][INFO] - {"epoch": 54, "update": 53.719, "loss": "5.455", "nll_loss": "0.015", "loss_recon": "0.106", "loss_info_nce": "4.397", "ppl": "1.01", "wps": "1213.8", "ups": "0.41", "wpb": "2956.5", "bsz": "385.1", "num_updates": "13000", "lr": "8.04348e-05", "gnorm": "1.42", "clip": "0", "loss_scale": "8192", "train_wall": "481", "gb_free": "2.2", "wall": "33709"}
[2025-07-02 05:28:14,348][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 54 @ 13068 updates
[2025-07-02 05:28:14,348][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint54.pt
[2025-07-02 05:28:16,217][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint54.pt
[2025-07-02 05:28:16,859][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint54.pt (epoch 54 @ 13068 updates, score None) (writing took 2.510075099999085 seconds)
[2025-07-02 05:28:16,859][fairseq_cli.train][INFO] - end of epoch 54 (average epoch stats below)
[2025-07-02 05:28:16,859][train][INFO] - {"epoch": 54, "train_loss": "5.452", "train_nll_loss": "0.015", "train_loss_recon": "0.106", "train_loss_info_nce": "4.393", "train_ppl": "1.01", "train_wps": "1207.8", "train_ups": "0.41", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "13068", "train_lr": "8.0287e-05", "train_gnorm": "1.368", "train_clip": "0", "train_loss_scale": "8192", "train_train_wall": "586", "train_gb_free": "2.2", "train_wall": "33879"}
[2025-07-02 05:28:17,075][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 05:28:17,075][fairseq.trainer][INFO] - begin training epoch 55
[2025-07-02 05:28:17,075][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 05:33:59,626][train_inner][INFO] - {"epoch": 55, "update": 54.545, "loss": "5.446", "nll_loss": "0.015", "loss_recon": "0.106", "loss_info_nce": "4.386", "ppl": "1.01", "wps": "1154.4", "ups": "0.39", "wpb": "2955.5", "bsz": "385.6", "num_updates": "13200", "lr": "8e-05", "gnorm": "1.513", "clip": "0", "loss_scale": "8192", "train_wall": "506", "gb_free": "2.2", "wall": "34222"}
[2025-07-02 05:38:21,415][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-02 05:38:50,937][valid][INFO] - {"epoch": 55, "valid_loss": "5.091", "valid_nll_loss": "0.014", "valid_loss_recon": "0.098", "valid_loss_info_nce": "4.107", "valid_ppl": "1.01", "valid_wps": "2804.6", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "13310", "valid_best_loss": "5.091"}
[2025-07-02 05:38:50,937][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 55 @ 13310 updates
[2025-07-02 05:38:50,945][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint55.pt
[2025-07-02 05:38:52,735][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint55.pt
[2025-07-02 05:38:54,106][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint55.pt (epoch 55 @ 13310 updates, score 5.091) (writing took 3.1635242000047583 seconds)
[2025-07-02 05:38:54,106][fairseq_cli.train][INFO] - end of epoch 55 (average epoch stats below)
[2025-07-02 05:38:54,132][train][INFO] - {"epoch": 55, "train_loss": "5.438", "train_nll_loss": "0.015", "train_loss_recon": "0.106", "train_loss_info_nce": "4.381", "train_ppl": "1.01", "train_wps": "1122.9", "train_ups": "0.38", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "13310", "train_lr": "7.97609e-05", "train_gnorm": "1.442", "train_clip": "0", "train_loss_scale": "8192", "train_train_wall": "601", "train_gb_free": "2.2", "train_wall": "34516"}
[2025-07-02 05:38:54,374][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 05:38:54,374][fairseq.trainer][INFO] - begin training epoch 56
[2025-07-02 05:38:54,374][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 05:42:38,131][train_inner][INFO] - {"epoch": 56, "update": 55.372, "loss": "5.429", "nll_loss": "0.015", "loss_recon": "0.105", "loss_info_nce": "4.376", "ppl": "1.01", "wps": "1139.3", "ups": "0.39", "wpb": "2953.7", "bsz": "385.9", "num_updates": "13400", "lr": "7.95652e-05", "gnorm": "1.425", "clip": "0", "loss_scale": "8192", "train_wall": "483", "gb_free": "2.2", "wall": "34740"}
[2025-07-02 05:48:43,059][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 56 @ 13552 updates
[2025-07-02 05:48:43,075][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint56.pt
[2025-07-02 05:48:44,919][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint56.pt
[2025-07-02 05:48:45,519][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint56.pt (epoch 56 @ 13552 updates, score None) (writing took 2.4592336999994586 seconds)
[2025-07-02 05:48:45,519][fairseq_cli.train][INFO] - end of epoch 56 (average epoch stats below)
[2025-07-02 05:48:45,535][train][INFO] - {"epoch": 56, "train_loss": "5.423", "train_nll_loss": "0.015", "train_loss_recon": "0.105", "train_loss_info_nce": "4.369", "train_ppl": "1.01", "train_wps": "1209.9", "train_ups": "0.41", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "13552", "train_lr": "7.92348e-05", "train_gnorm": "1.384", "train_clip": "0", "train_loss_scale": "8192", "train_train_wall": "586", "train_gb_free": "2.2", "train_wall": "35107"}
[2025-07-02 05:48:45,762][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 05:48:45,762][fairseq.trainer][INFO] - begin training epoch 57
[2025-07-02 05:48:45,762][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 05:50:43,886][train_inner][INFO] - {"epoch": 57, "update": 56.198, "loss": "5.417", "nll_loss": "0.015", "loss_recon": "0.105", "loss_info_nce": "4.363", "ppl": "1.01", "wps": "1215.4", "ups": "0.41", "wpb": "2951.8", "bsz": "386.9", "num_updates": "13600", "lr": "7.91304e-05", "gnorm": "1.281", "clip": "0", "loss_scale": "8192", "train_wall": "480", "gb_free": "2.2", "wall": "35226"}
[2025-07-02 05:58:24,224][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 57 @ 13794 updates
[2025-07-02 05:58:24,239][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint57.pt
[2025-07-02 05:58:26,109][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint57.pt
[2025-07-02 05:58:26,725][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint57.pt (epoch 57 @ 13794 updates, score None) (writing took 2.489883499998541 seconds)
[2025-07-02 05:58:26,725][fairseq_cli.train][INFO] - end of epoch 57 (average epoch stats below)
[2025-07-02 05:58:26,741][train][INFO] - {"epoch": 57, "train_loss": "5.407", "train_nll_loss": "0.015", "train_loss_recon": "0.105", "train_loss_info_nce": "4.355", "train_ppl": "1.01", "train_wps": "1231.2", "train_ups": "0.42", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "13794", "train_lr": "7.87087e-05", "train_gnorm": "1.246", "train_clip": "0", "train_loss_scale": "8192", "train_train_wall": "575", "train_gb_free": "2.2", "train_wall": "35689"}
[2025-07-02 05:58:26,910][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 05:58:26,910][fairseq.trainer][INFO] - begin training epoch 58
[2025-07-02 05:58:26,910][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 05:58:43,081][train_inner][INFO] - {"epoch": 58, "update": 57.025, "loss": "5.405", "nll_loss": "0.015", "loss_recon": "0.105", "loss_info_nce": "4.354", "ppl": "1.01", "wps": "1234.7", "ups": "0.42", "wpb": "2958.3", "bsz": "384.6", "num_updates": "13800", "lr": "7.86957e-05", "gnorm": "1.265", "clip": "0", "loss_scale": "8192", "train_wall": "474", "gb_free": "2.2", "wall": "35705"}
[2025-07-02 06:06:38,067][train_inner][INFO] - {"epoch": 58, "update": 57.851, "loss": "5.392", "nll_loss": "0.015", "loss_recon": "0.105", "loss_info_nce": "4.343", "ppl": "1.01", "wps": "1248", "ups": "0.42", "wpb": "2963.9", "bsz": "386.8", "num_updates": "14000", "lr": "7.82609e-05", "gnorm": "1.416", "clip": "0", "loss_scale": "8192", "train_wall": "474", "gb_free": "2.2", "wall": "36180"}
[2025-07-02 06:08:02,563][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 58 @ 14036 updates
[2025-07-02 06:08:02,563][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint58.pt
[2025-07-02 06:08:04,427][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint58.pt
[2025-07-02 06:08:05,069][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint58.pt (epoch 58 @ 14036 updates, score None) (writing took 2.509867499997199 seconds)
[2025-07-02 06:08:05,069][fairseq_cli.train][INFO] - end of epoch 58 (average epoch stats below)
[2025-07-02 06:08:05,084][train][INFO] - {"epoch": 58, "train_loss": "5.392", "train_nll_loss": "0.015", "train_loss_recon": "0.105", "train_loss_info_nce": "4.343", "train_ppl": "1.01", "train_wps": "1237.3", "train_ups": "0.42", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "14036", "train_lr": "7.81826e-05", "train_gnorm": "1.367", "train_clip": "0", "train_loss_scale": "8192", "train_train_wall": "573", "train_gb_free": "2.2", "train_wall": "36267"}
[2025-07-02 06:08:05,288][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 06:08:05,288][fairseq.trainer][INFO] - begin training epoch 59
[2025-07-02 06:08:05,288][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 06:14:37,831][train_inner][INFO] - {"epoch": 59, "update": 58.678, "loss": "5.383", "nll_loss": "0.015", "loss_recon": "0.105", "loss_info_nce": "4.334", "ppl": "1.01", "wps": "1231.3", "ups": "0.42", "wpb": "2953.7", "bsz": "386", "num_updates": "14200", "lr": "7.78261e-05", "gnorm": "1.442", "clip": "0", "loss_scale": "8192", "train_wall": "475", "gb_free": "2.2", "wall": "36660"}
[2025-07-02 06:17:41,344][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 59 @ 14278 updates
[2025-07-02 06:17:41,344][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint59.pt
[2025-07-02 06:17:43,130][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint59.pt
[2025-07-02 06:17:43,704][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint59.pt (epoch 59 @ 14278 updates, score None) (writing took 2.3580148999972153 seconds)
[2025-07-02 06:17:43,704][fairseq_cli.train][INFO] - end of epoch 59 (average epoch stats below)
[2025-07-02 06:17:43,704][train][INFO] - {"epoch": 59, "train_loss": "5.38", "train_nll_loss": "0.015", "train_loss_recon": "0.105", "train_loss_info_nce": "4.332", "train_ppl": "1.01", "train_wps": "1236.7", "train_ups": "0.42", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "14278", "train_lr": "7.76565e-05", "train_gnorm": "1.505", "train_clip": "0", "train_loss_scale": "8192", "train_train_wall": "573", "train_gb_free": "2.2", "train_wall": "36846"}
[2025-07-02 06:17:43,904][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 06:17:43,904][fairseq.trainer][INFO] - begin training epoch 60
[2025-07-02 06:17:43,904][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 06:22:34,800][train_inner][INFO] - {"epoch": 60, "update": 59.504, "loss": "5.377", "nll_loss": "0.015", "loss_recon": "0.105", "loss_info_nce": "4.329", "ppl": "1.01", "wps": "1239.7", "ups": "0.42", "wpb": "2956.5", "bsz": "385.3", "num_updates": "14400", "lr": "7.73913e-05", "gnorm": "1.37", "clip": "0", "loss_scale": "16384", "train_wall": "471", "gb_free": "2.2", "wall": "37137"}
[2025-07-02 06:27:18,016][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-02 06:27:48,242][valid][INFO] - {"epoch": 60, "valid_loss": "5.068", "valid_nll_loss": "0.014", "valid_loss_recon": "0.1", "valid_loss_info_nce": "4.068", "valid_ppl": "1.01", "valid_wps": "2733.8", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "14520", "valid_best_loss": "5.068"}
[2025-07-02 06:27:48,242][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 60 @ 14520 updates
[2025-07-02 06:27:48,242][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint60.pt
[2025-07-02 06:27:49,972][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint60.pt
[2025-07-02 06:27:51,494][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint60.pt (epoch 60 @ 14520 updates, score 5.068) (writing took 3.26079000000027 seconds)
[2025-07-02 06:27:51,494][fairseq_cli.train][INFO] - end of epoch 60 (average epoch stats below)
[2025-07-02 06:27:51,520][train][INFO] - {"epoch": 60, "train_loss": "5.367", "train_nll_loss": "0.014", "train_loss_recon": "0.104", "train_loss_info_nce": "4.322", "train_ppl": "1.01", "train_wps": "1177.3", "train_ups": "0.4", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "14520", "train_lr": "7.71304e-05", "train_gnorm": "1.337", "train_clip": "0", "train_loss_scale": "16384", "train_train_wall": "571", "train_gb_free": "2.2", "train_wall": "37453"}
[2025-07-02 06:27:51,734][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 06:27:51,734][fairseq.trainer][INFO] - begin training epoch 61
[2025-07-02 06:27:51,734][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 06:31:04,152][train_inner][INFO] - {"epoch": 61, "update": 60.331, "loss": "5.356", "nll_loss": "0.014", "loss_recon": "0.104", "loss_info_nce": "4.315", "ppl": "1.01", "wps": "1160.2", "ups": "0.39", "wpb": "2954.6", "bsz": "385.9", "num_updates": "14600", "lr": "7.69565e-05", "gnorm": "1.5", "clip": "0", "loss_scale": "16384", "train_wall": "473", "gb_free": "2.2", "wall": "37646"}
[2025-07-02 06:38:00,681][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 61 @ 14762 updates
[2025-07-02 06:38:00,681][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint61.pt
[2025-07-02 06:38:02,445][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint61.pt
[2025-07-02 06:38:03,331][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint61.pt (epoch 61 @ 14762 updates, score None) (writing took 2.6394295999998576 seconds)
[2025-07-02 06:38:03,331][fairseq_cli.train][INFO] - end of epoch 61 (average epoch stats below)
[2025-07-02 06:38:03,347][train][INFO] - {"epoch": 61, "train_loss": "5.354", "train_nll_loss": "0.014", "train_loss_recon": "0.104", "train_loss_info_nce": "4.311", "train_ppl": "1.01", "train_wps": "1169.6", "train_ups": "0.4", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "14762", "train_lr": "7.66043e-05", "train_gnorm": "1.401", "train_clip": "0", "train_loss_scale": "16384", "train_train_wall": "606", "train_gb_free": "2.2", "train_wall": "38065"}
[2025-07-02 06:38:03,558][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 06:38:03,564][fairseq.trainer][INFO] - begin training epoch 62
[2025-07-02 06:38:03,564][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 06:39:47,490][train_inner][INFO] - {"epoch": 62, "update": 61.157, "loss": "5.353", "nll_loss": "0.014", "loss_recon": "0.104", "loss_info_nce": "4.31", "ppl": "1.01", "wps": "1129.9", "ups": "0.38", "wpb": "2956.5", "bsz": "385.4", "num_updates": "14800", "lr": "7.65217e-05", "gnorm": "1.256", "clip": "0", "loss_scale": "16384", "train_wall": "517", "gb_free": "2.2", "wall": "38169"}
[2025-07-02 06:48:47,079][train_inner][INFO] - {"epoch": 62, "update": 61.983, "loss": "5.344", "nll_loss": "0.014", "loss_recon": "0.104", "loss_info_nce": "4.302", "ppl": "1.01", "wps": "1098.6", "ups": "0.37", "wpb": "2963.9", "bsz": "386.8", "num_updates": "15000", "lr": "7.6087e-05", "gnorm": "1.307", "clip": "0", "loss_scale": "16384", "train_wall": "539", "gb_free": "2.2", "wall": "38709"}
[2025-07-02 06:48:56,229][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 62 @ 15004 updates
[2025-07-02 06:48:56,229][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint62.pt
[2025-07-02 06:48:58,088][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint62.pt
[2025-07-02 06:48:58,691][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint62.pt (epoch 62 @ 15004 updates, score None) (writing took 2.4736195000004955 seconds)
[2025-07-02 06:48:58,691][fairseq_cli.train][INFO] - end of epoch 62 (average epoch stats below)
[2025-07-02 06:48:58,707][train][INFO] - {"epoch": 62, "train_loss": "5.344", "train_nll_loss": "0.014", "train_loss_recon": "0.104", "train_loss_info_nce": "4.303", "train_ppl": "1.01", "train_wps": "1091.8", "train_ups": "0.37", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "15004", "train_lr": "7.60783e-05", "train_gnorm": "1.297", "train_clip": "0", "train_loss_scale": "16384", "train_train_wall": "650", "train_gb_free": "2.2", "train_wall": "38721"}
[2025-07-02 06:48:58,918][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 06:48:58,918][fairseq.trainer][INFO] - begin training epoch 63
[2025-07-02 06:48:58,918][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 06:58:02,755][train_inner][INFO] - {"epoch": 63, "update": 62.81, "loss": "5.333", "nll_loss": "0.014", "loss_recon": "0.104", "loss_info_nce": "4.293", "ppl": "1.01", "wps": "1063.5", "ups": "0.36", "wpb": "2954.6", "bsz": "385.6", "num_updates": "15200", "lr": "7.56522e-05", "gnorm": "1.307", "clip": "0", "loss_scale": "16384", "train_wall": "550", "gb_free": "2.2", "wall": "39265"}
[2025-07-02 07:00:34,112][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 63 @ 15246 updates
[2025-07-02 07:00:34,113][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint63.pt
[2025-07-02 07:00:35,971][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint63.pt
[2025-07-02 07:00:36,943][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint63.pt (epoch 63 @ 15246 updates, score None) (writing took 2.831329900000128 seconds)
[2025-07-02 07:00:36,945][fairseq_cli.train][INFO] - end of epoch 63 (average epoch stats below)
[2025-07-02 07:00:36,965][train][INFO] - {"epoch": 63, "train_loss": "5.332", "train_nll_loss": "0.014", "train_loss_recon": "0.104", "train_loss_info_nce": "4.293", "train_ppl": "1.01", "train_wps": "1024.8", "train_ups": "0.35", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "15246", "train_lr": "7.55522e-05", "train_gnorm": "1.27", "train_clip": "0", "train_loss_scale": "16384", "train_train_wall": "692", "train_gb_free": "2.2", "train_wall": "39419"}
[2025-07-02 07:00:37,230][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 07:00:37,234][fairseq.trainer][INFO] - begin training epoch 64
[2025-07-02 07:00:37,234][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 07:07:06,852][train_inner][INFO] - {"epoch": 64, "update": 63.636, "loss": "5.326", "nll_loss": "0.014", "loss_recon": "0.104", "loss_info_nce": "4.286", "ppl": "1.01", "wps": "1086.4", "ups": "0.37", "wpb": "2955.5", "bsz": "385.6", "num_updates": "15400", "lr": "7.52174e-05", "gnorm": "1.266", "clip": "0", "loss_scale": "16384", "train_wall": "538", "gb_free": "2.2", "wall": "39809"}
[2025-07-02 07:11:17,183][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 64 @ 15488 updates
[2025-07-02 07:11:17,183][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint64.pt
[2025-07-02 07:11:19,091][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint64.pt
[2025-07-02 07:11:20,118][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint64.pt (epoch 64 @ 15488 updates, score None) (writing took 2.9356641999984276 seconds)
[2025-07-02 07:11:20,118][fairseq_cli.train][INFO] - end of epoch 64 (average epoch stats below)
[2025-07-02 07:11:20,118][train][INFO] - {"epoch": 64, "train_loss": "5.32", "train_nll_loss": "0.014", "train_loss_recon": "0.104", "train_loss_info_nce": "4.283", "train_ppl": "1.01", "train_wps": "1112.5", "train_ups": "0.38", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "15488", "train_lr": "7.50261e-05", "train_gnorm": "1.261", "train_clip": "0", "train_loss_scale": "16384", "train_train_wall": "636", "train_gb_free": "2.2", "train_wall": "40062"}
[2025-07-02 07:11:20,518][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 07:11:20,534][fairseq.trainer][INFO] - begin training epoch 65
[2025-07-02 07:11:20,534][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 07:16:53,775][train_inner][INFO] - {"epoch": 65, "update": 64.463, "loss": "5.313", "nll_loss": "0.014", "loss_recon": "0.103", "loss_info_nce": "4.279", "ppl": "1.01", "wps": "1008.4", "ups": "0.34", "wpb": "2959.3", "bsz": "384.4", "num_updates": "15600", "lr": "7.47826e-05", "gnorm": "1.224", "clip": "0", "loss_scale": "16384", "train_wall": "578", "gb_free": "2.2", "wall": "40396"}
[2025-07-02 07:23:01,992][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-02 07:23:58,651][valid][INFO] - {"epoch": 65, "valid_loss": "5.001", "valid_nll_loss": "0.014", "valid_loss_recon": "0.097", "valid_loss_info_nce": "4.031", "valid_ppl": "1.01", "valid_wps": "1454.8", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "15730", "valid_best_loss": "5.001"}
[2025-07-02 07:23:58,653][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 65 @ 15730 updates
[2025-07-02 07:23:58,654][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint65.pt
[2025-07-02 07:24:00,695][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint65.pt
[2025-07-02 07:24:02,934][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint65.pt (epoch 65 @ 15730 updates, score 5.001) (writing took 4.2804408999945736 seconds)
[2025-07-02 07:24:02,934][fairseq_cli.train][INFO] - end of epoch 65 (average epoch stats below)
[2025-07-02 07:24:02,959][train][INFO] - {"epoch": 65, "train_loss": "5.311", "train_nll_loss": "0.014", "train_loss_recon": "0.104", "train_loss_info_nce": "4.276", "train_ppl": "1.01", "train_wps": "938", "train_ups": "0.32", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "15730", "train_lr": "7.45e-05", "train_gnorm": "1.306", "train_clip": "0", "train_loss_scale": "16384", "train_train_wall": "696", "train_gb_free": "2.2", "train_wall": "40825"}
[2025-07-02 07:24:03,359][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 07:24:03,365][fairseq.trainer][INFO] - begin training epoch 66
[2025-07-02 07:24:03,366][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 07:27:30,255][train_inner][INFO] - {"epoch": 66, "update": 65.289, "loss": "5.309", "nll_loss": "0.014", "loss_recon": "0.104", "loss_info_nce": "4.274", "ppl": "1.01", "wps": "927.9", "ups": "0.31", "wpb": "2952.8", "bsz": "386.5", "num_updates": "15800", "lr": "7.43478e-05", "gnorm": "1.33", "clip": "0", "loss_scale": "16384", "train_wall": "570", "gb_free": "2.2", "wall": "41032"}
[2025-07-02 07:35:47,034][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 66 @ 15972 updates
[2025-07-02 07:35:47,034][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint66.pt
[2025-07-02 07:35:49,040][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint66.pt
[2025-07-02 07:35:50,181][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint66.pt (epoch 66 @ 15972 updates, score None) (writing took 3.149440200002573 seconds)
[2025-07-02 07:35:50,181][fairseq_cli.train][INFO] - end of epoch 66 (average epoch stats below)
[2025-07-02 07:35:50,181][train][INFO] - {"epoch": 66, "train_loss": "5.307", "train_nll_loss": "0.014", "train_loss_recon": "0.104", "train_loss_info_nce": "4.271", "train_ppl": "1.01", "train_wps": "1011.8", "train_ups": "0.34", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "15972", "train_lr": "7.39739e-05", "train_gnorm": "1.331", "train_clip": "0", "train_loss_scale": "16384", "train_train_wall": "698", "train_gb_free": "2.2", "train_wall": "41532"}
[2025-07-02 07:35:50,559][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 07:35:50,570][fairseq.trainer][INFO] - begin training epoch 67
[2025-07-02 07:35:50,570][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 07:37:11,364][train_inner][INFO] - {"epoch": 67, "update": 66.116, "loss": "5.305", "nll_loss": "0.014", "loss_recon": "0.104", "loss_info_nce": "4.268", "ppl": "1.01", "wps": "1016.2", "ups": "0.34", "wpb": "2952.8", "bsz": "386.2", "num_updates": "16000", "lr": "7.3913e-05", "gnorm": "1.287", "clip": "0", "loss_scale": "16384", "train_wall": "572", "gb_free": "2.2", "wall": "41613"}
[2025-07-02 07:45:14,122][train_inner][INFO] - {"epoch": 67, "update": 66.942, "loss": "5.293", "nll_loss": "0.014", "loss_recon": "0.103", "loss_info_nce": "4.262", "ppl": "1.01", "wps": "1228.3", "ups": "0.41", "wpb": "2964.8", "bsz": "386.6", "num_updates": "16200", "lr": "7.34783e-05", "gnorm": "1.176", "clip": "0", "loss_scale": "16384", "train_wall": "481", "gb_free": "2.2", "wall": "42096"}
[2025-07-02 07:45:46,956][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 67 @ 16214 updates
[2025-07-02 07:45:46,956][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint67.pt
[2025-07-02 07:45:48,831][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint67.pt
[2025-07-02 07:45:49,457][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint67.pt (epoch 67 @ 16214 updates, score None) (writing took 2.5016926000025705 seconds)
[2025-07-02 07:45:49,457][fairseq_cli.train][INFO] - end of epoch 67 (average epoch stats below)
[2025-07-02 07:45:49,457][train][INFO] - {"epoch": 67, "train_loss": "5.295", "train_nll_loss": "0.014", "train_loss_recon": "0.103", "train_loss_info_nce": "4.262", "train_ppl": "1.01", "train_wps": "1194", "train_ups": "0.4", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "16214", "train_lr": "7.34478e-05", "train_gnorm": "1.144", "train_clip": "0", "train_loss_scale": "16384", "train_train_wall": "591", "train_gb_free": "2.2", "train_wall": "42131"}
[2025-07-02 07:45:49,657][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 07:45:49,657][fairseq.trainer][INFO] - begin training epoch 68
[2025-07-02 07:45:49,657][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 07:53:51,309][train_inner][INFO] - {"epoch": 68, "update": 67.769, "loss": "5.284", "nll_loss": "0.014", "loss_recon": "0.103", "loss_info_nce": "4.253", "ppl": "1.01", "wps": "1142.6", "ups": "0.39", "wpb": "2954.6", "bsz": "385.6", "num_updates": "16400", "lr": "7.30435e-05", "gnorm": "1.157", "clip": "0", "loss_scale": "32768", "train_wall": "512", "gb_free": "2.2", "wall": "42613"}
[2025-07-02 07:56:11,138][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 68 @ 16456 updates
[2025-07-02 07:56:11,138][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint68.pt
[2025-07-02 07:56:12,900][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint68.pt
[2025-07-02 07:56:13,568][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint68.pt (epoch 68 @ 16456 updates, score None) (writing took 2.428396799994516 seconds)
[2025-07-02 07:56:13,568][fairseq_cli.train][INFO] - end of epoch 68 (average epoch stats below)
[2025-07-02 07:56:13,568][train][INFO] - {"epoch": 68, "train_loss": "5.282", "train_nll_loss": "0.014", "train_loss_recon": "0.103", "train_loss_info_nce": "4.251", "train_ppl": "1.01", "train_wps": "1146.5", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "16456", "train_lr": "7.29217e-05", "train_gnorm": "1.175", "train_clip": "0", "train_loss_scale": "32768", "train_train_wall": "618", "train_gb_free": "2.2", "train_wall": "42755"}
[2025-07-02 07:56:13,814][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 07:56:13,814][fairseq.trainer][INFO] - begin training epoch 69
[2025-07-02 07:56:13,814][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 08:02:41,206][train_inner][INFO] - {"epoch": 69, "update": 68.595, "loss": "5.272", "nll_loss": "0.014", "loss_recon": "0.103", "loss_info_nce": "4.246", "ppl": "1.01", "wps": "1115.5", "ups": "0.38", "wpb": "2955.5", "bsz": "385.7", "num_updates": "16600", "lr": "7.26087e-05", "gnorm": "1.203", "clip": "0", "loss_scale": "32768", "train_wall": "524", "gb_free": "2.2", "wall": "43143"}
[2025-07-02 08:06:50,128][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 69 @ 16698 updates
[2025-07-02 08:06:50,128][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint69.pt
[2025-07-02 08:06:51,987][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint69.pt
[2025-07-02 08:06:52,630][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint69.pt (epoch 69 @ 16698 updates, score None) (writing took 2.502258799999254 seconds)
[2025-07-02 08:06:52,630][fairseq_cli.train][INFO] - end of epoch 69 (average epoch stats below)
[2025-07-02 08:06:52,646][train][INFO] - {"epoch": 69, "train_loss": "5.273", "train_nll_loss": "0.014", "train_loss_recon": "0.103", "train_loss_info_nce": "4.244", "train_ppl": "1.01", "train_wps": "1119.7", "train_ups": "0.38", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "16698", "train_lr": "7.23957e-05", "train_gnorm": "1.246", "train_clip": "0", "train_loss_scale": "32768", "train_train_wall": "633", "train_gb_free": "2.2", "train_wall": "43395"}
[2025-07-02 08:06:52,831][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 08:06:52,831][fairseq.trainer][INFO] - begin training epoch 70
[2025-07-02 08:06:52,831][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 08:11:02,583][train_inner][INFO] - {"epoch": 70, "update": 69.421, "loss": "5.27", "nll_loss": "0.014", "loss_recon": "0.103", "loss_info_nce": "4.242", "ppl": "1.01", "wps": "1179.4", "ups": "0.4", "wpb": "2956.5", "bsz": "385.2", "num_updates": "16800", "lr": "7.21739e-05", "gnorm": "1.29", "clip": "0", "loss_scale": "32768", "train_wall": "496", "gb_free": "2.2", "wall": "43644"}
[2025-07-02 08:17:04,559][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-02 08:17:37,126][valid][INFO] - {"epoch": 70, "valid_loss": "4.993", "valid_nll_loss": "0.014", "valid_loss_recon": "0.099", "valid_loss_info_nce": "4", "valid_ppl": "1.01", "valid_wps": "2531.1", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "16940", "valid_best_loss": "4.993"}
[2025-07-02 08:17:37,127][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 70 @ 16940 updates
[2025-07-02 08:17:37,128][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint70.pt
[2025-07-02 08:17:42,885][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint70.pt
[2025-07-02 08:17:44,444][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint70.pt (epoch 70 @ 16940 updates, score 4.993) (writing took 7.316675900001428 seconds)
[2025-07-02 08:17:44,446][fairseq_cli.train][INFO] - end of epoch 70 (average epoch stats below)
[2025-07-02 08:17:44,471][train][INFO] - {"epoch": 70, "train_loss": "5.265", "train_nll_loss": "0.014", "train_loss_recon": "0.103", "train_loss_info_nce": "4.237", "train_ppl": "1.01", "train_wps": "1097.8", "train_ups": "0.37", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "16940", "train_lr": "7.18696e-05", "train_gnorm": "1.155", "train_clip": "0", "train_loss_scale": "32768", "train_train_wall": "609", "train_gb_free": "2.2", "train_wall": "44046"}
[2025-07-02 08:17:44,715][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 08:17:44,718][fairseq.trainer][INFO] - begin training epoch 71
[2025-07-02 08:17:44,719][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 08:20:12,587][train_inner][INFO] - {"epoch": 71, "update": 70.248, "loss": "5.263", "nll_loss": "0.014", "loss_recon": "0.103", "loss_info_nce": "4.233", "ppl": "1.01", "wps": "1073.7", "ups": "0.36", "wpb": "2952.8", "bsz": "386.2", "num_updates": "17000", "lr": "7.17391e-05", "gnorm": "1.197", "clip": "0", "loss_scale": "32768", "train_wall": "507", "gb_free": "2.2", "wall": "44194"}
[2025-07-02 08:28:08,194][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 71 @ 17182 updates
[2025-07-02 08:28:08,194][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint71.pt
[2025-07-02 08:28:10,180][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint71.pt
[2025-07-02 08:28:11,290][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint71.pt (epoch 71 @ 17182 updates, score None) (writing took 3.091805199997907 seconds)
[2025-07-02 08:28:11,290][fairseq_cli.train][INFO] - end of epoch 71 (average epoch stats below)
[2025-07-02 08:28:11,306][train][INFO] - {"epoch": 71, "train_loss": "5.255", "train_nll_loss": "0.014", "train_loss_recon": "0.103", "train_loss_info_nce": "4.229", "train_ppl": "1.01", "train_wps": "1141.5", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "17182", "train_lr": "7.13435e-05", "train_gnorm": "1.187", "train_clip": "0", "train_loss_scale": "32768", "train_train_wall": "620", "train_gb_free": "2.2", "train_wall": "44673"}
[2025-07-02 08:28:11,690][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 08:28:11,690][fairseq.trainer][INFO] - begin training epoch 72
[2025-07-02 08:28:11,690][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 08:29:06,484][train_inner][INFO] - {"epoch": 72, "update": 71.074, "loss": "5.254", "nll_loss": "0.014", "loss_recon": "0.103", "loss_info_nce": "4.229", "ppl": "1.01", "wps": "1107.5", "ups": "0.37", "wpb": "2956.5", "bsz": "385.4", "num_updates": "17200", "lr": "7.13043e-05", "gnorm": "1.075", "clip": "0", "loss_scale": "32768", "train_wall": "526", "gb_free": "2.2", "wall": "44728"}
[2025-07-02 08:38:06,988][train_inner][INFO] - {"epoch": 72, "update": 71.901, "loss": "5.243", "nll_loss": "0.014", "loss_recon": "0.102", "loss_info_nce": "4.22", "ppl": "1.01", "wps": "1096.7", "ups": "0.37", "wpb": "2963.9", "bsz": "386.8", "num_updates": "17400", "lr": "7.08696e-05", "gnorm": "1.214", "clip": "0", "loss_scale": "32768", "train_wall": "538", "gb_free": "2.2", "wall": "45269"}
[2025-07-02 08:39:10,196][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 72 @ 17424 updates
[2025-07-02 08:39:10,196][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint72.pt
[2025-07-02 08:39:12,056][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint72.pt
[2025-07-02 08:39:12,712][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint72.pt (epoch 72 @ 17424 updates, score None) (writing took 2.5280855000019073 seconds)
[2025-07-02 08:39:12,712][fairseq_cli.train][INFO] - end of epoch 72 (average epoch stats below)
[2025-07-02 08:39:12,727][train][INFO] - {"epoch": 72, "train_loss": "5.244", "train_nll_loss": "0.014", "train_loss_recon": "0.102", "train_loss_info_nce": "4.22", "train_ppl": "1.01", "train_wps": "1081.8", "train_ups": "0.37", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "17424", "train_lr": "7.08174e-05", "train_gnorm": "1.187", "train_clip": "0", "train_loss_scale": "32768", "train_train_wall": "653", "train_gb_free": "2.2", "train_wall": "45335"}
[2025-07-02 08:39:12,926][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 08:39:12,926][fairseq.trainer][INFO] - begin training epoch 73
[2025-07-02 08:39:12,926][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 08:46:47,462][train_inner][INFO] - {"epoch": 73, "update": 72.727, "loss": "5.241", "nll_loss": "0.014", "loss_recon": "0.102", "loss_info_nce": "4.217", "ppl": "1.01", "wps": "1136.4", "ups": "0.38", "wpb": "2957.4", "bsz": "385.3", "num_updates": "17600", "lr": "7.04348e-05", "gnorm": "1.36", "clip": "0", "loss_scale": "32768", "train_wall": "514", "gb_free": "2.2", "wall": "45789"}
[2025-07-02 08:49:24,809][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 73 @ 17666 updates
[2025-07-02 08:49:24,809][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint73.pt
[2025-07-02 08:49:26,680][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint73.pt
[2025-07-02 08:49:27,328][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint73.pt (epoch 73 @ 17666 updates, score None) (writing took 2.5176839000050677 seconds)
[2025-07-02 08:49:27,328][fairseq_cli.train][INFO] - end of epoch 73 (average epoch stats below)
[2025-07-02 08:49:27,345][train][INFO] - {"epoch": 73, "train_loss": "5.238", "train_nll_loss": "0.014", "train_loss_recon": "0.102", "train_loss_info_nce": "4.215", "train_ppl": "1.01", "train_wps": "1164.2", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "17666", "train_lr": "7.02913e-05", "train_gnorm": "1.301", "train_clip": "0", "train_loss_scale": "32768", "train_train_wall": "608", "train_gb_free": "2.2", "train_wall": "45949"}
[2025-07-02 08:49:27,544][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 08:49:27,560][fairseq.trainer][INFO] - begin training epoch 74
[2025-07-02 08:49:27,560][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 08:54:51,730][train_inner][INFO] - {"epoch": 74, "update": 73.554, "loss": "5.235", "nll_loss": "0.014", "loss_recon": "0.102", "loss_info_nce": "4.21", "ppl": "1.01", "wps": "1220.6", "ups": "0.41", "wpb": "2955.5", "bsz": "385.2", "num_updates": "17800", "lr": "7e-05", "gnorm": "1.193", "clip": "0", "loss_scale": "32768", "train_wall": "479", "gb_free": "2.2", "wall": "46274"}
[2025-07-02 08:59:42,919][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 74 @ 17908 updates
[2025-07-02 08:59:42,919][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint74.pt
[2025-07-02 08:59:44,720][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint74.pt
[2025-07-02 08:59:45,533][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint74.pt (epoch 74 @ 17908 updates, score None) (writing took 2.616406300003291 seconds)
[2025-07-02 08:59:45,533][fairseq_cli.train][INFO] - end of epoch 74 (average epoch stats below)
[2025-07-02 08:59:45,549][train][INFO] - {"epoch": 74, "train_loss": "5.23", "train_nll_loss": "0.014", "train_loss_recon": "0.102", "train_loss_info_nce": "4.208", "train_ppl": "1.01", "train_wps": "1157.5", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "17908", "train_lr": "6.97652e-05", "train_gnorm": "1.255", "train_clip": "0", "train_loss_scale": "32768", "train_train_wall": "612", "train_gb_free": "2.2", "train_wall": "46567"}
[2025-07-02 08:59:45,807][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 08:59:45,817][fairseq.trainer][INFO] - begin training epoch 75
[2025-07-02 08:59:45,817][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 09:03:30,220][train_inner][INFO] - {"epoch": 75, "update": 74.38, "loss": "5.225", "nll_loss": "0.014", "loss_recon": "0.102", "loss_info_nce": "4.205", "ppl": "1.01", "wps": "1139.7", "ups": "0.39", "wpb": "2954.6", "bsz": "386.1", "num_updates": "18000", "lr": "6.95652e-05", "gnorm": "1.194", "clip": "0", "loss_scale": "32768", "train_wall": "512", "gb_free": "2.2", "wall": "46792"}
[2025-07-02 09:09:57,337][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-02 09:10:27,354][valid][INFO] - {"epoch": 75, "valid_loss": "4.948", "valid_nll_loss": "0.013", "valid_loss_recon": "0.097", "valid_loss_info_nce": "3.978", "valid_ppl": "1.01", "valid_wps": "2755.1", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "18150", "valid_best_loss": "4.948"}
[2025-07-02 09:10:27,354][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 75 @ 18150 updates
[2025-07-02 09:10:27,354][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint75.pt
[2025-07-02 09:10:29,210][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint75.pt
[2025-07-02 09:10:30,737][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint75.pt (epoch 75 @ 18150 updates, score 4.948) (writing took 3.382597399999213 seconds)
[2025-07-02 09:10:30,737][fairseq_cli.train][INFO] - end of epoch 75 (average epoch stats below)
[2025-07-02 09:10:30,754][train][INFO] - {"epoch": 75, "train_loss": "5.223", "train_nll_loss": "0.014", "train_loss_recon": "0.102", "train_loss_info_nce": "4.202", "train_ppl": "1.01", "train_wps": "1109.1", "train_ups": "0.38", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "18150", "train_lr": "6.92391e-05", "train_gnorm": "1.095", "train_clip": "0", "train_loss_scale": "32768", "train_train_wall": "607", "train_gb_free": "2.2", "train_wall": "47213"}
[2025-07-02 09:10:31,012][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 09:10:31,012][fairseq.trainer][INFO] - begin training epoch 76
[2025-07-02 09:10:31,012][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 09:12:35,310][train_inner][INFO] - {"epoch": 76, "update": 75.207, "loss": "5.22", "nll_loss": "0.014", "loss_recon": "0.102", "loss_info_nce": "4.199", "ppl": "1.01", "wps": "1083.4", "ups": "0.37", "wpb": "2952.8", "bsz": "385.7", "num_updates": "18200", "lr": "6.91304e-05", "gnorm": "1.059", "clip": "0", "loss_scale": "32768", "train_wall": "508", "gb_free": "2.2", "wall": "47337"}
[2025-07-02 09:20:40,909][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 76 @ 18392 updates
[2025-07-02 09:20:40,909][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint76.pt
[2025-07-02 09:20:43,020][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint76.pt
[2025-07-02 09:20:44,094][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint76.pt (epoch 76 @ 18392 updates, score None) (writing took 3.180408799998986 seconds)
[2025-07-02 09:20:44,094][fairseq_cli.train][INFO] - end of epoch 76 (average epoch stats below)
[2025-07-02 09:20:44,102][train][INFO] - {"epoch": 76, "train_loss": "5.217", "train_nll_loss": "0.014", "train_loss_recon": "0.102", "train_loss_info_nce": "4.197", "train_ppl": "1.01", "train_wps": "1166.6", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "18392", "train_lr": "6.8713e-05", "train_gnorm": "1.175", "train_clip": "0", "train_loss_scale": "32768", "train_train_wall": "606", "train_gb_free": "2.2", "train_wall": "47826"}
[2025-07-02 09:20:44,491][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 09:20:44,506][fairseq.trainer][INFO] - begin training epoch 77
[2025-07-02 09:20:44,506][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 09:21:10,173][train_inner][INFO] - {"epoch": 77, "update": 76.033, "loss": "5.218", "nll_loss": "0.014", "loss_recon": "0.102", "loss_info_nce": "4.198", "ppl": "1.01", "wps": "1148.8", "ups": "0.39", "wpb": "2957.4", "bsz": "385.3", "num_updates": "18400", "lr": "6.86957e-05", "gnorm": "1.202", "clip": "0", "loss_scale": "32768", "train_wall": "507", "gb_free": "2.2", "wall": "47852"}
[2025-07-02 09:29:45,768][train_inner][INFO] - {"epoch": 77, "update": 76.86, "loss": "5.21", "nll_loss": "0.014", "loss_recon": "0.102", "loss_info_nce": "4.19", "ppl": "1.01", "wps": "1149.7", "ups": "0.39", "wpb": "2963.9", "bsz": "386.8", "num_updates": "18600", "lr": "6.82609e-05", "gnorm": "1.267", "clip": "0", "loss_scale": "65536", "train_wall": "514", "gb_free": "2.2", "wall": "48368"}
[2025-07-02 09:31:08,042][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 77 @ 18634 updates
[2025-07-02 09:31:08,042][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint77.pt
[2025-07-02 09:31:09,812][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint77.pt
[2025-07-02 09:31:10,413][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint77.pt (epoch 77 @ 18634 updates, score None) (writing took 2.37709470000118 seconds)
[2025-07-02 09:31:10,413][fairseq_cli.train][INFO] - end of epoch 77 (average epoch stats below)
[2025-07-02 09:31:10,446][train][INFO] - {"epoch": 77, "train_loss": "5.209", "train_nll_loss": "0.014", "train_loss_recon": "0.102", "train_loss_info_nce": "4.19", "train_ppl": "1.01", "train_wps": "1142.5", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "18634", "train_lr": "6.8187e-05", "train_gnorm": "1.233", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "619", "train_gb_free": "2.2", "train_wall": "48452"}
[2025-07-02 09:31:10,644][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 09:31:10,660][fairseq.trainer][INFO] - begin training epoch 78
[2025-07-02 09:31:10,663][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 09:38:58,294][train_inner][INFO] - {"epoch": 78, "update": 77.686, "loss": "5.201", "nll_loss": "0.014", "loss_recon": "0.102", "loss_info_nce": "4.185", "ppl": "1.01", "wps": "1068.9", "ups": "0.36", "wpb": "2952.8", "bsz": "386.2", "num_updates": "18800", "lr": "6.78261e-05", "gnorm": "1.091", "clip": "0", "loss_scale": "65536", "train_wall": "546", "gb_free": "2.2", "wall": "48920"}
[2025-07-02 09:42:35,930][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 78 @ 18876 updates
[2025-07-02 09:42:35,930][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint78.pt
[2025-07-02 09:42:37,815][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint78.pt
[2025-07-02 09:42:38,448][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint78.pt (epoch 78 @ 18876 updates, score None) (writing took 2.523381300001347 seconds)
[2025-07-02 09:42:38,448][fairseq_cli.train][INFO] - end of epoch 78 (average epoch stats below)
[2025-07-02 09:42:38,464][train][INFO] - {"epoch": 78, "train_loss": "5.201", "train_nll_loss": "0.014", "train_loss_recon": "0.102", "train_loss_info_nce": "4.184", "train_ppl": "1.01", "train_wps": "1040", "train_ups": "0.35", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "18876", "train_lr": "6.76609e-05", "train_gnorm": "1.098", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "681", "train_gb_free": "2.2", "train_wall": "49140"}
[2025-07-02 09:42:38,657][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 09:42:38,657][fairseq.trainer][INFO] - begin training epoch 79
[2025-07-02 09:42:38,657][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 09:48:28,466][train_inner][INFO] - {"epoch": 79, "update": 78.512, "loss": "5.199", "nll_loss": "0.014", "loss_recon": "0.102", "loss_info_nce": "4.181", "ppl": "1.01", "wps": "1037.7", "ups": "0.35", "wpb": "2958.3", "bsz": "385", "num_updates": "19000", "lr": "6.73913e-05", "gnorm": "1.018", "clip": "0", "loss_scale": "65536", "train_wall": "564", "gb_free": "2.2", "wall": "49490"}
[2025-07-02 09:53:27,858][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 79 @ 19118 updates
[2025-07-02 09:53:27,859][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint79.pt
[2025-07-02 09:53:29,702][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint79.pt
[2025-07-02 09:53:30,298][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint79.pt (epoch 79 @ 19118 updates, score None) (writing took 2.440030600002501 seconds)
[2025-07-02 09:53:30,298][fairseq_cli.train][INFO] - end of epoch 79 (average epoch stats below)
[2025-07-02 09:53:30,306][train][INFO] - {"epoch": 79, "train_loss": "5.194", "train_nll_loss": "0.014", "train_loss_recon": "0.102", "train_loss_info_nce": "4.178", "train_ppl": "1.01", "train_wps": "1097.7", "train_ups": "0.37", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "19118", "train_lr": "6.71348e-05", "train_gnorm": "1.018", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "645", "train_gb_free": "2.2", "train_wall": "49792"}
[2025-07-02 09:53:30,497][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 09:53:30,501][fairseq.trainer][INFO] - begin training epoch 80
[2025-07-02 09:53:30,502][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 09:56:56,432][train_inner][INFO] - {"epoch": 80, "update": 79.339, "loss": "5.19", "nll_loss": "0.014", "loss_recon": "0.102", "loss_info_nce": "4.174", "ppl": "1.01", "wps": "1162.6", "ups": "0.39", "wpb": "2952.8", "bsz": "386.5", "num_updates": "19200", "lr": "6.69565e-05", "gnorm": "1.075", "clip": "0", "loss_scale": "65536", "train_wall": "502", "gb_free": "2.2", "wall": "49998"}
[2025-07-02 10:03:27,236][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-02 10:03:58,141][valid][INFO] - {"epoch": 80, "valid_loss": "4.91", "valid_nll_loss": "0.013", "valid_loss_recon": "0.095", "valid_loss_info_nce": "3.958", "valid_ppl": "1.01", "valid_wps": "2668.7", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "19360", "valid_best_loss": "4.91"}
[2025-07-02 10:03:58,156][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 80 @ 19360 updates
[2025-07-02 10:03:58,156][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint80.pt
[2025-07-02 10:03:59,940][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint80.pt
[2025-07-02 10:04:01,927][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint80.pt (epoch 80 @ 19360 updates, score 4.91) (writing took 3.7754671000002418 seconds)
[2025-07-02 10:04:01,927][fairseq_cli.train][INFO] - end of epoch 80 (average epoch stats below)
[2025-07-02 10:04:01,959][train][INFO] - {"epoch": 80, "train_loss": "5.185", "train_nll_loss": "0.014", "train_loss_recon": "0.101", "train_loss_info_nce": "4.172", "train_ppl": "1.01", "train_wps": "1132.9", "train_ups": "0.38", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "19360", "train_lr": "6.66087e-05", "train_gnorm": "1.102", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "593", "train_gb_free": "2.2", "train_wall": "50424"}
[2025-07-02 10:04:02,168][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 10:04:02,168][fairseq.trainer][INFO] - begin training epoch 81
[2025-07-02 10:04:02,168][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 10:05:43,097][train_inner][INFO] - {"epoch": 81, "update": 80.165, "loss": "5.181", "nll_loss": "0.014", "loss_recon": "0.101", "loss_info_nce": "4.171", "ppl": "1.01", "wps": "1123.5", "ups": "0.38", "wpb": "2958.3", "bsz": "384.5", "num_updates": "19400", "lr": "6.65217e-05", "gnorm": "1.105", "clip": "0", "loss_scale": "65536", "train_wall": "489", "gb_free": "2.2", "wall": "50525"}
[2025-07-02 10:14:05,986][train_inner][INFO] - {"epoch": 81, "update": 80.992, "loss": "5.183", "nll_loss": "0.014", "loss_recon": "0.101", "loss_info_nce": "4.169", "ppl": "1.01", "wps": "1178.4", "ups": "0.4", "wpb": "2963", "bsz": "387.1", "num_updates": "19600", "lr": "6.6087e-05", "gnorm": "1.121", "clip": "0", "loss_scale": "65536", "train_wall": "501", "gb_free": "2.2", "wall": "51028"}
[2025-07-02 10:14:09,281][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 81 @ 19602 updates
[2025-07-02 10:14:09,281][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint81.pt
[2025-07-02 10:14:11,262][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint81.pt
[2025-07-02 10:14:12,363][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint81.pt (epoch 81 @ 19602 updates, score None) (writing took 3.0842106999989483 seconds)
[2025-07-02 10:14:12,363][fairseq_cli.train][INFO] - end of epoch 81 (average epoch stats below)
[2025-07-02 10:14:12,379][train][INFO] - {"epoch": 81, "train_loss": "5.182", "train_nll_loss": "0.014", "train_loss_recon": "0.101", "train_loss_info_nce": "4.169", "train_ppl": "1.01", "train_wps": "1172.2", "train_ups": "0.4", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "19602", "train_lr": "6.60826e-05", "train_gnorm": "1.108", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "604", "train_gb_free": "2.2", "train_wall": "51034"}
[2025-07-02 10:14:12,779][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 10:14:12,779][fairseq.trainer][INFO] - begin training epoch 82
[2025-07-02 10:14:12,794][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 10:22:25,625][train_inner][INFO] - {"epoch": 82, "update": 81.818, "loss": "5.177", "nll_loss": "0.014", "loss_recon": "0.101", "loss_info_nce": "4.163", "ppl": "1.01", "wps": "1182.3", "ups": "0.4", "wpb": "2953.7", "bsz": "385.9", "num_updates": "19800", "lr": "6.56522e-05", "gnorm": "1.086", "clip": "0", "loss_scale": "65536", "train_wall": "491", "gb_free": "2.2", "wall": "51528"}
[2025-07-02 10:24:09,931][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 82 @ 19844 updates
[2025-07-02 10:24:09,947][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint82.pt
[2025-07-02 10:24:11,790][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint82.pt
[2025-07-02 10:24:12,391][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint82.pt (epoch 82 @ 19844 updates, score None) (writing took 2.458409399994707 seconds)
[2025-07-02 10:24:12,391][fairseq_cli.train][INFO] - end of epoch 82 (average epoch stats below)
[2025-07-02 10:24:12,406][train][INFO] - {"epoch": 82, "train_loss": "5.175", "train_nll_loss": "0.014", "train_loss_recon": "0.101", "train_loss_info_nce": "4.163", "train_ppl": "1.01", "train_wps": "1192.5", "train_ups": "0.4", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "19844", "train_lr": "6.55565e-05", "train_gnorm": "1.065", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "592", "train_gb_free": "2.2", "train_wall": "51634"}
[2025-07-02 10:24:12,618][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 10:24:12,618][fairseq.trainer][INFO] - begin training epoch 83
[2025-07-02 10:24:12,618][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 10:27:19,819][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32768.0
[2025-07-02 10:30:39,187][train_inner][INFO] - {"epoch": 83, "update": 82.649, "loss": "5.17", "nll_loss": "0.014", "loss_recon": "0.101", "loss_info_nce": "4.161", "ppl": "1.01", "wps": "1198.4", "ups": "0.41", "wpb": "2957.4", "bsz": "385.3", "num_updates": "20000", "lr": "6.52174e-05", "gnorm": "1.153", "clip": "0", "loss_scale": "32768", "train_wall": "488", "gb_free": "2.2", "wall": "52021"}
[2025-07-02 10:30:39,187][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-02 10:31:09,569][valid][INFO] - {"epoch": 83, "valid_loss": "4.877", "valid_nll_loss": "0.013", "valid_loss_recon": "0.093", "valid_loss_info_nce": "3.947", "valid_ppl": "1.01", "valid_wps": "2720.9", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "20000", "valid_best_loss": "4.877"}
[2025-07-02 10:34:41,810][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 83 @ 20085 updates
[2025-07-02 10:34:41,811][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint83.pt
[2025-07-02 10:34:43,520][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint83.pt
[2025-07-02 10:34:44,219][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint83.pt (epoch 83 @ 20085 updates, score None) (writing took 2.420401999996102 seconds)
[2025-07-02 10:34:44,219][fairseq_cli.train][INFO] - end of epoch 83 (average epoch stats below)
[2025-07-02 10:34:44,234][train][INFO] - {"epoch": 83, "train_loss": "5.171", "train_nll_loss": "0.014", "train_loss_recon": "0.101", "train_loss_info_nce": "4.16", "train_ppl": "1.01", "train_wps": "1127.8", "train_ups": "0.38", "train_wpb": "2956.7", "train_bsz": "385.8", "train_num_updates": "20085", "train_lr": "6.50326e-05", "train_gnorm": "1.235", "train_clip": "0", "train_loss_scale": "32768", "train_train_wall": "596", "train_gb_free": "2.2", "train_wall": "52266"}
[2025-07-02 10:34:44,434][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 10:34:44,434][fairseq.trainer][INFO] - begin training epoch 84
[2025-07-02 10:34:44,434][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 10:39:31,103][train_inner][INFO] - {"epoch": 84, "update": 83.475, "loss": "5.166", "nll_loss": "0.014", "loss_recon": "0.101", "loss_info_nce": "4.156", "ppl": "1.01", "wps": "1110.9", "ups": "0.38", "wpb": "2954.6", "bsz": "386", "num_updates": "20200", "lr": "6.47826e-05", "gnorm": "1.218", "clip": "0", "loss_scale": "32768", "train_wall": "496", "gb_free": "2.2", "wall": "52553"}
[2025-07-02 10:44:34,527][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 84 @ 20327 updates
[2025-07-02 10:44:34,527][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint84.pt
[2025-07-02 10:44:36,329][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint84.pt
[2025-07-02 10:44:37,014][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint84.pt (epoch 84 @ 20327 updates, score None) (writing took 2.4838688999952865 seconds)
[2025-07-02 10:44:37,014][fairseq_cli.train][INFO] - end of epoch 84 (average epoch stats below)
[2025-07-02 10:44:37,029][train][INFO] - {"epoch": 84, "train_loss": "5.163", "train_nll_loss": "0.014", "train_loss_recon": "0.101", "train_loss_info_nce": "4.153", "train_ppl": "1.01", "train_wps": "1207.1", "train_ups": "0.41", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "20327", "train_lr": "6.45065e-05", "train_gnorm": "1.088", "train_clip": "0", "train_loss_scale": "32768", "train_train_wall": "587", "train_gb_free": "2.2", "train_wall": "52859"}
[2025-07-02 10:44:37,256][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 10:44:37,256][fairseq.trainer][INFO] - begin training epoch 85
[2025-07-02 10:44:37,272][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 10:47:53,260][train_inner][INFO] - {"epoch": 85, "update": 84.302, "loss": "5.16", "nll_loss": "0.014", "loss_recon": "0.101", "loss_info_nce": "4.153", "ppl": "1.01", "wps": "1177.2", "ups": "0.4", "wpb": "2955.5", "bsz": "385.4", "num_updates": "20400", "lr": "6.43478e-05", "gnorm": "1.133", "clip": "0", "loss_scale": "32768", "train_wall": "496", "gb_free": "2.2", "wall": "53055"}
[2025-07-02 10:55:17,032][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-02 10:56:10,230][valid][INFO] - {"epoch": 85, "valid_loss": "4.887", "valid_nll_loss": "0.013", "valid_loss_recon": "0.094", "valid_loss_info_nce": "3.947", "valid_ppl": "1.01", "valid_wps": "1546.3", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "20569", "valid_best_loss": "4.887"}
[2025-07-02 10:56:10,230][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 85 @ 20569 updates
[2025-07-02 10:56:10,230][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint85.pt
[2025-07-02 10:56:12,196][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint85.pt
[2025-07-02 10:56:14,324][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint85.pt (epoch 85 @ 20569 updates, score 4.887) (writing took 4.093534700004966 seconds)
[2025-07-02 10:56:14,324][fairseq_cli.train][INFO] - end of epoch 85 (average epoch stats below)
[2025-07-02 10:56:14,365][train][INFO] - {"epoch": 85, "train_loss": "5.158", "train_nll_loss": "0.014", "train_loss_recon": "0.101", "train_loss_info_nce": "4.149", "train_ppl": "1.01", "train_wps": "1026.2", "train_ups": "0.35", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "20569", "train_lr": "6.39804e-05", "train_gnorm": "1.103", "train_clip": "0", "train_loss_scale": "32768", "train_train_wall": "636", "train_gb_free": "2.2", "train_wall": "53556"}
[2025-07-02 10:56:14,755][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 10:56:14,765][fairseq.trainer][INFO] - begin training epoch 86
[2025-07-02 10:56:14,765][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 10:57:38,367][train_inner][INFO] - {"epoch": 86, "update": 85.128, "loss": "5.159", "nll_loss": "0.014", "loss_recon": "0.101", "loss_info_nce": "4.148", "ppl": "1.01", "wps": "1009.6", "ups": "0.34", "wpb": "2953.7", "bsz": "385.9", "num_updates": "20600", "lr": "6.3913e-05", "gnorm": "1.034", "clip": "0", "loss_scale": "32768", "train_wall": "523", "gb_free": "2.2", "wall": "53640"}
[2025-07-02 11:05:48,731][train_inner][INFO] - {"epoch": 86, "update": 85.955, "loss": "5.152", "nll_loss": "0.014", "loss_recon": "0.101", "loss_info_nce": "4.145", "ppl": "1.01", "wps": "1209.6", "ups": "0.41", "wpb": "2965.8", "bsz": "386.3", "num_updates": "20800", "lr": "6.34783e-05", "gnorm": "1.142", "clip": "0", "loss_scale": "32768", "train_wall": "489", "gb_free": "2.2", "wall": "54131"}
[2025-07-02 11:06:13,479][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 86 @ 20811 updates
[2025-07-02 11:06:13,479][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint86.pt
[2025-07-02 11:06:15,330][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint86.pt
[2025-07-02 11:06:15,968][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint86.pt (epoch 86 @ 20811 updates, score None) (writing took 2.4910004999983357 seconds)
[2025-07-02 11:06:15,968][fairseq_cli.train][INFO] - end of epoch 86 (average epoch stats below)
[2025-07-02 11:06:15,977][train][INFO] - {"epoch": 86, "train_loss": "5.152", "train_nll_loss": "0.014", "train_loss_recon": "0.101", "train_loss_info_nce": "4.145", "train_ppl": "1.01", "train_wps": "1189.4", "train_ups": "0.4", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "20811", "train_lr": "6.34543e-05", "train_gnorm": "1.172", "train_clip": "0", "train_loss_scale": "32768", "train_train_wall": "595", "train_gb_free": "2.3", "train_wall": "54158"}
[2025-07-02 11:06:16,170][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 11:06:16,170][fairseq.trainer][INFO] - begin training epoch 87
[2025-07-02 11:06:16,170][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 11:13:56,207][train_inner][INFO] - {"epoch": 87, "update": 86.781, "loss": "5.152", "nll_loss": "0.014", "loss_recon": "0.101", "loss_info_nce": "4.143", "ppl": "1.01", "wps": "1212.2", "ups": "0.41", "wpb": "2954.6", "bsz": "385.6", "num_updates": "21000", "lr": "6.30435e-05", "gnorm": "1.1", "clip": "0", "loss_scale": "32768", "train_wall": "482", "gb_free": "2.2", "wall": "54618"}
[2025-07-02 11:16:12,771][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 87 @ 21053 updates
[2025-07-02 11:16:12,771][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint87.pt
[2025-07-02 11:16:14,714][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint87.pt
[2025-07-02 11:16:15,715][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint87.pt (epoch 87 @ 21053 updates, score None) (writing took 2.9431886000020313 seconds)
[2025-07-02 11:16:15,715][fairseq_cli.train][INFO] - end of epoch 87 (average epoch stats below)
[2025-07-02 11:16:15,715][train][INFO] - {"epoch": 87, "train_loss": "5.15", "train_nll_loss": "0.014", "train_loss_recon": "0.101", "train_loss_info_nce": "4.143", "train_ppl": "1.01", "train_wps": "1193.1", "train_ups": "0.4", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "21053", "train_lr": "6.29283e-05", "train_gnorm": "1.094", "train_clip": "0", "train_loss_scale": "32768", "train_train_wall": "593", "train_gb_free": "2.2", "train_wall": "54758"}
[2025-07-02 11:16:16,116][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 11:16:16,131][fairseq.trainer][INFO] - begin training epoch 88
[2025-07-02 11:16:16,131][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 11:22:55,894][train_inner][INFO] - {"epoch": 88, "update": 87.607, "loss": "5.145", "nll_loss": "0.014", "loss_recon": "0.1", "loss_info_nce": "4.14", "ppl": "1.01", "wps": "1096", "ups": "0.37", "wpb": "2957.4", "bsz": "385.6", "num_updates": "21200", "lr": "6.26087e-05", "gnorm": "1.098", "clip": "0", "loss_scale": "32768", "train_wall": "531", "gb_free": "2.2", "wall": "55158"}
[2025-07-02 11:27:06,623][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 88 @ 21295 updates
[2025-07-02 11:27:06,623][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint88.pt
[2025-07-02 11:27:08,456][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint88.pt
[2025-07-02 11:27:09,099][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint88.pt (epoch 88 @ 21295 updates, score None) (writing took 2.4650643999993918 seconds)
[2025-07-02 11:27:09,099][fairseq_cli.train][INFO] - end of epoch 88 (average epoch stats below)
[2025-07-02 11:27:09,109][train][INFO] - {"epoch": 88, "train_loss": "5.142", "train_nll_loss": "0.014", "train_loss_recon": "0.101", "train_loss_info_nce": "4.137", "train_ppl": "1.01", "train_wps": "1095.2", "train_ups": "0.37", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "21295", "train_lr": "6.24022e-05", "train_gnorm": "1.022", "train_clip": "0", "train_loss_scale": "32768", "train_train_wall": "644", "train_gb_free": "2.2", "train_wall": "55411"}
[2025-07-02 11:27:09,309][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 11:27:09,325][fairseq.trainer][INFO] - begin training epoch 89
[2025-07-02 11:27:09,325][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 11:31:31,324][train_inner][INFO] - {"epoch": 89, "update": 88.434, "loss": "5.138", "nll_loss": "0.014", "loss_recon": "0.1", "loss_info_nce": "4.134", "ppl": "1.01", "wps": "1145.8", "ups": "0.39", "wpb": "2952.8", "bsz": "385.9", "num_updates": "21400", "lr": "6.21739e-05", "gnorm": "1.063", "clip": "0", "loss_scale": "32768", "train_wall": "509", "gb_free": "2.2", "wall": "55673"}
[2025-07-02 11:37:09,342][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 89 @ 21537 updates
[2025-07-02 11:37:09,342][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint89.pt
[2025-07-02 11:37:11,145][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint89.pt
[2025-07-02 11:37:11,771][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint89.pt (epoch 89 @ 21537 updates, score None) (writing took 2.416053400003875 seconds)
[2025-07-02 11:37:11,772][fairseq_cli.train][INFO] - end of epoch 89 (average epoch stats below)
[2025-07-02 11:37:11,780][train][INFO] - {"epoch": 89, "train_loss": "5.139", "train_nll_loss": "0.014", "train_loss_recon": "0.1", "train_loss_info_nce": "4.134", "train_ppl": "1.01", "train_wps": "1187.3", "train_ups": "0.4", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "21537", "train_lr": "6.18761e-05", "train_gnorm": "1.062", "train_clip": "0", "train_loss_scale": "32768", "train_train_wall": "597", "train_gb_free": "2.2", "train_wall": "56014"}
[2025-07-02 11:37:12,000][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 11:37:12,003][fairseq.trainer][INFO] - begin training epoch 90
[2025-07-02 11:37:12,004][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 11:39:54,640][train_inner][INFO] - {"epoch": 90, "update": 89.26, "loss": "5.138", "nll_loss": "0.014", "loss_recon": "0.1", "loss_info_nce": "4.133", "ppl": "1.01", "wps": "1174.4", "ups": "0.4", "wpb": "2955.5", "bsz": "385.6", "num_updates": "21600", "lr": "6.17391e-05", "gnorm": "0.987", "clip": "0", "loss_scale": "32768", "train_wall": "497", "gb_free": "2.2", "wall": "56177"}
[2025-07-02 11:48:11,776][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-02 11:49:05,135][valid][INFO] - {"epoch": 90, "valid_loss": "4.857", "valid_nll_loss": "0.013", "valid_loss_recon": "0.093", "valid_loss_info_nce": "3.926", "valid_ppl": "1.01", "valid_wps": "1555.3", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "21779", "valid_best_loss": "4.857"}
[2025-07-02 11:49:05,137][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 90 @ 21779 updates
[2025-07-02 11:49:05,138][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint90.pt
[2025-07-02 11:49:07,118][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint90.pt
[2025-07-02 11:49:09,463][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint90.pt (epoch 90 @ 21779 updates, score 4.857) (writing took 4.325675100000808 seconds)
[2025-07-02 11:49:09,463][fairseq_cli.train][INFO] - end of epoch 90 (average epoch stats below)
[2025-07-02 11:49:09,485][train][INFO] - {"epoch": 90, "train_loss": "5.134", "train_nll_loss": "0.014", "train_loss_recon": "0.1", "train_loss_info_nce": "4.13", "train_ppl": "1.01", "train_wps": "997", "train_ups": "0.34", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "21779", "train_lr": "6.135e-05", "train_gnorm": "1.019", "train_clip": "0", "train_loss_scale": "32768", "train_train_wall": "655", "train_gb_free": "2.2", "train_wall": "56731"}
[2025-07-02 11:49:09,889][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 11:49:09,895][fairseq.trainer][INFO] - begin training epoch 91
[2025-07-02 11:49:09,895][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 11:50:10,958][train_inner][INFO] - {"epoch": 91, "update": 90.087, "loss": "5.133", "nll_loss": "0.014", "loss_recon": "0.1", "loss_info_nce": "4.129", "ppl": "1.01", "wps": "959.4", "ups": "0.32", "wpb": "2956.5", "bsz": "385.1", "num_updates": "21800", "lr": "6.13043e-05", "gnorm": "1.006", "clip": "0", "loss_scale": "32768", "train_wall": "553", "gb_free": "2.2", "wall": "56793"}
[2025-07-02 11:59:42,646][train_inner][INFO] - {"epoch": 91, "update": 90.913, "loss": "5.127", "nll_loss": "0.014", "loss_recon": "0.1", "loss_info_nce": "4.124", "ppl": "1.01", "wps": "1036.6", "ups": "0.35", "wpb": "2963", "bsz": "387.4", "num_updates": "22000", "lr": "6.08696e-05", "gnorm": "1.035", "clip": "0", "loss_scale": "65536", "train_wall": "568", "gb_free": "2.2", "wall": "57365"}
[2025-07-02 12:00:38,832][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 91 @ 22021 updates
[2025-07-02 12:00:38,832][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint91.pt
[2025-07-02 12:00:40,741][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint91.pt
[2025-07-02 12:00:42,118][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint91.pt (epoch 91 @ 22021 updates, score None) (writing took 3.286436299997149 seconds)
[2025-07-02 12:00:42,118][fairseq_cli.train][INFO] - end of epoch 91 (average epoch stats below)
[2025-07-02 12:00:42,141][train][INFO] - {"epoch": 91, "train_loss": "5.128", "train_nll_loss": "0.014", "train_loss_recon": "0.1", "train_loss_info_nce": "4.125", "train_ppl": "1.01", "train_wps": "1033.1", "train_ups": "0.35", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "22021", "train_lr": "6.08239e-05", "train_gnorm": "1.035", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "682", "train_gb_free": "2.2", "train_wall": "57424"}
[2025-07-02 12:00:42,534][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 12:00:42,541][fairseq.trainer][INFO] - begin training epoch 92
[2025-07-02 12:00:42,541][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 12:08:35,076][train_inner][INFO] - {"epoch": 92, "update": 91.74, "loss": "5.124", "nll_loss": "0.014", "loss_recon": "0.1", "loss_info_nce": "4.124", "ppl": "1.01", "wps": "1109.9", "ups": "0.38", "wpb": "2954.6", "bsz": "385.8", "num_updates": "22200", "lr": "6.04348e-05", "gnorm": "1.103", "clip": "0", "loss_scale": "65536", "train_wall": "524", "gb_free": "2.2", "wall": "57897"}
[2025-07-02 12:11:31,283][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 92 @ 22263 updates
[2025-07-02 12:11:31,283][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint92.pt
[2025-07-02 12:11:33,217][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint92.pt
[2025-07-02 12:11:34,310][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint92.pt (epoch 92 @ 22263 updates, score None) (writing took 3.0151334000038332 seconds)
[2025-07-02 12:11:34,310][fairseq_cli.train][INFO] - end of epoch 92 (average epoch stats below)
[2025-07-02 12:11:34,319][train][INFO] - {"epoch": 92, "train_loss": "5.123", "train_nll_loss": "0.014", "train_loss_recon": "0.1", "train_loss_info_nce": "4.122", "train_ppl": "1.01", "train_wps": "1097.2", "train_ups": "0.37", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "22263", "train_lr": "6.02978e-05", "train_gnorm": "1.036", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "644", "train_gb_free": "2.2", "train_wall": "58076"}
[2025-07-02 12:11:34,710][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 12:11:34,712][fairseq.trainer][INFO] - begin training epoch 93
[2025-07-02 12:11:34,712][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 12:18:18,922][train_inner][INFO] - {"epoch": 93, "update": 92.566, "loss": "5.121", "nll_loss": "0.014", "loss_recon": "0.1", "loss_info_nce": "4.119", "ppl": "1.01", "wps": "1012.8", "ups": "0.34", "wpb": "2956.5", "bsz": "385.4", "num_updates": "22400", "lr": "6e-05", "gnorm": "0.978", "clip": "0", "loss_scale": "65536", "train_wall": "575", "gb_free": "2.2", "wall": "58481"}
[2025-07-02 12:23:25,464][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 93 @ 22505 updates
[2025-07-02 12:23:25,464][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint93.pt
[2025-07-02 12:23:27,424][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint93.pt
[2025-07-02 12:23:28,655][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint93.pt (epoch 93 @ 22505 updates, score None) (writing took 3.1936639999985346 seconds)
[2025-07-02 12:23:28,655][fairseq_cli.train][INFO] - end of epoch 93 (average epoch stats below)
[2025-07-02 12:23:28,667][train][INFO] - {"epoch": 93, "train_loss": "5.119", "train_nll_loss": "0.014", "train_loss_recon": "0.1", "train_loss_info_nce": "4.119", "train_ppl": "1.01", "train_wps": "1001.7", "train_ups": "0.34", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "22505", "train_lr": "5.97717e-05", "train_gnorm": "0.998", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "705", "train_gb_free": "2.2", "train_wall": "58791"}
[2025-07-02 12:23:29,067][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 12:23:29,067][fairseq.trainer][INFO] - begin training epoch 94
[2025-07-02 12:23:29,067][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 12:28:10,067][train_inner][INFO] - {"epoch": 94, "update": 93.393, "loss": "5.117", "nll_loss": "0.014", "loss_recon": "0.1", "loss_info_nce": "4.118", "ppl": "1.01", "wps": "1000.9", "ups": "0.34", "wpb": "2958.3", "bsz": "384", "num_updates": "22600", "lr": "5.95652e-05", "gnorm": "0.954", "clip": "0", "loss_scale": "65536", "train_wall": "582", "gb_free": "2.2", "wall": "59072"}
[2025-07-02 12:35:17,902][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 94 @ 22747 updates
[2025-07-02 12:35:17,902][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint94.pt
[2025-07-02 12:35:19,909][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint94.pt
[2025-07-02 12:35:20,915][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint94.pt (epoch 94 @ 22747 updates, score None) (writing took 3.01791050000611 seconds)
[2025-07-02 12:35:20,915][fairseq_cli.train][INFO] - end of epoch 94 (average epoch stats below)
[2025-07-02 12:35:20,924][train][INFO] - {"epoch": 94, "train_loss": "5.116", "train_nll_loss": "0.014", "train_loss_recon": "0.1", "train_loss_info_nce": "4.116", "train_ppl": "1.01", "train_wps": "1004.6", "train_ups": "0.34", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "22747", "train_lr": "5.92457e-05", "train_gnorm": "0.926", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "703", "train_gb_free": "2.2", "train_wall": "59503"}
[2025-07-02 12:35:21,332][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 12:35:21,334][fairseq.trainer][INFO] - begin training epoch 95
[2025-07-02 12:35:21,334][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 12:37:39,361][train_inner][INFO] - {"epoch": 95, "update": 94.219, "loss": "5.112", "nll_loss": "0.014", "loss_recon": "0.1", "loss_info_nce": "4.112", "ppl": "1.01", "wps": "1036.7", "ups": "0.35", "wpb": "2950.9", "bsz": "387.2", "num_updates": "22800", "lr": "5.91304e-05", "gnorm": "0.946", "clip": "0", "loss_scale": "65536", "train_wall": "561", "gb_free": "2.2", "wall": "59641"}
[2025-07-02 12:45:12,372][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-02 12:45:42,575][valid][INFO] - {"epoch": 95, "valid_loss": "4.872", "valid_nll_loss": "0.013", "valid_loss_recon": "0.096", "valid_loss_info_nce": "3.916", "valid_ppl": "1.01", "valid_wps": "2733.5", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "22989", "valid_best_loss": "4.857"}
[2025-07-02 12:45:42,575][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 95 @ 22989 updates
[2025-07-02 12:45:42,575][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint95.pt
[2025-07-02 12:45:44,413][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint95.pt
[2025-07-02 12:45:45,201][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint95.pt (epoch 95 @ 22989 updates, score 4.872) (writing took 2.6241714999996475 seconds)
[2025-07-02 12:45:45,201][fairseq_cli.train][INFO] - end of epoch 95 (average epoch stats below)
[2025-07-02 12:45:45,211][train][INFO] - {"epoch": 95, "train_loss": "5.109", "train_nll_loss": "0.014", "train_loss_recon": "0.1", "train_loss_info_nce": "4.11", "train_ppl": "1.01", "train_wps": "1146.2", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "22989", "train_lr": "5.87196e-05", "train_gnorm": "0.953", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "587", "train_gb_free": "2.2", "train_wall": "60127"}
[2025-07-02 12:45:45,442][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 12:45:45,442][fairseq.trainer][INFO] - begin training epoch 96
[2025-07-02 12:45:45,442][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 12:46:13,757][train_inner][INFO] - {"epoch": 96, "update": 95.045, "loss": "5.11", "nll_loss": "0.014", "loss_recon": "0.1", "loss_info_nce": "4.111", "ppl": "1.01", "wps": "1149.5", "ups": "0.39", "wpb": "2956.5", "bsz": "385.1", "num_updates": "23000", "lr": "5.86957e-05", "gnorm": "0.948", "clip": "0", "loss_scale": "65536", "train_wall": "478", "gb_free": "2.2", "wall": "60156"}
[2025-07-02 12:54:17,363][train_inner][INFO] - {"epoch": 96, "update": 95.872, "loss": "5.105", "nll_loss": "0.014", "loss_recon": "0.1", "loss_info_nce": "4.108", "ppl": "1.01", "wps": "1225.8", "ups": "0.41", "wpb": "2963.9", "bsz": "386.8", "num_updates": "23200", "lr": "5.82609e-05", "gnorm": "0.925", "clip": "0", "loss_scale": "65536", "train_wall": "482", "gb_free": "2.2", "wall": "60639"}
[2025-07-02 12:55:30,868][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 96 @ 23231 updates
[2025-07-02 12:55:30,868][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint96.pt
[2025-07-02 12:55:32,712][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint96.pt
[2025-07-02 12:55:33,302][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint96.pt (epoch 96 @ 23231 updates, score None) (writing took 2.4261410999970394 seconds)
[2025-07-02 12:55:33,302][fairseq_cli.train][INFO] - end of epoch 96 (average epoch stats below)
[2025-07-02 12:55:33,312][train][INFO] - {"epoch": 96, "train_loss": "5.106", "train_nll_loss": "0.014", "train_loss_recon": "0.1", "train_loss_info_nce": "4.109", "train_ppl": "1.01", "train_wps": "1216.7", "train_ups": "0.41", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "23231", "train_lr": "5.81935e-05", "train_gnorm": "0.937", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "582", "train_gb_free": "2.2", "train_wall": "60715"}
[2025-07-02 12:55:33,511][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 12:55:33,511][fairseq.trainer][INFO] - begin training epoch 97
[2025-07-02 12:55:33,511][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 13:02:50,505][train_inner][INFO] - {"epoch": 97, "update": 96.698, "loss": "5.106", "nll_loss": "0.014", "loss_recon": "0.1", "loss_info_nce": "4.109", "ppl": "1.01", "wps": "1152.3", "ups": "0.39", "wpb": "2956.5", "bsz": "385.4", "num_updates": "23400", "lr": "5.78261e-05", "gnorm": "0.994", "clip": "0", "loss_scale": "65536", "train_wall": "508", "gb_free": "2.2", "wall": "61152"}
[2025-07-02 13:05:54,457][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 97 @ 23473 updates
[2025-07-02 13:05:54,459][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint97.pt
[2025-07-02 13:05:56,489][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint97.pt
[2025-07-02 13:05:57,823][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint97.pt (epoch 97 @ 23473 updates, score None) (writing took 3.367666600002849 seconds)
[2025-07-02 13:05:57,823][fairseq_cli.train][INFO] - end of epoch 97 (average epoch stats below)
[2025-07-02 13:05:57,846][train][INFO] - {"epoch": 97, "train_loss": "5.103", "train_nll_loss": "0.014", "train_loss_recon": "0.1", "train_loss_info_nce": "4.105", "train_ppl": "1.01", "train_wps": "1145.8", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "23473", "train_lr": "5.76674e-05", "train_gnorm": "0.999", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "618", "train_gb_free": "2.3", "train_wall": "61340"}
[2025-07-02 13:05:58,206][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 13:05:58,216][fairseq.trainer][INFO] - begin training epoch 98
[2025-07-02 13:05:58,216][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 13:11:39,091][train_inner][INFO] - {"epoch": 98, "update": 97.525, "loss": "5.1", "nll_loss": "0.014", "loss_recon": "0.1", "loss_info_nce": "4.102", "ppl": "1.01", "wps": "1118.3", "ups": "0.38", "wpb": "2955.5", "bsz": "385.4", "num_updates": "23600", "lr": "5.73913e-05", "gnorm": "0.995", "clip": "0", "loss_scale": "65536", "train_wall": "520", "gb_free": "2.2", "wall": "61681"}
[2025-07-02 13:16:30,992][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 98 @ 23715 updates
[2025-07-02 13:16:30,992][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint98.pt
[2025-07-02 13:16:33,003][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint98.pt
[2025-07-02 13:16:34,274][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint98.pt (epoch 98 @ 23715 updates, score None) (writing took 3.2769790000020294 seconds)
[2025-07-02 13:16:34,275][fairseq_cli.train][INFO] - end of epoch 98 (average epoch stats below)
[2025-07-02 13:16:34,298][train][INFO] - {"epoch": 98, "train_loss": "5.1", "train_nll_loss": "0.014", "train_loss_recon": "0.1", "train_loss_info_nce": "4.103", "train_ppl": "1.01", "train_wps": "1124.3", "train_ups": "0.38", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "23715", "train_lr": "5.71413e-05", "train_gnorm": "0.978", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "628", "train_gb_free": "2.2", "train_wall": "61976"}
[2025-07-02 13:16:34,669][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 13:16:34,669][fairseq.trainer][INFO] - begin training epoch 99
[2025-07-02 13:16:34,669][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 13:20:42,152][train_inner][INFO] - {"epoch": 99, "update": 98.351, "loss": "5.095", "nll_loss": "0.014", "loss_recon": "0.1", "loss_info_nce": "4.1", "ppl": "1.01", "wps": "1087.8", "ups": "0.37", "wpb": "2953.7", "bsz": "385.9", "num_updates": "23800", "lr": "5.69565e-05", "gnorm": "0.922", "clip": "0", "loss_scale": "65536", "train_wall": "535", "gb_free": "2.2", "wall": "62224"}
[2025-07-02 13:27:19,561][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 99 @ 23957 updates
[2025-07-02 13:27:19,562][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint99.pt
[2025-07-02 13:27:21,389][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint99.pt
[2025-07-02 13:27:22,217][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint99.pt (epoch 99 @ 23957 updates, score None) (writing took 2.657675600006769 seconds)
[2025-07-02 13:27:22,217][fairseq_cli.train][INFO] - end of epoch 99 (average epoch stats below)
[2025-07-02 13:27:22,233][train][INFO] - {"epoch": 99, "train_loss": "5.093", "train_nll_loss": "0.014", "train_loss_recon": "0.099", "train_loss_info_nce": "4.098", "train_ppl": "1.01", "train_wps": "1104.4", "train_ups": "0.37", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "23957", "train_lr": "5.66152e-05", "train_gnorm": "0.921", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "639", "train_gb_free": "2.2", "train_wall": "62624"}
[2025-07-02 13:27:22,475][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 13:27:22,475][fairseq.trainer][INFO] - begin training epoch 100
[2025-07-02 13:27:22,475][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 13:29:10,652][train_inner][INFO] - {"epoch": 100, "update": 99.178, "loss": "5.094", "nll_loss": "0.014", "loss_recon": "0.1", "loss_info_nce": "4.098", "ppl": "1.01", "wps": "1162.8", "ups": "0.39", "wpb": "2956.5", "bsz": "385.1", "num_updates": "24000", "lr": "5.65217e-05", "gnorm": "0.963", "clip": "0", "loss_scale": "65536", "train_wall": "502", "gb_free": "2.2", "wall": "62733"}
[2025-07-02 13:30:19,282][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 65536.0
[2025-07-02 13:38:31,980][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-02 13:39:05,283][valid][INFO] - {"epoch": 100, "valid_loss": "4.809", "valid_nll_loss": "0.013", "valid_loss_recon": "0.09", "valid_loss_info_nce": "3.905", "valid_ppl": "1.01", "valid_wps": "2468.2", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "24198", "valid_best_loss": "4.809"}
[2025-07-02 13:39:05,283][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 100 @ 24198 updates
[2025-07-02 13:39:05,283][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint100.pt
[2025-07-02 13:39:07,062][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint100.pt
[2025-07-02 13:39:08,986][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint100.pt (epoch 100 @ 24198 updates, score 4.809) (writing took 3.698179099999834 seconds)
[2025-07-02 13:39:08,986][fairseq_cli.train][INFO] - end of epoch 100 (average epoch stats below)
[2025-07-02 13:39:09,028][train][INFO] - {"epoch": 100, "train_loss": "5.09", "train_nll_loss": "0.014", "train_loss_recon": "0.099", "train_loss_info_nce": "4.096", "train_ppl": "1.01", "train_wps": "1008.2", "train_ups": "0.34", "train_wpb": "2956.7", "train_bsz": "385.8", "train_num_updates": "24198", "train_lr": "5.60913e-05", "train_gnorm": "0.95", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "665", "train_gb_free": "2.2", "train_wall": "63331"}
[2025-07-02 13:39:09,246][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 13:39:09,246][fairseq.trainer][INFO] - begin training epoch 101
[2025-07-02 13:39:09,246][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 13:39:16,828][train_inner][INFO] - {"epoch": 101, "update": 100.008, "loss": "5.088", "nll_loss": "0.014", "loss_recon": "0.099", "loss_info_nce": "4.095", "ppl": "1.01", "wps": "974.5", "ups": "0.33", "wpb": "2953.7", "bsz": "386.4", "num_updates": "24200", "lr": "5.6087e-05", "gnorm": "0.933", "clip": "0", "loss_scale": "65536", "train_wall": "565", "gb_free": "2.2", "wall": "63339"}
[2025-07-02 13:47:48,628][train_inner][INFO] - {"epoch": 101, "update": 100.835, "loss": "5.089", "nll_loss": "0.014", "loss_recon": "0.099", "loss_info_nce": "4.095", "ppl": "1.01", "wps": "1157.5", "ups": "0.39", "wpb": "2962.1", "bsz": "387.4", "num_updates": "24400", "lr": "5.56522e-05", "gnorm": "0.896", "clip": "0", "loss_scale": "65536", "train_wall": "510", "gb_free": "2.2", "wall": "63851"}
[2025-07-02 13:49:37,228][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 101 @ 24440 updates
[2025-07-02 13:49:37,228][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint101.pt
[2025-07-02 13:49:41,953][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint101.pt
[2025-07-02 13:49:42,665][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint101.pt (epoch 101 @ 24440 updates, score None) (writing took 5.440264299999399 seconds)
[2025-07-02 13:49:42,665][fairseq_cli.train][INFO] - end of epoch 101 (average epoch stats below)
[2025-07-02 13:49:42,681][train][INFO] - {"epoch": 101, "train_loss": "5.09", "train_nll_loss": "0.014", "train_loss_recon": "0.099", "train_loss_info_nce": "4.095", "train_ppl": "1.01", "train_wps": "1129.2", "train_ups": "0.38", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "24440", "train_lr": "5.55652e-05", "train_gnorm": "0.927", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "625", "train_gb_free": "2.2", "train_wall": "63965"}
[2025-07-02 13:49:42,897][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 13:49:42,907][fairseq.trainer][INFO] - begin training epoch 102
[2025-07-02 13:49:42,908][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 13:57:01,644][train_inner][INFO] - {"epoch": 102, "update": 101.661, "loss": "5.086", "nll_loss": "0.014", "loss_recon": "0.099", "loss_info_nce": "4.091", "ppl": "1.01", "wps": "1069.6", "ups": "0.36", "wpb": "2957.4", "bsz": "384.8", "num_updates": "24600", "lr": "5.52174e-05", "gnorm": "0.975", "clip": "0", "loss_scale": "65536", "train_wall": "544", "gb_free": "2.2", "wall": "64404"}
[2025-07-02 14:00:30,316][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 102 @ 24682 updates
[2025-07-02 14:00:30,317][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint102.pt
[2025-07-02 14:00:32,094][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint102.pt
[2025-07-02 14:00:32,712][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint102.pt (epoch 102 @ 24682 updates, score None) (writing took 2.3964194000000134 seconds)
[2025-07-02 14:00:32,713][fairseq_cli.train][INFO] - end of epoch 102 (average epoch stats below)
[2025-07-02 14:00:32,720][train][INFO] - {"epoch": 102, "train_loss": "5.084", "train_nll_loss": "0.014", "train_loss_recon": "0.099", "train_loss_info_nce": "4.091", "train_ppl": "1.01", "train_wps": "1100.8", "train_ups": "0.37", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "24682", "train_lr": "5.50391e-05", "train_gnorm": "0.996", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "644", "train_gb_free": "2.2", "train_wall": "64615"}
[2025-07-02 14:00:32,928][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 14:00:32,932][fairseq.trainer][INFO] - begin training epoch 103
[2025-07-02 14:00:32,932][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 14:05:32,325][train_inner][INFO] - {"epoch": 103, "update": 102.488, "loss": "5.084", "nll_loss": "0.014", "loss_recon": "0.099", "loss_info_nce": "4.091", "ppl": "1.01", "wps": "1157.1", "ups": "0.39", "wpb": "2954.6", "bsz": "385.9", "num_updates": "24800", "lr": "5.47826e-05", "gnorm": "0.938", "clip": "0", "loss_scale": "65536", "train_wall": "505", "gb_free": "2.2", "wall": "64914"}
[2025-07-02 14:10:32,329][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 103 @ 24924 updates
[2025-07-02 14:10:32,329][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint103.pt
[2025-07-02 14:10:34,171][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint103.pt
[2025-07-02 14:10:34,849][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint103.pt (epoch 103 @ 24924 updates, score None) (writing took 2.514590099999623 seconds)
[2025-07-02 14:10:34,849][fairseq_cli.train][INFO] - end of epoch 103 (average epoch stats below)
[2025-07-02 14:10:34,858][train][INFO] - {"epoch": 103, "train_loss": "5.079", "train_nll_loss": "0.014", "train_loss_recon": "0.099", "train_loss_info_nce": "4.087", "train_ppl": "1.01", "train_wps": "1188.4", "train_ups": "0.4", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "24924", "train_lr": "5.4513e-05", "train_gnorm": "0.848", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "596", "train_gb_free": "2.2", "train_wall": "65217"}
[2025-07-02 14:10:35,072][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 14:10:35,072][fairseq.trainer][INFO] - begin training epoch 104
[2025-07-02 14:10:35,072][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 14:13:45,092][train_inner][INFO] - {"epoch": 104, "update": 103.314, "loss": "5.078", "nll_loss": "0.014", "loss_recon": "0.099", "loss_info_nce": "4.085", "ppl": "1.01", "wps": "1200", "ups": "0.41", "wpb": "2956.5", "bsz": "385.6", "num_updates": "25000", "lr": "5.43478e-05", "gnorm": "0.912", "clip": "0", "loss_scale": "65536", "train_wall": "487", "gb_free": "2.2", "wall": "65407"}
[2025-07-02 14:13:45,092][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-02 14:14:17,872][valid][INFO] - {"epoch": 104, "valid_loss": "4.834", "valid_nll_loss": "0.013", "valid_loss_recon": "0.094", "valid_loss_info_nce": "3.898", "valid_ppl": "1.01", "valid_wps": "2530.4", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "25000", "valid_best_loss": "4.809"}
[2025-07-02 14:14:17,873][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 104 @ 25000 updates
[2025-07-02 14:14:17,874][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint_104_25000.pt
[2025-07-02 14:14:19,690][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint_104_25000.pt
[2025-07-02 14:14:20,402][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint_104_25000.pt (epoch 104 @ 25000 updates, score 4.834) (writing took 2.5287333999949624 seconds)
[2025-07-02 14:21:49,449][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 104 @ 25166 updates
[2025-07-02 14:21:49,449][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint104.pt
[2025-07-02 14:21:51,455][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint104.pt
[2025-07-02 14:21:52,631][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint104.pt (epoch 104 @ 25166 updates, score None) (writing took 3.184376799996244 seconds)
[2025-07-02 14:21:52,631][fairseq_cli.train][INFO] - end of epoch 104 (average epoch stats below)
[2025-07-02 14:21:52,645][train][INFO] - {"epoch": 104, "train_loss": "5.075", "train_nll_loss": "0.014", "train_loss_recon": "0.099", "train_loss_info_nce": "4.084", "train_ppl": "1.01", "train_wps": "1055.7", "train_ups": "0.36", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "25166", "train_lr": "5.3987e-05", "train_gnorm": "0.968", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "635", "train_gb_free": "2.2", "train_wall": "65895"}
[2025-07-02 14:21:53,016][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 14:21:53,022][fairseq.trainer][INFO] - begin training epoch 105
[2025-07-02 14:21:53,023][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 14:23:33,352][train_inner][INFO] - {"epoch": 105, "update": 104.14, "loss": "5.073", "nll_loss": "0.014", "loss_recon": "0.099", "loss_info_nce": "4.084", "ppl": "1.01", "wps": "1004.9", "ups": "0.34", "wpb": "2955.5", "bsz": "385.4", "num_updates": "25200", "lr": "5.3913e-05", "gnorm": "0.934", "clip": "0", "loss_scale": "65536", "train_wall": "545", "gb_free": "2.2", "wall": "65995"}
[2025-07-02 14:33:08,160][train_inner][INFO] - {"epoch": 105, "update": 104.967, "loss": "5.071", "nll_loss": "0.014", "loss_recon": "0.099", "loss_info_nce": "4.081", "ppl": "1.01", "wps": "1031.3", "ups": "0.35", "wpb": "2963.9", "bsz": "386.8", "num_updates": "25400", "lr": "5.34783e-05", "gnorm": "0.835", "clip": "0", "loss_scale": "65536", "train_wall": "571", "gb_free": "2.2", "wall": "66570"}
[2025-07-02 14:33:28,596][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-02 14:34:21,818][valid][INFO] - {"epoch": 105, "valid_loss": "4.838", "valid_nll_loss": "0.013", "valid_loss_recon": "0.094", "valid_loss_info_nce": "3.893", "valid_ppl": "1.01", "valid_wps": "1549.6", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "25408", "valid_best_loss": "4.809"}
[2025-07-02 14:34:21,818][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 105 @ 25408 updates
[2025-07-02 14:34:21,818][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint105.pt
[2025-07-02 14:34:23,785][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint105.pt
[2025-07-02 14:34:24,911][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint105.pt (epoch 105 @ 25408 updates, score 4.838) (writing took 3.100093300003209 seconds)
[2025-07-02 14:34:24,911][fairseq_cli.train][INFO] - end of epoch 105 (average epoch stats below)
[2025-07-02 14:34:24,927][train][INFO] - {"epoch": 105, "train_loss": "5.072", "train_nll_loss": "0.014", "train_loss_recon": "0.099", "train_loss_info_nce": "4.081", "train_ppl": "1.01", "train_wps": "951.2", "train_ups": "0.32", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "25408", "train_lr": "5.34609e-05", "train_gnorm": "0.848", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "689", "train_gb_free": "2.2", "train_wall": "66647"}
[2025-07-02 14:34:25,284][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 14:34:25,300][fairseq.trainer][INFO] - begin training epoch 106
[2025-07-02 14:34:25,300][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 14:43:55,709][train_inner][INFO] - {"epoch": 106, "update": 105.793, "loss": "5.072", "nll_loss": "0.014", "loss_recon": "0.099", "loss_info_nce": "4.081", "ppl": "1.01", "wps": "912.3", "ups": "0.31", "wpb": "2953.7", "bsz": "385.9", "num_updates": "25600", "lr": "5.30435e-05", "gnorm": "1.012", "clip": "0", "loss_scale": "65536", "train_wall": "585", "gb_free": "2.2", "wall": "67218"}
[2025-07-02 14:46:16,483][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 106 @ 25650 updates
[2025-07-02 14:46:16,484][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint106.pt
[2025-07-02 14:46:18,511][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint106.pt
[2025-07-02 14:46:19,611][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint106.pt (epoch 106 @ 25650 updates, score None) (writing took 3.141977899998892 seconds)
[2025-07-02 14:46:19,611][fairseq_cli.train][INFO] - end of epoch 106 (average epoch stats below)
[2025-07-02 14:46:19,626][train][INFO] - {"epoch": 106, "train_loss": "5.071", "train_nll_loss": "0.014", "train_loss_recon": "0.099", "train_loss_info_nce": "4.081", "train_ppl": "1.01", "train_wps": "1001.2", "train_ups": "0.34", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "25650", "train_lr": "5.29348e-05", "train_gnorm": "0.994", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "705", "train_gb_free": "2.2", "train_wall": "67362"}
[2025-07-02 14:46:20,016][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 14:46:20,032][fairseq.trainer][INFO] - begin training epoch 107
[2025-07-02 14:46:20,032][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 14:53:52,022][train_inner][INFO] - {"epoch": 107, "update": 106.62, "loss": "5.064", "nll_loss": "0.014", "loss_recon": "0.099", "loss_info_nce": "4.076", "ppl": "1.01", "wps": "991.3", "ups": "0.34", "wpb": "2955.5", "bsz": "385.4", "num_updates": "25800", "lr": "5.26087e-05", "gnorm": "0.913", "clip": "0", "loss_scale": "65536", "train_wall": "587", "gb_free": "2.2", "wall": "67814"}
[2025-07-02 14:57:44,931][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 107 @ 25892 updates
[2025-07-02 14:57:44,931][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint107.pt
[2025-07-02 14:57:46,709][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint107.pt
[2025-07-02 14:57:47,306][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint107.pt (epoch 107 @ 25892 updates, score None) (writing took 2.3811013999948045 seconds)
[2025-07-02 14:57:47,306][fairseq_cli.train][INFO] - end of epoch 107 (average epoch stats below)
[2025-07-02 14:57:47,317][train][INFO] - {"epoch": 107, "train_loss": "5.066", "train_nll_loss": "0.014", "train_loss_recon": "0.099", "train_loss_info_nce": "4.077", "train_ppl": "1.01", "train_wps": "1040.5", "train_ups": "0.35", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "25892", "train_lr": "5.24087e-05", "train_gnorm": "0.882", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "679", "train_gb_free": "2.2", "train_wall": "68049"}
[2025-07-02 14:57:47,519][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 14:57:47,519][fairseq.trainer][INFO] - begin training epoch 108
[2025-07-02 14:57:47,519][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 15:03:04,919][train_inner][INFO] - {"epoch": 108, "update": 107.446, "loss": "5.067", "nll_loss": "0.014", "loss_recon": "0.099", "loss_info_nce": "4.078", "ppl": "1.01", "wps": "1068.8", "ups": "0.36", "wpb": "2954.6", "bsz": "386.1", "num_updates": "26000", "lr": "5.21739e-05", "gnorm": "0.892", "clip": "0", "loss_scale": "65536", "train_wall": "547", "gb_free": "2.2", "wall": "68367"}
[2025-07-02 15:09:36,107][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 108 @ 26134 updates
[2025-07-02 15:09:36,107][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint108.pt
[2025-07-02 15:09:38,119][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint108.pt
[2025-07-02 15:09:39,452][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint108.pt (epoch 108 @ 26134 updates, score None) (writing took 3.3451133000053233 seconds)
[2025-07-02 15:09:39,452][fairseq_cli.train][INFO] - end of epoch 108 (average epoch stats below)
[2025-07-02 15:09:39,462][train][INFO] - {"epoch": 108, "train_loss": "5.063", "train_nll_loss": "0.014", "train_loss_recon": "0.099", "train_loss_info_nce": "4.074", "train_ppl": "1.01", "train_wps": "1004.8", "train_ups": "0.34", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "26134", "train_lr": "5.18826e-05", "train_gnorm": "0.881", "train_clip": "0", "train_loss_scale": "131072", "train_train_wall": "703", "train_gb_free": "2.2", "train_wall": "68761"}
[2025-07-02 15:09:39,879][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 15:09:39,885][fairseq.trainer][INFO] - begin training epoch 109
[2025-07-02 15:09:39,885][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 15:13:03,087][train_inner][INFO] - {"epoch": 109, "update": 108.273, "loss": "5.063", "nll_loss": "0.014", "loss_recon": "0.099", "loss_info_nce": "4.073", "ppl": "1.01", "wps": "988.8", "ups": "0.33", "wpb": "2957.4", "bsz": "385.1", "num_updates": "26200", "lr": "5.17391e-05", "gnorm": "0.853", "clip": "0", "loss_scale": "131072", "train_wall": "589", "gb_free": "2.2", "wall": "68965"}
[2025-07-02 15:14:07,725][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 65536.0
[2025-07-02 15:20:15,400][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 109 @ 26375 updates
[2025-07-02 15:20:15,400][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint109.pt
[2025-07-02 15:20:17,169][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint109.pt
[2025-07-02 15:20:17,764][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint109.pt (epoch 109 @ 26375 updates, score None) (writing took 2.373674500006018 seconds)
[2025-07-02 15:20:17,780][fairseq_cli.train][INFO] - end of epoch 109 (average epoch stats below)
[2025-07-02 15:20:17,780][train][INFO] - {"epoch": 109, "train_loss": "5.06", "train_nll_loss": "0.014", "train_loss_recon": "0.099", "train_loss_info_nce": "4.072", "train_ppl": "1.01", "train_wps": "1116.3", "train_ups": "0.38", "train_wpb": "2956.7", "train_bsz": "385.8", "train_num_updates": "26375", "train_lr": "5.13587e-05", "train_gnorm": "0.87", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "631", "train_gb_free": "2.2", "train_wall": "69400"}
[2025-07-02 15:20:17,968][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 15:20:17,984][fairseq.trainer][INFO] - begin training epoch 110
[2025-07-02 15:20:17,984][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 15:21:20,316][train_inner][INFO] - {"epoch": 110, "update": 109.103, "loss": "5.058", "nll_loss": "0.014", "loss_recon": "0.099", "loss_info_nce": "4.072", "ppl": "1.01", "wps": "1188.1", "ups": "0.4", "wpb": "2953.7", "bsz": "385.9", "num_updates": "26400", "lr": "5.13043e-05", "gnorm": "0.853", "clip": "0", "loss_scale": "65536", "train_wall": "491", "gb_free": "2.2", "wall": "69462"}
[2025-07-02 15:29:47,841][train_inner][INFO] - {"epoch": 110, "update": 109.93, "loss": "5.058", "nll_loss": "0.014", "loss_recon": "0.099", "loss_info_nce": "4.071", "ppl": "1.01", "wps": "1168.4", "ups": "0.39", "wpb": "2964.8", "bsz": "387.1", "num_updates": "26600", "lr": "5.08696e-05", "gnorm": "0.877", "clip": "0", "loss_scale": "65536", "train_wall": "507", "gb_free": "2.2", "wall": "69970"}
[2025-07-02 15:30:29,089][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-02 15:31:01,420][valid][INFO] - {"epoch": 110, "valid_loss": "4.784", "valid_nll_loss": "0.013", "valid_loss_recon": "0.09", "valid_loss_info_nce": "3.889", "valid_ppl": "1.01", "valid_wps": "2563.8", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "26617", "valid_best_loss": "4.784"}
[2025-07-02 15:31:01,420][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 110 @ 26617 updates
[2025-07-02 15:31:01,420][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint110.pt
[2025-07-02 15:31:03,208][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250701 4en 1de w norm\output_model\checkpoints\checkpoint110.pt
[2025-07-02 15:31:04,815][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint110.pt (epoch 110 @ 26617 updates, score 4.784) (writing took 3.4013356000068597 seconds)
[2025-07-02 15:31:04,815][fairseq_cli.train][INFO] - end of epoch 110 (average epoch stats below)
[2025-07-02 15:31:04,835][train][INFO] - {"epoch": 110, "train_loss": "5.058", "train_nll_loss": "0.014", "train_loss_recon": "0.099", "train_loss_info_nce": "4.07", "train_ppl": "1.01", "train_wps": "1105.9", "train_ups": "0.37", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "26617", "train_lr": "5.08326e-05", "train_gnorm": "0.861", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "608", "train_gb_free": "2.2", "train_wall": "70047"}
[2025-07-02 15:31:05,046][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 15:31:05,046][fairseq.trainer][INFO] - begin training epoch 111
[2025-07-02 15:31:05,046][fairseq_cli.train][INFO] - Start iterating over samples
