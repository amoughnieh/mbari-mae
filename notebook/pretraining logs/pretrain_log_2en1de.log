[2025-07-04 13:02:30,370][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'tblog', 'wandb_project': None, 'azureml_logging': False, 'seed': 1337, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': 'C:\\Users\\Ali\\OneDrive - Georgia Institute of Technology\\25-5 Summer\\CS 7643 - Deep Learning\\_Project\\MAE-AST-Public\\mae_ast', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 6, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 1048576, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 5, 'validate_interval_updates': 10000, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1048576, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [8], 'lr': [0.0001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 25000, 'keep_interval_updates': 1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'mae_ast', 'ast_kernel_size_chan': 16, 'ast_kernel_size_time': 16, 'ast_kernel_stride_chan': 16, 'ast_kernel_stride_time': 16, 'encoder_layers': 2, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_norm_first': False, 'feature_grad_mult': 0.1, 'use_post_enc_proj': False, 'decoder_embed_dim': 768, 'decoder_layers': 1, 'decoder_layerdrop': 0.0, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'random_mask_prob': 0.75, 'mask_length': 10, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'checkpoint_activations': False, 'max_token_length': 48000, 'enc_sine_pos': True, 'enc_conv_pos': False, 'dec_sine_pos': True, 'dec_conv_pos': False}, 'task': {'_name': 'mae_ast_pretraining', 'data': 'D:\\MBARI 2KHz\\training\\input_dir', 'sample_rate': 2000, 'normalize': True, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 40000, 'min_sample_size': 5000, 'random_crop': True, 'pad_audio': False, 'feature_type': 'fbank', 'feature_rate': 100, 'feature_dim': 128, 'deltas': False, 'mask_spans': False, 'mask_type': random_mask}, 'criterion': {'_name': 'mae_ast', 'reconstruction_weight': 10.0, 'classification_weight': 1.0}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0001]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 4000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000.0, 'lr': [0.0001]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2025-07-04 13:02:30,375][mae_ast.tasks.mae_ast_pretraining][INFO] - current directory is D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model
[2025-07-04 13:02:30,375][mae_ast.tasks.mae_ast_pretraining][INFO] - MAEPretrainingTask Config {'_name': 'mae_ast_pretraining', 'data': 'D:\\MBARI 2KHz\\training\\input_dir', 'sample_rate': 2000, 'normalize': True, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 40000, 'min_sample_size': 5000, 'random_crop': True, 'pad_audio': False, 'feature_type': 'fbank', 'feature_rate': 100, 'feature_dim': 128, 'deltas': False, 'mask_spans': False, 'mask_type': random_mask}
[2025-07-04 13:02:30,375][mae_ast.models.mae_ast][INFO] - MAEModel Config: {'_name': 'mae_ast', 'ast_kernel_size_chan': 16, 'ast_kernel_size_time': 16, 'ast_kernel_stride_chan': 16, 'ast_kernel_stride_time': 16, 'encoder_layers': 2, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_norm_first': False, 'feature_grad_mult': 0.1, 'use_post_enc_proj': False, 'decoder_embed_dim': 768, 'decoder_layers': 1, 'decoder_layerdrop': 0.0, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'random_mask_prob': 0.75, 'mask_length': 10, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'checkpoint_activations': False, 'max_token_length': 48000, 'enc_sine_pos': True, 'enc_conv_pos': False, 'dec_sine_pos': True, 'dec_conv_pos': False}
[2025-07-04 13:02:30,726][fairseq_cli.train][INFO] - MAE_AST(
  (feature_extractor): Identity()
  (post_extract_proj): Linear(in_features=256, out_features=768, bias=True)
  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (batch_norm): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  (unfold): Unfold(kernel_size=(16, 16), dilation=1, padding=0, stride=(16, 16))
  (dropout_input): Dropout(p=0.1, inplace=False)
  (enc_sine_pos_embed): SinusoidalPositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (dec_sine_pos_embed): SinusoidalPositionalEncoding()
  (decoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (final_proj_reconstruction): Linear(in_features=768, out_features=256, bias=True)
  (final_proj_classification): Linear(in_features=768, out_features=256, bias=True)
)
[2025-07-04 13:02:30,729][fairseq_cli.train][INFO] - task: MAE_AST_Pretraining_Task
[2025-07-04 13:02:30,729][fairseq_cli.train][INFO] - model: MAE_AST
[2025-07-04 13:02:30,729][fairseq_cli.train][INFO] - criterion: MAE_AST_Criterion
[2025-07-04 13:02:30,729][fairseq_cli.train][INFO] - num. shared model params: 21,859,584 (num. trained: 21,859,584)
[2025-07-04 13:02:30,729][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2025-07-04 13:02:30,918][mae_ast.data.mae_ast_dataset][INFO] - max_keep=None, min_keep=5000, loaded 10374, skipped 0 short and 0 long, longest-loaded=20000, shortest-loaded=10000
[2025-07-04 13:02:30,918][mae_ast.data.mae_ast_dataset][INFO] - pad_audio=False, random_crop=True, normalize=True, max_sample_size=40000
[2025-07-04 13:02:31,114][fairseq.trainer][INFO] - detected shared parameter: batch_norm.weight <- batch_norm.bias
[2025-07-04 13:02:31,114][fairseq.utils][INFO] - ***********************CUDA enviroments for all 1 workers***********************
[2025-07-04 13:02:31,114][fairseq.utils][INFO] - rank   0: capabilities =  7.5  ; total memory = 6.000 GB ; name = NVIDIA GeForce RTX 2060                 
[2025-07-04 13:02:31,114][fairseq.utils][INFO] - ***********************CUDA enviroments for all 1 workers***********************
[2025-07-04 13:02:31,114][fairseq_cli.train][INFO] - training on 1 devices (GPUs/TPUs)
[2025-07-04 13:02:31,114][fairseq_cli.train][INFO] - max tokens per device = 1048576 and max sentences per device = None
[2025-07-04 13:02:31,114][fairseq.trainer][INFO] - Preparing to load checkpoint checkpoints\checkpoint_last.pt
[2025-07-04 13:02:31,114][fairseq.trainer][INFO] - No existing checkpoint found checkpoints\checkpoint_last.pt
[2025-07-04 13:02:31,114][fairseq.trainer][INFO] - loading train data for epoch 1
[2025-07-04 13:02:31,172][mae_ast.data.mae_ast_dataset][INFO] - max_keep=None, min_keep=5000, loaded 93367, skipped 0 short and 0 long, longest-loaded=20000, shortest-loaded=10000
[2025-07-04 13:02:31,172][mae_ast.data.mae_ast_dataset][INFO] - pad_audio=False, random_crop=True, normalize=True, max_sample_size=40000
[2025-07-04 13:02:32,129][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 13:02:32,129][fairseq.trainer][INFO] - begin training epoch 1
[2025-07-04 13:02:32,129][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 13:09:54,301][train_inner][INFO] - {"epoch": 1, "update": 0.826, "loss": "19.482", "nll_loss": "0.053", "loss_recon": "0.708", "loss_info_nce": "12.404", "ppl": "1.04", "wps": "1409", "ups": "0.48", "wpb": "2965.8", "bsz": "386.3", "num_updates": "200", "lr": "5e-06", "gnorm": "29.686", "clip": "54.5", "loss_scale": "128", "train_wall": "422", "gb_free": "2.9", "wall": "438"}
[2025-07-04 13:11:21,271][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1 @ 242 updates
[2025-07-04 13:11:21,271][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint1.pt
[2025-07-04 13:11:22,664][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint1.pt
[2025-07-04 13:11:24,586][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint1.pt (epoch 1 @ 242 updates, score None) (writing took 3.311967299999992 seconds)
[2025-07-04 13:11:24,586][fairseq_cli.train][INFO] - end of epoch 1 (average epoch stats below)
[2025-07-04 13:11:24,602][train][INFO] - {"epoch": 1, "train_loss": "18.572", "train_nll_loss": "0.05", "train_loss_recon": "0.666", "train_loss_info_nce": "11.903", "train_ppl": "1.04", "train_wps": "1384.9", "train_ups": "0.47", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "242", "train_lr": "6.05e-06", "train_gnorm": "25.133", "train_clip": "45", "train_loss_scale": "128", "train_train_wall": "508", "train_gb_free": "2.9", "train_wall": "533"}
[2025-07-04 13:11:24,784][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 13:11:24,799][fairseq.trainer][INFO] - begin training epoch 2
[2025-07-04 13:11:24,799][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 13:17:03,978][train_inner][INFO] - {"epoch": 2, "update": 1.653, "loss": "12.525", "nll_loss": "0.034", "loss_recon": "0.341", "loss_info_nce": "9.119", "ppl": "1.02", "wps": "1374.9", "ups": "0.47", "wpb": "2953.7", "bsz": "386.4", "num_updates": "400", "lr": "1e-05", "gnorm": "2.795", "clip": "0", "loss_scale": "128", "train_wall": "423", "gb_free": "2.9", "wall": "873"}
[2025-07-04 13:20:02,676][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 2 @ 484 updates
[2025-07-04 13:20:02,677][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint2.pt
[2025-07-04 13:20:04,054][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint2.pt
[2025-07-04 13:20:04,572][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint2.pt (epoch 2 @ 484 updates, score None) (writing took 1.8959529999999631 seconds)
[2025-07-04 13:20:04,572][fairseq_cli.train][INFO] - end of epoch 2 (average epoch stats below)
[2025-07-04 13:20:04,577][train][INFO] - {"epoch": 2, "train_loss": "11.768", "train_nll_loss": "0.032", "train_loss_recon": "0.283", "train_loss_info_nce": "8.932", "train_ppl": "1.02", "train_wps": "1376.1", "train_ups": "0.47", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "484", "train_lr": "1.21e-05", "train_gnorm": "2.522", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "515", "train_gb_free": "2.9", "train_wall": "1053"}
[2025-07-04 13:20:04,767][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 13:20:04,770][fairseq.trainer][INFO] - begin training epoch 3
[2025-07-04 13:20:04,771][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 13:20:29,140][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'tblog', 'wandb_project': None, 'azureml_logging': False, 'seed': 1337, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': 'C:\\Users\\Ali\\OneDrive - Georgia Institute of Technology\\25-5 Summer\\CS 7643 - Deep Learning\\_Project\\MAE-AST-Public\\mae_ast', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 6, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 2097152, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 5, 'validate_interval_updates': 10000, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2097152, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [4], 'lr': [0.0001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 25000, 'keep_interval_updates': 1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'mae_ast', 'ast_kernel_size_chan': 16, 'ast_kernel_size_time': 16, 'ast_kernel_stride_chan': 16, 'ast_kernel_stride_time': 16, 'encoder_layers': 2, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_norm_first': False, 'feature_grad_mult': 0.1, 'use_post_enc_proj': False, 'decoder_embed_dim': 768, 'decoder_layers': 1, 'decoder_layerdrop': 0.0, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'random_mask_prob': 0.75, 'mask_length': 10, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'checkpoint_activations': False, 'max_token_length': 48000, 'enc_sine_pos': True, 'enc_conv_pos': False, 'dec_sine_pos': True, 'dec_conv_pos': False}, 'task': {'_name': 'mae_ast_pretraining', 'data': 'D:\\MBARI 2KHz\\training\\input_dir', 'sample_rate': 2000, 'normalize': True, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 40000, 'min_sample_size': 5000, 'random_crop': True, 'pad_audio': False, 'feature_type': 'fbank', 'feature_rate': 100, 'feature_dim': 128, 'deltas': False, 'mask_spans': False, 'mask_type': random_mask}, 'criterion': {'_name': 'mae_ast', 'reconstruction_weight': 10.0, 'classification_weight': 1.0}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0001]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 4000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000.0, 'lr': [0.0001]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2025-07-04 13:20:29,144][mae_ast.tasks.mae_ast_pretraining][INFO] - current directory is D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model
[2025-07-04 13:20:29,144][mae_ast.tasks.mae_ast_pretraining][INFO] - MAEPretrainingTask Config {'_name': 'mae_ast_pretraining', 'data': 'D:\\MBARI 2KHz\\training\\input_dir', 'sample_rate': 2000, 'normalize': True, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 40000, 'min_sample_size': 5000, 'random_crop': True, 'pad_audio': False, 'feature_type': 'fbank', 'feature_rate': 100, 'feature_dim': 128, 'deltas': False, 'mask_spans': False, 'mask_type': random_mask}
[2025-07-04 13:20:29,147][mae_ast.models.mae_ast][INFO] - MAEModel Config: {'_name': 'mae_ast', 'ast_kernel_size_chan': 16, 'ast_kernel_size_time': 16, 'ast_kernel_stride_chan': 16, 'ast_kernel_stride_time': 16, 'encoder_layers': 2, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_norm_first': False, 'feature_grad_mult': 0.1, 'use_post_enc_proj': False, 'decoder_embed_dim': 768, 'decoder_layers': 1, 'decoder_layerdrop': 0.0, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'random_mask_prob': 0.75, 'mask_length': 10, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'checkpoint_activations': False, 'max_token_length': 48000, 'enc_sine_pos': True, 'enc_conv_pos': False, 'dec_sine_pos': True, 'dec_conv_pos': False}
[2025-07-04 13:20:29,486][fairseq_cli.train][INFO] - MAE_AST(
  (feature_extractor): Identity()
  (post_extract_proj): Linear(in_features=256, out_features=768, bias=True)
  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (batch_norm): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  (unfold): Unfold(kernel_size=(16, 16), dilation=1, padding=0, stride=(16, 16))
  (dropout_input): Dropout(p=0.1, inplace=False)
  (enc_sine_pos_embed): SinusoidalPositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (dec_sine_pos_embed): SinusoidalPositionalEncoding()
  (decoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (final_proj_reconstruction): Linear(in_features=768, out_features=256, bias=True)
  (final_proj_classification): Linear(in_features=768, out_features=256, bias=True)
)
[2025-07-04 13:20:29,488][fairseq_cli.train][INFO] - task: MAE_AST_Pretraining_Task
[2025-07-04 13:20:29,488][fairseq_cli.train][INFO] - model: MAE_AST
[2025-07-04 13:20:29,488][fairseq_cli.train][INFO] - criterion: MAE_AST_Criterion
[2025-07-04 13:20:29,488][fairseq_cli.train][INFO] - num. shared model params: 21,859,584 (num. trained: 21,859,584)
[2025-07-04 13:20:29,489][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2025-07-04 13:20:29,661][mae_ast.data.mae_ast_dataset][INFO] - max_keep=None, min_keep=5000, loaded 10374, skipped 0 short and 0 long, longest-loaded=20000, shortest-loaded=10000
[2025-07-04 13:20:29,661][mae_ast.data.mae_ast_dataset][INFO] - pad_audio=False, random_crop=True, normalize=True, max_sample_size=40000
[2025-07-04 13:20:29,854][fairseq.trainer][INFO] - detected shared parameter: batch_norm.weight <- batch_norm.bias
[2025-07-04 13:20:29,854][fairseq.utils][INFO] - ***********************CUDA enviroments for all 1 workers***********************
[2025-07-04 13:20:29,854][fairseq.utils][INFO] - rank   0: capabilities =  7.5  ; total memory = 6.000 GB ; name = NVIDIA GeForce RTX 2060                 
[2025-07-04 13:20:29,854][fairseq.utils][INFO] - ***********************CUDA enviroments for all 1 workers***********************
[2025-07-04 13:20:29,855][fairseq_cli.train][INFO] - training on 1 devices (GPUs/TPUs)
[2025-07-04 13:20:29,855][fairseq_cli.train][INFO] - max tokens per device = 2097152 and max sentences per device = None
[2025-07-04 13:20:29,856][fairseq.trainer][INFO] - Preparing to load checkpoint checkpoints\checkpoint_last.pt
[2025-07-04 13:20:30,558][fairseq.trainer][INFO] - Loaded checkpoint checkpoints\checkpoint_last.pt (epoch 3 @ 484 updates)
[2025-07-04 13:20:30,558][fairseq.trainer][INFO] - loading train data for epoch 3
[2025-07-04 13:20:30,610][mae_ast.data.mae_ast_dataset][INFO] - max_keep=None, min_keep=5000, loaded 93367, skipped 0 short and 0 long, longest-loaded=20000, shortest-loaded=10000
[2025-07-04 13:20:30,611][mae_ast.data.mae_ast_dataset][INFO] - pad_audio=False, random_crop=True, normalize=True, max_sample_size=40000
[2025-07-04 13:20:31,204][fairseq.data.iterators][INFO] - grouped total_num_itrs = 224
[2025-07-04 13:20:31,208][fairseq.trainer][INFO] - begin training epoch 3
[2025-07-04 13:20:31,208][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 13:35:18,921][train_inner][INFO] - {"epoch": 3, "update": 2.518, "loss": "10.959", "nll_loss": "0.03", "loss_recon": "0.227", "loss_info_nce": "8.688", "ppl": "1.02", "wps": "197.9", "ups": "0.13", "wpb": "1480", "bsz": "417.6", "num_updates": "600", "lr": "1.5e-05", "gnorm": "2.717", "clip": "0", "loss_scale": "128", "train_wall": "870", "gb_free": "0", "wall": "888"}
[2025-07-04 13:49:12,737][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 3 @ 708 updates
[2025-07-04 13:49:12,738][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint3.pt
[2025-07-04 13:49:14,114][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint3.pt
[2025-07-04 13:49:14,750][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint3.pt (epoch 3 @ 708 updates, score None) (writing took 2.012776699999904 seconds)
[2025-07-04 13:49:14,752][fairseq_cli.train][INFO] - end of epoch 3 (average epoch stats below)
[2025-07-04 13:49:14,794][train][INFO] - {"epoch": 3, "train_loss": "10.908", "train_nll_loss": "0.029", "train_loss_recon": "0.225", "train_loss_info_nce": "8.658", "train_ppl": "1.02", "train_wps": "193.9", "train_ups": "0.13", "train_wpb": "1475.5", "train_bsz": "416.8", "train_num_updates": "708", "train_lr": "1.77e-05", "train_gnorm": "3.783", "train_clip": "4", "train_loss_scale": "128", "train_train_wall": "1704", "train_gb_free": "0", "train_wall": "1725"}
[2025-07-04 13:49:15,238][fairseq.data.iterators][INFO] - grouped total_num_itrs = 224
[2025-07-04 13:49:15,242][fairseq.trainer][INFO] - begin training epoch 4
[2025-07-04 13:49:15,242][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 13:49:43,104][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'tblog', 'wandb_project': None, 'azureml_logging': False, 'seed': 1337, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': 'C:\\Users\\Ali\\OneDrive - Georgia Institute of Technology\\25-5 Summer\\CS 7643 - Deep Learning\\_Project\\MAE-AST-Public\\mae_ast', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 6, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 1048576, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 5, 'validate_interval_updates': 10000, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1048576, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [8], 'lr': [0.0001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 25000, 'keep_interval_updates': 1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'mae_ast', 'ast_kernel_size_chan': 16, 'ast_kernel_size_time': 16, 'ast_kernel_stride_chan': 16, 'ast_kernel_stride_time': 16, 'encoder_layers': 2, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_norm_first': False, 'feature_grad_mult': 0.1, 'use_post_enc_proj': False, 'decoder_embed_dim': 768, 'decoder_layers': 1, 'decoder_layerdrop': 0.0, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'random_mask_prob': 0.75, 'mask_length': 10, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'checkpoint_activations': False, 'max_token_length': 48000, 'enc_sine_pos': True, 'enc_conv_pos': False, 'dec_sine_pos': True, 'dec_conv_pos': False}, 'task': {'_name': 'mae_ast_pretraining', 'data': 'D:\\MBARI 2KHz\\training\\input_dir', 'sample_rate': 2000, 'normalize': True, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 40000, 'min_sample_size': 5000, 'random_crop': True, 'pad_audio': False, 'feature_type': 'fbank', 'feature_rate': 100, 'feature_dim': 128, 'deltas': False, 'mask_spans': False, 'mask_type': random_mask}, 'criterion': {'_name': 'mae_ast', 'reconstruction_weight': 10.0, 'classification_weight': 1.0}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0001]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 4000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000.0, 'lr': [0.0001]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2025-07-04 13:49:43,108][mae_ast.tasks.mae_ast_pretraining][INFO] - current directory is D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model
[2025-07-04 13:49:43,108][mae_ast.tasks.mae_ast_pretraining][INFO] - MAEPretrainingTask Config {'_name': 'mae_ast_pretraining', 'data': 'D:\\MBARI 2KHz\\training\\input_dir', 'sample_rate': 2000, 'normalize': True, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 40000, 'min_sample_size': 5000, 'random_crop': True, 'pad_audio': False, 'feature_type': 'fbank', 'feature_rate': 100, 'feature_dim': 128, 'deltas': False, 'mask_spans': False, 'mask_type': random_mask}
[2025-07-04 13:49:43,111][mae_ast.models.mae_ast][INFO] - MAEModel Config: {'_name': 'mae_ast', 'ast_kernel_size_chan': 16, 'ast_kernel_size_time': 16, 'ast_kernel_stride_chan': 16, 'ast_kernel_stride_time': 16, 'encoder_layers': 2, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_norm_first': False, 'feature_grad_mult': 0.1, 'use_post_enc_proj': False, 'decoder_embed_dim': 768, 'decoder_layers': 1, 'decoder_layerdrop': 0.0, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'random_mask_prob': 0.75, 'mask_length': 10, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'checkpoint_activations': False, 'max_token_length': 48000, 'enc_sine_pos': True, 'enc_conv_pos': False, 'dec_sine_pos': True, 'dec_conv_pos': False}
[2025-07-04 13:49:43,454][fairseq_cli.train][INFO] - MAE_AST(
  (feature_extractor): Identity()
  (post_extract_proj): Linear(in_features=256, out_features=768, bias=True)
  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (batch_norm): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  (unfold): Unfold(kernel_size=(16, 16), dilation=1, padding=0, stride=(16, 16))
  (dropout_input): Dropout(p=0.1, inplace=False)
  (enc_sine_pos_embed): SinusoidalPositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (dec_sine_pos_embed): SinusoidalPositionalEncoding()
  (decoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (final_proj_reconstruction): Linear(in_features=768, out_features=256, bias=True)
  (final_proj_classification): Linear(in_features=768, out_features=256, bias=True)
)
[2025-07-04 13:49:43,455][fairseq_cli.train][INFO] - task: MAE_AST_Pretraining_Task
[2025-07-04 13:49:43,455][fairseq_cli.train][INFO] - model: MAE_AST
[2025-07-04 13:49:43,456][fairseq_cli.train][INFO] - criterion: MAE_AST_Criterion
[2025-07-04 13:49:43,456][fairseq_cli.train][INFO] - num. shared model params: 21,859,584 (num. trained: 21,859,584)
[2025-07-04 13:49:43,457][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2025-07-04 13:49:43,621][mae_ast.data.mae_ast_dataset][INFO] - max_keep=None, min_keep=5000, loaded 10374, skipped 0 short and 0 long, longest-loaded=20000, shortest-loaded=10000
[2025-07-04 13:49:43,621][mae_ast.data.mae_ast_dataset][INFO] - pad_audio=False, random_crop=True, normalize=True, max_sample_size=40000
[2025-07-04 13:49:43,814][fairseq.trainer][INFO] - detected shared parameter: batch_norm.weight <- batch_norm.bias
[2025-07-04 13:49:43,814][fairseq.utils][INFO] - ***********************CUDA enviroments for all 1 workers***********************
[2025-07-04 13:49:43,814][fairseq.utils][INFO] - rank   0: capabilities =  7.5  ; total memory = 6.000 GB ; name = NVIDIA GeForce RTX 2060                 
[2025-07-04 13:49:43,814][fairseq.utils][INFO] - ***********************CUDA enviroments for all 1 workers***********************
[2025-07-04 13:49:43,815][fairseq_cli.train][INFO] - training on 1 devices (GPUs/TPUs)
[2025-07-04 13:49:43,815][fairseq_cli.train][INFO] - max tokens per device = 1048576 and max sentences per device = None
[2025-07-04 13:49:43,816][fairseq.trainer][INFO] - Preparing to load checkpoint checkpoints\checkpoint_last.pt
[2025-07-04 13:49:44,529][fairseq.trainer][INFO] - Loaded checkpoint checkpoints\checkpoint_last.pt (epoch 4 @ 708 updates)
[2025-07-04 13:49:44,530][fairseq.trainer][INFO] - loading train data for epoch 4
[2025-07-04 13:49:44,582][mae_ast.data.mae_ast_dataset][INFO] - max_keep=None, min_keep=5000, loaded 93367, skipped 0 short and 0 long, longest-loaded=20000, shortest-loaded=10000
[2025-07-04 13:49:44,583][mae_ast.data.mae_ast_dataset][INFO] - pad_audio=False, random_crop=True, normalize=True, max_sample_size=40000
[2025-07-04 13:49:45,008][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 13:49:45,011][fairseq.trainer][INFO] - begin training epoch 4
[2025-07-04 13:49:45,012][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 13:53:23,494][train_inner][INFO] - {"epoch": 4, "update": 3.38, "loss": "10.789", "nll_loss": "0.029", "loss_recon": "0.22", "loss_info_nce": "8.592", "ppl": "1.02", "wps": "1370.6", "ups": "0.46", "wpb": "2969.9", "bsz": "385.7", "num_updates": "800", "lr": "2e-05", "gnorm": "3.209", "clip": "0", "loss_scale": "128", "train_wall": "200", "gb_free": "2.9", "wall": "218"}
[2025-07-04 13:59:18,132][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 4 @ 950 updates
[2025-07-04 13:59:18,132][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint4.pt
[2025-07-04 13:59:19,584][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint4.pt
[2025-07-04 13:59:20,460][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint4.pt (epoch 4 @ 950 updates, score None) (writing took 2.3295544000000064 seconds)
[2025-07-04 13:59:20,460][fairseq_cli.train][INFO] - end of epoch 4 (average epoch stats below)
[2025-07-04 13:59:20,476][train][INFO] - {"epoch": 4, "train_loss": "10.726", "train_nll_loss": "0.029", "train_loss_recon": "0.216", "train_loss_info_nce": "8.562", "train_ppl": "1.02", "train_wps": "1282.3", "train_ups": "0.43", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "950", "train_lr": "2.375e-05", "train_gnorm": "3.564", "train_clip": "0.4", "train_loss_scale": "128", "train_train_wall": "553", "train_gb_free": "2.9", "train_wall": "577"}
[2025-07-04 13:59:20,814][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 13:59:20,814][fairseq.trainer][INFO] - begin training epoch 5
[2025-07-04 13:59:20,814][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 14:01:31,927][train_inner][INFO] - {"epoch": 5, "update": 4.207, "loss": "10.667", "nll_loss": "0.029", "loss_recon": "0.213", "loss_info_nce": "8.536", "ppl": "1.02", "wps": "1208.3", "ups": "0.41", "wpb": "2950.9", "bsz": "386", "num_updates": "1000", "lr": "2.5e-05", "gnorm": "3.785", "clip": "0.5", "loss_scale": "128", "train_wall": "480", "gb_free": "2.9", "wall": "708"}
[2025-07-04 14:09:39,369][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-04 14:10:43,542][valid][INFO] - {"epoch": 5, "valid_loss": "10.301", "valid_nll_loss": "0.028", "valid_loss_recon": "0.19", "valid_loss_info_nce": "8.397", "valid_ppl": "1.02", "valid_wps": "1615.3", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "1192"}
[2025-07-04 14:10:43,552][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 5 @ 1192 updates
[2025-07-04 14:10:43,552][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint5.pt
[2025-07-04 14:10:45,124][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint5.pt
[2025-07-04 14:10:47,847][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint5.pt (epoch 5 @ 1192 updates, score 10.301) (writing took 4.2979349000002 seconds)
[2025-07-04 14:10:47,847][fairseq_cli.train][INFO] - end of epoch 5 (average epoch stats below)
[2025-07-04 14:10:47,888][train][INFO] - {"epoch": 5, "train_loss": "10.533", "train_nll_loss": "0.028", "train_loss_recon": "0.204", "train_loss_info_nce": "8.496", "train_ppl": "1.02", "train_wps": "1041", "train_ups": "0.35", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "1192", "train_lr": "2.98e-05", "train_gnorm": "4.128", "train_clip": "3.7", "train_loss_scale": "128", "train_train_wall": "610", "train_gb_free": "2.9", "train_wall": "1264"}
[2025-07-04 14:10:48,310][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 14:10:48,325][fairseq.trainer][INFO] - begin training epoch 6
[2025-07-04 14:10:48,325][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 14:11:12,389][train_inner][INFO] - {"epoch": 6, "update": 5.033, "loss": "10.511", "nll_loss": "0.028", "loss_recon": "0.202", "loss_info_nce": "8.491", "ppl": "1.02", "wps": "1019", "ups": "0.34", "wpb": "2957.4", "bsz": "385.6", "num_updates": "1200", "lr": "3e-05", "gnorm": "4.244", "clip": "4.5", "loss_scale": "128", "train_wall": "504", "gb_free": "2.9", "wall": "1289"}
[2025-07-04 14:19:42,018][train_inner][INFO] - {"epoch": 6, "update": 5.86, "loss": "10.403", "nll_loss": "0.028", "loss_recon": "0.195", "loss_info_nce": "8.454", "ppl": "1.02", "wps": "1162.8", "ups": "0.39", "wpb": "2963", "bsz": "387.1", "num_updates": "1400", "lr": "3.5e-05", "gnorm": "4.95", "clip": "9.5", "loss_scale": "128", "train_wall": "506", "gb_free": "2.9", "wall": "1798"}
[2025-07-04 14:21:05,428][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 6 @ 1434 updates
[2025-07-04 14:21:05,444][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint6.pt
[2025-07-04 14:21:06,931][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint6.pt
[2025-07-04 14:21:07,764][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint6.pt (epoch 6 @ 1434 updates, score None) (writing took 2.3349975000000995 seconds)
[2025-07-04 14:21:07,764][fairseq_cli.train][INFO] - end of epoch 6 (average epoch stats below)
[2025-07-04 14:21:07,780][train][INFO] - {"epoch": 6, "train_loss": "10.399", "train_nll_loss": "0.028", "train_loss_recon": "0.195", "train_loss_info_nce": "8.453", "train_ppl": "1.02", "train_wps": "1154.3", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "1434", "train_lr": "3.585e-05", "train_gnorm": "4.967", "train_clip": "9.1", "train_loss_scale": "128", "train_train_wall": "610", "train_gb_free": "2.9", "train_wall": "1884"}
[2025-07-04 14:21:08,118][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 14:21:08,118][fairseq.trainer][INFO] - begin training epoch 7
[2025-07-04 14:21:08,118][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 14:28:15,521][train_inner][INFO] - {"epoch": 7, "update": 6.686, "loss": "10.316", "nll_loss": "0.028", "loss_recon": "0.189", "loss_info_nce": "8.423", "ppl": "1.02", "wps": "1151.5", "ups": "0.39", "wpb": "2956.5", "bsz": "385.6", "num_updates": "1600", "lr": "4e-05", "gnorm": "6.225", "clip": "19.5", "loss_scale": "128", "train_wall": "504", "gb_free": "2.9", "wall": "2312"}
[2025-07-04 14:31:25,891][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 7 @ 1676 updates
[2025-07-04 14:31:25,901][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint7.pt
[2025-07-04 14:31:27,409][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint7.pt
[2025-07-04 14:31:28,231][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint7.pt (epoch 7 @ 1676 updates, score None) (writing took 2.3327933999999004 seconds)
[2025-07-04 14:31:28,231][fairseq_cli.train][INFO] - end of epoch 7 (average epoch stats below)
[2025-07-04 14:31:28,231][train][INFO] - {"epoch": 7, "train_loss": "10.32", "train_nll_loss": "0.028", "train_loss_recon": "0.19", "train_loss_info_nce": "8.421", "train_ppl": "1.02", "train_wps": "1153.3", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "1676", "train_lr": "4.19e-05", "train_gnorm": "8.648", "train_clip": "25.2", "train_loss_scale": "128", "train_train_wall": "611", "train_gb_free": "2.9", "train_wall": "2504"}
[2025-07-04 14:31:28,610][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 14:31:28,626][fairseq.trainer][INFO] - begin training epoch 8
[2025-07-04 14:31:28,626][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 14:36:48,912][train_inner][INFO] - {"epoch": 8, "update": 7.512, "loss": "10.296", "nll_loss": "0.028", "loss_recon": "0.188", "loss_info_nce": "8.416", "ppl": "1.02", "wps": "1151", "ups": "0.39", "wpb": "2954.6", "bsz": "385.9", "num_updates": "1800", "lr": "4.5e-05", "gnorm": "12.216", "clip": "32", "loss_scale": "128", "train_wall": "504", "gb_free": "2.9", "wall": "2825"}
[2025-07-04 14:41:46,844][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 8 @ 1918 updates
[2025-07-04 14:41:46,844][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint8.pt
[2025-07-04 14:41:48,310][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint8.pt
[2025-07-04 14:41:49,182][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint8.pt (epoch 8 @ 1918 updates, score None) (writing took 2.3468909000002895 seconds)
[2025-07-04 14:41:49,182][fairseq_cli.train][INFO] - end of epoch 8 (average epoch stats below)
[2025-07-04 14:41:49,198][train][INFO] - {"epoch": 8, "train_loss": "10.227", "train_nll_loss": "0.028", "train_loss_recon": "0.184", "train_loss_info_nce": "8.385", "train_ppl": "1.02", "train_wps": "1152.3", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "1918", "train_lr": "4.795e-05", "train_gnorm": "8.784", "train_clip": "25.2", "train_loss_scale": "128", "train_train_wall": "611", "train_gb_free": "2.9", "train_wall": "3125"}
[2025-07-04 14:41:49,564][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 14:41:49,564][fairseq.trainer][INFO] - begin training epoch 9
[2025-07-04 14:41:49,564][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 14:45:22,921][train_inner][INFO] - {"epoch": 9, "update": 8.339, "loss": "10.205", "nll_loss": "0.028", "loss_recon": "0.184", "loss_info_nce": "8.361", "ppl": "1.02", "wps": "1148.9", "ups": "0.39", "wpb": "2952.8", "bsz": "386.2", "num_updates": "2000", "lr": "5e-05", "gnorm": "9.458", "clip": "31", "loss_scale": "128", "train_wall": "505", "gb_free": "2.9", "wall": "3339"}
[2025-07-04 14:52:04,623][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 9 @ 2160 updates
[2025-07-04 14:52:04,623][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint9.pt
[2025-07-04 14:52:06,135][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint9.pt
[2025-07-04 14:52:06,944][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint9.pt (epoch 9 @ 2160 updates, score None) (writing took 2.312796199999866 seconds)
[2025-07-04 14:52:06,944][fairseq_cli.train][INFO] - end of epoch 9 (average epoch stats below)
[2025-07-04 14:52:06,944][train][INFO] - {"epoch": 9, "train_loss": "10.201", "train_nll_loss": "0.028", "train_loss_recon": "0.184", "train_loss_info_nce": "8.359", "train_ppl": "1.02", "train_wps": "1158.3", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "2160", "train_lr": "5.4e-05", "train_gnorm": "13.882", "train_clip": "47.1", "train_loss_scale": "128", "train_train_wall": "609", "train_gb_free": "2.9", "train_wall": "3743"}
[2025-07-04 14:52:07,323][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 14:52:07,323][fairseq.trainer][INFO] - begin training epoch 10
[2025-07-04 14:52:07,338][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 14:53:53,198][train_inner][INFO] - {"epoch": 10, "update": 9.165, "loss": "10.168", "nll_loss": "0.027", "loss_recon": "0.182", "loss_info_nce": "8.346", "ppl": "1.02", "wps": "1159.2", "ups": "0.39", "wpb": "2957.4", "bsz": "384.8", "num_updates": "2200", "lr": "5.5e-05", "gnorm": "11.547", "clip": "37", "loss_scale": "128", "train_wall": "502", "gb_free": "2.9", "wall": "3849"}
[2025-07-04 15:02:22,539][train_inner][INFO] - {"epoch": 10, "update": 9.992, "loss": "10.039", "nll_loss": "0.027", "loss_recon": "0.176", "loss_info_nce": "8.279", "ppl": "1.02", "wps": "1164.2", "ups": "0.39", "wpb": "2964.8", "bsz": "386.6", "num_updates": "2400", "lr": "6e-05", "gnorm": "7.194", "clip": "18.5", "loss_scale": "128", "train_wall": "505", "gb_free": "2.9", "wall": "4359"}
[2025-07-04 15:02:25,454][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-04 15:03:17,934][valid][INFO] - {"epoch": 10, "valid_loss": "9.814", "valid_nll_loss": "0.027", "valid_loss_recon": "0.166", "valid_loss_info_nce": "8.157", "valid_ppl": "1.02", "valid_wps": "1562.9", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "2402", "valid_best_loss": "9.814"}
[2025-07-04 15:03:17,934][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 10 @ 2402 updates
[2025-07-04 15:03:17,934][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint10.pt
[2025-07-04 15:03:19,483][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint10.pt
[2025-07-04 15:03:21,242][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint10.pt (epoch 10 @ 2402 updates, score 9.814) (writing took 3.3010289000003468 seconds)
[2025-07-04 15:03:21,242][fairseq_cli.train][INFO] - end of epoch 10 (average epoch stats below)
[2025-07-04 15:03:21,252][train][INFO] - {"epoch": 10, "train_loss": "10.047", "train_nll_loss": "0.027", "train_loss_recon": "0.176", "train_loss_info_nce": "8.282", "train_ppl": "1.02", "train_wps": "1061.2", "train_ups": "0.36", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "2402", "train_lr": "6.005e-05", "train_gnorm": "6.586", "train_clip": "15.7", "train_loss_scale": "128", "train_train_wall": "610", "train_gb_free": "2.9", "train_wall": "4417"}
[2025-07-04 15:03:21,661][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 15:03:21,677][fairseq.trainer][INFO] - begin training epoch 11
[2025-07-04 15:03:21,677][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 15:11:52,424][train_inner][INFO] - {"epoch": 11, "update": 10.818, "loss": "9.972", "nll_loss": "0.027", "loss_recon": "0.174", "loss_info_nce": "8.235", "ppl": "1.02", "wps": "1037.3", "ups": "0.35", "wpb": "2955.5", "bsz": "385.4", "num_updates": "2600", "lr": "6.5e-05", "gnorm": "9.852", "clip": "38", "loss_scale": "128", "train_wall": "507", "gb_free": "2.9", "wall": "4929"}
[2025-07-04 15:13:41,991][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 11 @ 2644 updates
[2025-07-04 15:13:42,006][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint11.pt
[2025-07-04 15:13:43,613][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint11.pt
[2025-07-04 15:13:44,493][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint11.pt (epoch 11 @ 2644 updates, score None) (writing took 2.4934009999997215 seconds)
[2025-07-04 15:13:44,493][fairseq_cli.train][INFO] - end of epoch 11 (average epoch stats below)
[2025-07-04 15:13:44,503][train][INFO] - {"epoch": 11, "train_loss": "9.981", "train_nll_loss": "0.027", "train_loss_recon": "0.174", "train_loss_info_nce": "8.241", "train_ppl": "1.02", "train_wps": "1148.1", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "2644", "train_lr": "6.61e-05", "train_gnorm": "11.136", "train_clip": "44.6", "train_loss_scale": "128", "train_train_wall": "613", "train_gb_free": "2.9", "train_wall": "5041"}
[2025-07-04 15:13:44,805][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 15:13:44,805][fairseq.trainer][INFO] - begin training epoch 12
[2025-07-04 15:13:44,815][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 15:20:02,904][train_inner][INFO] - {"epoch": 12, "update": 11.645, "loss": "9.908", "nll_loss": "0.027", "loss_recon": "0.171", "loss_info_nce": "8.199", "ppl": "1.02", "wps": "1204.8", "ups": "0.41", "wpb": "2954.6", "bsz": "385.9", "num_updates": "2800", "lr": "7e-05", "gnorm": "8.463", "clip": "29.5", "loss_scale": "256", "train_wall": "482", "gb_free": "2.9", "wall": "5419"}
[2025-07-04 15:23:05,462][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 12 @ 2886 updates
[2025-07-04 15:23:05,462][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint12.pt
[2025-07-04 15:23:06,794][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint12.pt
[2025-07-04 15:23:07,291][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint12.pt (epoch 12 @ 2886 updates, score None) (writing took 1.8238901000004262 seconds)
[2025-07-04 15:23:07,291][fairseq_cli.train][INFO] - end of epoch 12 (average epoch stats below)
[2025-07-04 15:23:07,291][train][INFO] - {"epoch": 12, "train_loss": "9.852", "train_nll_loss": "0.027", "train_loss_recon": "0.169", "train_loss_info_nce": "8.166", "train_ppl": "1.02", "train_wps": "1271.4", "train_ups": "0.43", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "2886", "train_lr": "7.215e-05", "train_gnorm": "6.346", "train_clip": "16.5", "train_loss_scale": "256", "train_train_wall": "555", "train_gb_free": "2.9", "train_wall": "5603"}
[2025-07-04 15:23:07,505][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 15:23:07,515][fairseq.trainer][INFO] - begin training epoch 13
[2025-07-04 15:23:07,515][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 16:04:09,829][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'tblog', 'wandb_project': None, 'azureml_logging': False, 'seed': 1337, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': 'C:\\Users\\Ali\\OneDrive - Georgia Institute of Technology\\25-5 Summer\\CS 7643 - Deep Learning\\_Project\\MAE-AST-Public\\mae_ast', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 6, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 1048576, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 5, 'validate_interval_updates': 10000, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1048576, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [8], 'lr': [0.0001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 25000, 'keep_interval_updates': 1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'mae_ast', 'ast_kernel_size_chan': 16, 'ast_kernel_size_time': 16, 'ast_kernel_stride_chan': 16, 'ast_kernel_stride_time': 16, 'encoder_layers': 2, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_norm_first': False, 'feature_grad_mult': 0.1, 'use_post_enc_proj': False, 'decoder_embed_dim': 768, 'decoder_layers': 1, 'decoder_layerdrop': 0.0, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'random_mask_prob': 0.75, 'mask_length': 10, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'checkpoint_activations': False, 'max_token_length': 48000, 'enc_sine_pos': True, 'enc_conv_pos': False, 'dec_sine_pos': True, 'dec_conv_pos': False}, 'task': {'_name': 'mae_ast_pretraining', 'data': 'D:\\MBARI 2KHz\\training\\input_dir', 'sample_rate': 2000, 'normalize': True, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 40000, 'min_sample_size': 5000, 'random_crop': True, 'pad_audio': False, 'feature_type': 'fbank', 'feature_rate': 100, 'feature_dim': 128, 'deltas': False, 'mask_spans': False, 'mask_type': random_mask}, 'criterion': {'_name': 'mae_ast', 'reconstruction_weight': 10.0, 'classification_weight': 1.0}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0001]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 4000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000.0, 'lr': [0.0001]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2025-07-04 16:04:09,845][mae_ast.tasks.mae_ast_pretraining][INFO] - current directory is D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model
[2025-07-04 16:04:09,845][mae_ast.tasks.mae_ast_pretraining][INFO] - MAEPretrainingTask Config {'_name': 'mae_ast_pretraining', 'data': 'D:\\MBARI 2KHz\\training\\input_dir', 'sample_rate': 2000, 'normalize': True, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 40000, 'min_sample_size': 5000, 'random_crop': True, 'pad_audio': False, 'feature_type': 'fbank', 'feature_rate': 100, 'feature_dim': 128, 'deltas': False, 'mask_spans': False, 'mask_type': random_mask}
[2025-07-04 16:04:09,845][mae_ast.models.mae_ast][INFO] - MAEModel Config: {'_name': 'mae_ast', 'ast_kernel_size_chan': 16, 'ast_kernel_size_time': 16, 'ast_kernel_stride_chan': 16, 'ast_kernel_stride_time': 16, 'encoder_layers': 2, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_norm_first': False, 'feature_grad_mult': 0.1, 'use_post_enc_proj': False, 'decoder_embed_dim': 768, 'decoder_layers': 1, 'decoder_layerdrop': 0.0, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'random_mask_prob': 0.75, 'mask_length': 10, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'checkpoint_activations': False, 'max_token_length': 48000, 'enc_sine_pos': True, 'enc_conv_pos': False, 'dec_sine_pos': True, 'dec_conv_pos': False}
[2025-07-04 16:04:10,182][fairseq_cli.train][INFO] - MAE_AST(
  (feature_extractor): Identity()
  (post_extract_proj): Linear(in_features=256, out_features=768, bias=True)
  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (batch_norm): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  (unfold): Unfold(kernel_size=(16, 16), dilation=1, padding=0, stride=(16, 16))
  (dropout_input): Dropout(p=0.1, inplace=False)
  (enc_sine_pos_embed): SinusoidalPositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (dec_sine_pos_embed): SinusoidalPositionalEncoding()
  (decoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (final_proj_reconstruction): Linear(in_features=768, out_features=256, bias=True)
  (final_proj_classification): Linear(in_features=768, out_features=256, bias=True)
)
[2025-07-04 16:04:10,182][fairseq_cli.train][INFO] - task: MAE_AST_Pretraining_Task
[2025-07-04 16:04:10,182][fairseq_cli.train][INFO] - model: MAE_AST
[2025-07-04 16:04:10,182][fairseq_cli.train][INFO] - criterion: MAE_AST_Criterion
[2025-07-04 16:04:10,182][fairseq_cli.train][INFO] - num. shared model params: 21,859,584 (num. trained: 21,859,584)
[2025-07-04 16:04:10,182][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2025-07-04 16:04:10,379][mae_ast.data.mae_ast_dataset][INFO] - max_keep=None, min_keep=5000, loaded 10374, skipped 0 short and 0 long, longest-loaded=20000, shortest-loaded=10000
[2025-07-04 16:04:10,379][mae_ast.data.mae_ast_dataset][INFO] - pad_audio=False, random_crop=True, normalize=True, max_sample_size=40000
[2025-07-04 16:04:10,591][fairseq.trainer][INFO] - detected shared parameter: batch_norm.weight <- batch_norm.bias
[2025-07-04 16:04:10,591][fairseq.utils][INFO] - ***********************CUDA enviroments for all 1 workers***********************
[2025-07-04 16:04:10,591][fairseq.utils][INFO] - rank   0: capabilities =  7.5  ; total memory = 6.000 GB ; name = NVIDIA GeForce RTX 2060                 
[2025-07-04 16:04:10,591][fairseq.utils][INFO] - ***********************CUDA enviroments for all 1 workers***********************
[2025-07-04 16:04:10,591][fairseq_cli.train][INFO] - training on 1 devices (GPUs/TPUs)
[2025-07-04 16:04:10,591][fairseq_cli.train][INFO] - max tokens per device = 1048576 and max sentences per device = None
[2025-07-04 16:04:10,648][fairseq.trainer][INFO] - Preparing to load checkpoint checkpoints\checkpoint_last.pt
[2025-07-04 16:04:13,124][fairseq.trainer][INFO] - Loaded checkpoint checkpoints\checkpoint_last.pt (epoch 13 @ 2886 updates)
[2025-07-04 16:04:13,124][fairseq.trainer][INFO] - loading train data for epoch 13
[2025-07-04 16:04:13,165][mae_ast.data.mae_ast_dataset][INFO] - max_keep=None, min_keep=5000, loaded 93367, skipped 0 short and 0 long, longest-loaded=20000, shortest-loaded=10000
[2025-07-04 16:04:13,181][mae_ast.data.mae_ast_dataset][INFO] - pad_audio=False, random_crop=True, normalize=True, max_sample_size=40000
[2025-07-04 16:04:13,729][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 16:04:13,729][fairseq.trainer][INFO] - begin training epoch 13
[2025-07-04 16:04:13,729][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 16:08:29,928][train_inner][INFO] - {"epoch": 13, "update": 12.471, "loss": "9.736", "nll_loss": "0.026", "loss_recon": "0.163", "loss_info_nce": "8.102", "ppl": "1.02", "wps": "1424.7", "ups": "0.48", "wpb": "2962.9", "bsz": "387.4", "num_updates": "3000", "lr": "7.5e-05", "gnorm": "7.388", "clip": "28.1", "loss_scale": "256", "train_wall": "238", "gb_free": "2.9", "wall": "257"}
[2025-07-04 16:12:53,317][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 13 @ 3128 updates
[2025-07-04 16:12:53,317][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint13.pt
[2025-07-04 16:12:54,715][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint13.pt
[2025-07-04 16:12:55,162][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint13.pt (epoch 13 @ 3128 updates, score None) (writing took 1.8509970000000067 seconds)
[2025-07-04 16:12:55,162][fairseq_cli.train][INFO] - end of epoch 13 (average epoch stats below)
[2025-07-04 16:12:55,177][train][INFO] - {"epoch": 13, "train_loss": "9.699", "train_nll_loss": "0.026", "train_loss_recon": "0.162", "train_loss_info_nce": "8.076", "train_ppl": "1.02", "train_wps": "1417.2", "train_ups": "0.48", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "3128", "train_lr": "7.82e-05", "train_gnorm": "7.09", "train_clip": "23.1", "train_loss_scale": "256", "train_train_wall": "501", "train_gb_free": "2.9", "train_wall": "525"}
[2025-07-04 16:12:55,360][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 16:12:55,360][fairseq.trainer][INFO] - begin training epoch 14
[2025-07-04 16:12:55,360][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 16:15:26,938][train_inner][INFO] - {"epoch": 14, "update": 13.298, "loss": "9.626", "nll_loss": "0.026", "loss_recon": "0.159", "loss_info_nce": "8.031", "ppl": "1.02", "wps": "1418", "ups": "0.48", "wpb": "2956.5", "bsz": "385.1", "num_updates": "3200", "lr": "8e-05", "gnorm": "6.23", "clip": "15.5", "loss_scale": "256", "train_wall": "412", "gb_free": "2.9", "wall": "676"}
[2025-07-04 16:21:21,439][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 14 @ 3370 updates
[2025-07-04 16:21:21,439][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint14.pt
[2025-07-04 16:21:22,962][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint14.pt
[2025-07-04 16:21:23,429][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint14.pt (epoch 14 @ 3370 updates, score None) (writing took 2.00058550000017 seconds)
[2025-07-04 16:21:23,429][fairseq_cli.train][INFO] - end of epoch 14 (average epoch stats below)
[2025-07-04 16:21:23,445][train][INFO] - {"epoch": 14, "train_loss": "9.509", "train_nll_loss": "0.026", "train_loss_recon": "0.155", "train_loss_info_nce": "7.956", "train_ppl": "1.02", "train_wps": "1407.8", "train_ups": "0.48", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "3370", "train_lr": "8.425e-05", "train_gnorm": "6.12", "train_clip": "14", "train_loss_scale": "256", "train_train_wall": "503", "train_gb_free": "2.9", "train_wall": "1033"}
[2025-07-04 16:21:23,612][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 16:21:23,612][fairseq.trainer][INFO] - begin training epoch 15
[2025-07-04 16:21:23,612][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 16:22:28,063][train_inner][INFO] - {"epoch": 15, "update": 14.124, "loss": "9.48", "nll_loss": "0.026", "loss_recon": "0.154", "loss_info_nce": "7.935", "ppl": "1.02", "wps": "1403.6", "ups": "0.47", "wpb": "2955.5", "bsz": "385.6", "num_updates": "3400", "lr": "8.5e-05", "gnorm": "6.355", "clip": "14", "loss_scale": "256", "train_wall": "416", "gb_free": "2.9", "wall": "1097"}
[2025-07-04 16:29:27,483][train_inner][INFO] - {"epoch": 15, "update": 14.95, "loss": "9.348", "nll_loss": "0.025", "loss_recon": "0.15", "loss_info_nce": "7.849", "ppl": "1.02", "wps": "1412.9", "ups": "0.48", "wpb": "2963", "bsz": "387.1", "num_updates": "3600", "lr": "9e-05", "gnorm": "6.28", "clip": "15.5", "loss_scale": "256", "train_wall": "418", "gb_free": "2.9", "wall": "1517"}
[2025-07-04 17:16:17,887][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-04 17:17:03,387][valid][INFO] - {"epoch": 15, "valid_loss": "8.976", "valid_nll_loss": "0.024", "valid_loss_recon": "0.138", "valid_loss_info_nce": "7.596", "valid_ppl": "1.02", "valid_wps": "2698.6", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "3612", "valid_best_loss": "8.976"}
[2025-07-04 17:17:03,387][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 15 @ 3612 updates
[2025-07-04 17:17:03,387][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint15.pt
[2025-07-04 17:17:04,824][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint15.pt
[2025-07-04 17:17:05,669][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint15.pt (epoch 15 @ 3612 updates, score 8.976) (writing took 2.285000900000341 seconds)
[2025-07-04 17:17:05,669][fairseq_cli.train][INFO] - end of epoch 15 (average epoch stats below)
[2025-07-04 17:17:05,685][train][INFO] - {"epoch": 15, "train_loss": "9.356", "train_nll_loss": "0.025", "train_loss_recon": "0.15", "train_loss_info_nce": "7.854", "train_ppl": "1.02", "train_wps": "214.1", "train_ups": "0.07", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "3612", "train_lr": "9.03e-05", "train_gnorm": "6.123", "train_clip": "13.6", "train_loss_scale": "256", "train_train_wall": "505", "train_gb_free": "2.9", "train_wall": "4375"}
[2025-07-04 17:17:05,909][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 17:17:05,925][fairseq.trainer][INFO] - begin training epoch 16
[2025-07-04 17:17:05,925][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 17:23:10,103][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'tblog', 'wandb_project': None, 'azureml_logging': False, 'seed': 1337, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': 'C:\\Users\\Ali\\OneDrive - Georgia Institute of Technology\\25-5 Summer\\CS 7643 - Deep Learning\\_Project\\MAE-AST-Public\\mae_ast', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 6, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 1048576, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 5, 'validate_interval_updates': 10000, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1048576, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [8], 'lr': [0.0001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 25000, 'keep_interval_updates': 1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'mae_ast', 'ast_kernel_size_chan': 16, 'ast_kernel_size_time': 16, 'ast_kernel_stride_chan': 16, 'ast_kernel_stride_time': 16, 'encoder_layers': 2, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_norm_first': False, 'feature_grad_mult': 0.1, 'use_post_enc_proj': False, 'decoder_embed_dim': 768, 'decoder_layers': 1, 'decoder_layerdrop': 0.0, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'random_mask_prob': 0.75, 'mask_length': 10, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'checkpoint_activations': False, 'max_token_length': 48000, 'enc_sine_pos': True, 'enc_conv_pos': False, 'dec_sine_pos': True, 'dec_conv_pos': False}, 'task': {'_name': 'mae_ast_pretraining', 'data': 'D:\\MBARI 2KHz\\training\\input_dir', 'sample_rate': 2000, 'normalize': True, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 40000, 'min_sample_size': 5000, 'random_crop': True, 'pad_audio': False, 'feature_type': 'fbank', 'feature_rate': 100, 'feature_dim': 128, 'deltas': False, 'mask_spans': False, 'mask_type': random_mask}, 'criterion': {'_name': 'mae_ast', 'reconstruction_weight': 10.0, 'classification_weight': 1.0}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0001]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 4000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000.0, 'lr': [0.0001]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2025-07-04 17:23:10,103][mae_ast.tasks.mae_ast_pretraining][INFO] - current directory is D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model
[2025-07-04 17:23:10,103][mae_ast.tasks.mae_ast_pretraining][INFO] - MAEPretrainingTask Config {'_name': 'mae_ast_pretraining', 'data': 'D:\\MBARI 2KHz\\training\\input_dir', 'sample_rate': 2000, 'normalize': True, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 40000, 'min_sample_size': 5000, 'random_crop': True, 'pad_audio': False, 'feature_type': 'fbank', 'feature_rate': 100, 'feature_dim': 128, 'deltas': False, 'mask_spans': False, 'mask_type': random_mask}
[2025-07-04 17:23:10,113][mae_ast.models.mae_ast][INFO] - MAEModel Config: {'_name': 'mae_ast', 'ast_kernel_size_chan': 16, 'ast_kernel_size_time': 16, 'ast_kernel_stride_chan': 16, 'ast_kernel_stride_time': 16, 'encoder_layers': 2, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_norm_first': False, 'feature_grad_mult': 0.1, 'use_post_enc_proj': False, 'decoder_embed_dim': 768, 'decoder_layers': 1, 'decoder_layerdrop': 0.0, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'random_mask_prob': 0.75, 'mask_length': 10, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'checkpoint_activations': False, 'max_token_length': 48000, 'enc_sine_pos': True, 'enc_conv_pos': False, 'dec_sine_pos': True, 'dec_conv_pos': False}
[2025-07-04 17:23:10,441][fairseq_cli.train][INFO] - MAE_AST(
  (feature_extractor): Identity()
  (post_extract_proj): Linear(in_features=256, out_features=768, bias=True)
  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (batch_norm): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  (unfold): Unfold(kernel_size=(16, 16), dilation=1, padding=0, stride=(16, 16))
  (dropout_input): Dropout(p=0.1, inplace=False)
  (enc_sine_pos_embed): SinusoidalPositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (dec_sine_pos_embed): SinusoidalPositionalEncoding()
  (decoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (final_proj_reconstruction): Linear(in_features=768, out_features=256, bias=True)
  (final_proj_classification): Linear(in_features=768, out_features=256, bias=True)
)
[2025-07-04 17:23:10,441][fairseq_cli.train][INFO] - task: MAE_AST_Pretraining_Task
[2025-07-04 17:23:10,441][fairseq_cli.train][INFO] - model: MAE_AST
[2025-07-04 17:23:10,441][fairseq_cli.train][INFO] - criterion: MAE_AST_Criterion
[2025-07-04 17:23:10,441][fairseq_cli.train][INFO] - num. shared model params: 21,859,584 (num. trained: 21,859,584)
[2025-07-04 17:23:10,441][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2025-07-04 17:23:11,326][mae_ast.data.mae_ast_dataset][INFO] - max_keep=None, min_keep=5000, loaded 10374, skipped 0 short and 0 long, longest-loaded=20000, shortest-loaded=10000
[2025-07-04 17:23:11,326][mae_ast.data.mae_ast_dataset][INFO] - pad_audio=False, random_crop=True, normalize=True, max_sample_size=40000
[2025-07-04 17:23:11,547][fairseq.trainer][INFO] - detected shared parameter: batch_norm.weight <- batch_norm.bias
[2025-07-04 17:23:11,547][fairseq.utils][INFO] - ***********************CUDA enviroments for all 1 workers***********************
[2025-07-04 17:23:11,547][fairseq.utils][INFO] - rank   0: capabilities =  7.5  ; total memory = 6.000 GB ; name = NVIDIA GeForce RTX 2060                 
[2025-07-04 17:23:11,547][fairseq.utils][INFO] - ***********************CUDA enviroments for all 1 workers***********************
[2025-07-04 17:23:11,547][fairseq_cli.train][INFO] - training on 1 devices (GPUs/TPUs)
[2025-07-04 17:23:11,547][fairseq_cli.train][INFO] - max tokens per device = 1048576 and max sentences per device = None
[2025-07-04 17:23:11,547][fairseq.trainer][INFO] - Preparing to load checkpoint checkpoints\checkpoint_last.pt
[2025-07-04 17:23:14,013][fairseq.trainer][INFO] - Loaded checkpoint checkpoints\checkpoint_last.pt (epoch 16 @ 3612 updates)
[2025-07-04 17:23:14,013][fairseq.trainer][INFO] - loading train data for epoch 16
[2025-07-04 17:23:14,070][mae_ast.data.mae_ast_dataset][INFO] - max_keep=None, min_keep=5000, loaded 93367, skipped 0 short and 0 long, longest-loaded=20000, shortest-loaded=10000
[2025-07-04 17:23:14,070][mae_ast.data.mae_ast_dataset][INFO] - pad_audio=False, random_crop=True, normalize=True, max_sample_size=40000
[2025-07-04 17:23:14,521][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 17:23:14,521][fairseq.trainer][INFO] - begin training epoch 16
[2025-07-04 17:23:14,521][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 17:30:06,091][train_inner][INFO] - {"epoch": 16, "update": 15.777, "loss": "9.216", "nll_loss": "0.025", "loss_recon": "0.146", "loss_info_nce": "7.756", "ppl": "1.02", "wps": "1429.4", "ups": "0.48", "wpb": "2965.1", "bsz": "386.9", "num_updates": "3800", "lr": "9.5e-05", "gnorm": "5.339", "clip": "6.9", "loss_scale": "256", "train_wall": "391", "gb_free": "2.9", "wall": "410"}
[2025-07-04 17:31:56,363][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 16 @ 3854 updates
[2025-07-04 17:31:56,363][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint16.pt
[2025-07-04 17:31:57,692][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint16.pt
[2025-07-04 17:31:58,112][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint16.pt (epoch 16 @ 3854 updates, score None) (writing took 1.7561964999999873 seconds)
[2025-07-04 17:31:58,112][fairseq_cli.train][INFO] - end of epoch 16 (average epoch stats below)
[2025-07-04 17:31:58,128][train][INFO] - {"epoch": 16, "train_loss": "9.201", "train_nll_loss": "0.025", "train_loss_recon": "0.146", "train_loss_info_nce": "7.745", "train_ppl": "1.02", "train_wps": "1411.7", "train_ups": "0.48", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "3854", "train_lr": "9.635e-05", "train_gnorm": "5.162", "train_clip": "5.8", "train_loss_scale": "256", "train_train_wall": "501", "train_gb_free": "2.9", "train_wall": "527"}
[2025-07-04 17:31:58,299][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 17:31:58,299][fairseq.trainer][INFO] - begin training epoch 17
[2025-07-04 17:31:58,299][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 17:37:04,565][train_inner][INFO] - {"epoch": 17, "update": 16.603, "loss": "9.089", "nll_loss": "0.025", "loss_recon": "0.143", "loss_info_nce": "7.662", "ppl": "1.02", "wps": "1411.2", "ups": "0.48", "wpb": "2952.8", "bsz": "385.7", "num_updates": "4000", "lr": "0.0001", "gnorm": "4.839", "clip": "5.5", "loss_scale": "256", "train_wall": "414", "gb_free": "2.9", "wall": "833"}
[2025-07-04 17:40:22,447][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 17 @ 4096 updates
[2025-07-04 17:40:22,447][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint17.pt
[2025-07-04 17:40:23,835][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint17.pt
[2025-07-04 17:40:24,293][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint17.pt (epoch 17 @ 4096 updates, score None) (writing took 1.8439183000000412 seconds)
[2025-07-04 17:40:24,293][fairseq_cli.train][INFO] - end of epoch 17 (average epoch stats below)
[2025-07-04 17:40:24,293][train][INFO] - {"epoch": 17, "train_loss": "9.033", "train_nll_loss": "0.024", "train_loss_recon": "0.141", "train_loss_info_nce": "7.62", "train_ppl": "1.02", "train_wps": "1413.7", "train_ups": "0.48", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "4096", "train_lr": "9.97913e-05", "train_gnorm": "4.846", "train_clip": "4.1", "train_loss_scale": "256", "train_train_wall": "501", "train_gb_free": "2.9", "train_wall": "1033"}
[2025-07-04 17:40:24,475][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 17:40:24,475][fairseq.trainer][INFO] - begin training epoch 18
[2025-07-04 17:40:24,475][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 17:44:02,851][train_inner][INFO] - {"epoch": 18, "update": 17.43, "loss": "8.957", "nll_loss": "0.024", "loss_recon": "0.14", "loss_info_nce": "7.557", "ppl": "1.02", "wps": "1413.6", "ups": "0.48", "wpb": "2956.5", "bsz": "385.4", "num_updates": "4200", "lr": "9.95652e-05", "gnorm": "5.321", "clip": "5", "loss_scale": "256", "train_wall": "413", "gb_free": "2.9", "wall": "1251"}
[2025-07-04 17:48:48,799][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 18 @ 4338 updates
[2025-07-04 17:48:48,799][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint18.pt
[2025-07-04 17:48:54,547][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint18.pt
[2025-07-04 17:48:55,002][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint18.pt (epoch 18 @ 4338 updates, score None) (writing took 6.200561099999959 seconds)
[2025-07-04 17:48:55,002][fairseq_cli.train][INFO] - end of epoch 18 (average epoch stats below)
[2025-07-04 17:48:55,018][train][INFO] - {"epoch": 18, "train_loss": "8.886", "train_nll_loss": "0.024", "train_loss_recon": "0.138", "train_loss_info_nce": "7.504", "train_ppl": "1.02", "train_wps": "1401.1", "train_ups": "0.47", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "4338", "train_lr": "9.92652e-05", "train_gnorm": "5.188", "train_clip": "5", "train_loss_scale": "256", "train_train_wall": "502", "train_gb_free": "2.9", "train_wall": "1543"}
[2025-07-04 17:48:55,225][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 17:48:55,225][fairseq.trainer][INFO] - begin training epoch 19
[2025-07-04 17:48:55,225][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 17:51:06,472][train_inner][INFO] - {"epoch": 19, "update": 18.256, "loss": "8.842", "nll_loss": "0.024", "loss_recon": "0.137", "loss_info_nce": "7.471", "ppl": "1.02", "wps": "1395.4", "ups": "0.47", "wpb": "2955.5", "bsz": "385.9", "num_updates": "4400", "lr": "9.91304e-05", "gnorm": "5.041", "clip": "5.5", "loss_scale": "256", "train_wall": "414", "gb_free": "2.9", "wall": "1675"}
[2025-07-04 17:57:19,533][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 19 @ 4580 updates
[2025-07-04 17:57:19,533][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint19.pt
[2025-07-04 17:57:27,175][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint19.pt
[2025-07-04 17:57:27,612][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint19.pt (epoch 19 @ 4580 updates, score None) (writing took 8.076618399999916 seconds)
[2025-07-04 17:57:27,612][fairseq_cli.train][INFO] - end of epoch 19 (average epoch stats below)
[2025-07-04 17:57:27,612][train][INFO] - {"epoch": 19, "train_loss": "8.768", "train_nll_loss": "0.024", "train_loss_recon": "0.136", "train_loss_info_nce": "7.404", "train_ppl": "1.02", "train_wps": "1395.9", "train_ups": "0.47", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "4580", "train_lr": "9.87391e-05", "train_gnorm": "5.241", "train_clip": "9.5", "train_loss_scale": "256", "train_train_wall": "501", "train_gb_free": "2.9", "train_wall": "2056"}
[2025-07-04 17:57:27,783][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 17:57:27,783][fairseq.trainer][INFO] - begin training epoch 20
[2025-07-04 17:57:27,783][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 17:58:11,320][train_inner][INFO] - {"epoch": 20, "update": 19.083, "loss": "8.741", "nll_loss": "0.024", "loss_recon": "0.136", "loss_info_nce": "7.383", "ppl": "1.02", "wps": "1390.9", "ups": "0.47", "wpb": "2954.6", "bsz": "385.6", "num_updates": "4600", "lr": "9.86957e-05", "gnorm": "4.911", "clip": "7", "loss_scale": "256", "train_wall": "414", "gb_free": "2.9", "wall": "2100"}
[2025-07-04 18:06:16,900][train_inner][INFO] - {"epoch": 20, "update": 19.909, "loss": "8.607", "nll_loss": "0.023", "loss_recon": "0.136", "loss_info_nce": "7.246", "ppl": "1.02", "wps": "1221.2", "ups": "0.41", "wpb": "2964.8", "bsz": "386.6", "num_updates": "4800", "lr": "9.82609e-05", "gnorm": "4.976", "clip": "6.5", "loss_scale": "256", "train_wall": "481", "gb_free": "2.9", "wall": "2585"}
[2025-07-04 18:07:09,516][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-04 18:08:09,546][valid][INFO] - {"epoch": 20, "valid_loss": "7.865", "valid_nll_loss": "0.021", "valid_loss_recon": "0.123", "valid_loss_info_nce": "6.638", "valid_ppl": "1.01", "valid_wps": "1712.8", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "4822", "valid_best_loss": "7.865"}
[2025-07-04 18:08:09,546][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 20 @ 4822 updates
[2025-07-04 18:08:09,546][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint20.pt
[2025-07-04 18:08:11,112][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint20.pt
[2025-07-04 18:08:12,618][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint20.pt (epoch 20 @ 4822 updates, score 7.865) (writing took 3.0754314000000704 seconds)
[2025-07-04 18:08:12,618][fairseq_cli.train][INFO] - end of epoch 20 (average epoch stats below)
[2025-07-04 18:08:12,634][train][INFO] - {"epoch": 20, "train_loss": "8.599", "train_nll_loss": "0.023", "train_loss_recon": "0.136", "train_loss_info_nce": "7.241", "train_ppl": "1.02", "train_wps": "1109.4", "train_ups": "0.38", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "4822", "train_lr": "9.8213e-05", "train_gnorm": "4.807", "train_clip": "5.4", "train_loss_scale": "256", "train_train_wall": "576", "train_gb_free": "2.9", "train_wall": "2701"}
[2025-07-04 18:08:13,030][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 18:08:13,030][fairseq.trainer][INFO] - begin training epoch 21
[2025-07-04 18:08:13,030][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 18:15:43,859][train_inner][INFO] - {"epoch": 21, "update": 20.736, "loss": "8.272", "nll_loss": "0.022", "loss_recon": "0.133", "loss_info_nce": "6.941", "ppl": "1.02", "wps": "1043.3", "ups": "0.35", "wpb": "2957.4", "bsz": "385.1", "num_updates": "5000", "lr": "9.78261e-05", "gnorm": "4.787", "clip": "6", "loss_scale": "256", "train_wall": "496", "gb_free": "2.9", "wall": "3152"}
[2025-07-04 18:18:22,247][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 21 @ 5064 updates
[2025-07-04 18:18:22,263][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint21.pt
[2025-07-04 18:18:23,782][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint21.pt
[2025-07-04 18:18:24,532][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint21.pt (epoch 21 @ 5064 updates, score None) (writing took 2.274287799999911 seconds)
[2025-07-04 18:18:24,532][fairseq_cli.train][INFO] - end of epoch 21 (average epoch stats below)
[2025-07-04 18:18:24,532][train][INFO] - {"epoch": 21, "train_loss": "8.191", "train_nll_loss": "0.022", "train_loss_recon": "0.133", "train_loss_info_nce": "6.864", "train_ppl": "1.02", "train_wps": "1169.4", "train_ups": "0.4", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "5064", "train_lr": "9.7687e-05", "train_gnorm": "4.979", "train_clip": "7.9", "train_loss_scale": "256", "train_train_wall": "600", "train_gb_free": "2.9", "train_wall": "3313"}
[2025-07-04 18:18:24,901][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 18:18:24,917][fairseq.trainer][INFO] - begin training epoch 22
[2025-07-04 18:18:24,917][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 18:24:11,359][train_inner][INFO] - {"epoch": 22, "update": 21.562, "loss": "7.943", "nll_loss": "0.021", "loss_recon": "0.13", "loss_info_nce": "6.638", "ppl": "1.01", "wps": "1162.9", "ups": "0.39", "wpb": "2950.9", "bsz": "386.4", "num_updates": "5200", "lr": "9.73913e-05", "gnorm": "4.258", "clip": "4", "loss_scale": "256", "train_wall": "498", "gb_free": "2.9", "wall": "3660"}
[2025-07-04 18:28:35,581][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 22 @ 5306 updates
[2025-07-04 18:28:35,581][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint22.pt
[2025-07-04 18:28:37,048][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint22.pt
[2025-07-04 18:28:37,798][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint22.pt (epoch 22 @ 5306 updates, score None) (writing took 2.213439600000129 seconds)
[2025-07-04 18:28:37,798][fairseq_cli.train][INFO] - end of epoch 22 (average epoch stats below)
[2025-07-04 18:28:37,798][train][INFO] - {"epoch": 22, "train_loss": "7.861", "train_nll_loss": "0.021", "train_loss_recon": "0.13", "train_loss_info_nce": "6.56", "train_ppl": "1.01", "train_wps": "1166.8", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "5306", "train_lr": "9.71609e-05", "train_gnorm": "4.158", "train_clip": "2.1", "train_loss_scale": "256", "train_train_wall": "603", "train_gb_free": "2.9", "train_wall": "3926"}
[2025-07-04 18:28:38,121][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 18:28:38,121][fairseq.trainer][INFO] - begin training epoch 23
[2025-07-04 18:28:38,121][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 18:32:37,201][train_inner][INFO] - {"epoch": 23, "update": 22.388, "loss": "7.76", "nll_loss": "0.021", "loss_recon": "0.129", "loss_info_nce": "6.47", "ppl": "1.01", "wps": "1170", "ups": "0.4", "wpb": "2959.3", "bsz": "384.3", "num_updates": "5400", "lr": "9.69565e-05", "gnorm": "4.32", "clip": "2", "loss_scale": "256", "train_wall": "497", "gb_free": "2.9", "wall": "4166"}
[2025-07-04 18:38:47,200][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 23 @ 5548 updates
[2025-07-04 18:38:47,200][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint23.pt
[2025-07-04 18:38:48,695][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint23.pt
[2025-07-04 18:38:49,412][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint23.pt (epoch 23 @ 5548 updates, score None) (writing took 2.21144060000006 seconds)
[2025-07-04 18:38:49,412][fairseq_cli.train][INFO] - end of epoch 23 (average epoch stats below)
[2025-07-04 18:38:49,412][train][INFO] - {"epoch": 23, "train_loss": "7.646", "train_nll_loss": "0.021", "train_loss_recon": "0.128", "train_loss_info_nce": "6.365", "train_ppl": "1.01", "train_wps": "1169.9", "train_ups": "0.4", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "5548", "train_lr": "9.66348e-05", "train_gnorm": "3.755", "train_clip": "1.2", "train_loss_scale": "256", "train_train_wall": "602", "train_gb_free": "2.9", "train_wall": "4538"}
[2025-07-04 18:38:49,796][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 18:38:49,796][fairseq.trainer][INFO] - begin training epoch 24
[2025-07-04 18:38:49,796][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 18:41:03,713][train_inner][INFO] - {"epoch": 24, "update": 23.215, "loss": "7.585", "nll_loss": "0.02", "loss_recon": "0.128", "loss_info_nce": "6.308", "ppl": "1.01", "wps": "1165.6", "ups": "0.39", "wpb": "2951.8", "bsz": "387.2", "num_updates": "5600", "lr": "9.65217e-05", "gnorm": "4.133", "clip": "3", "loss_scale": "256", "train_wall": "498", "gb_free": "2.9", "wall": "4672"}
[2025-07-04 18:48:54,914][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 24 @ 5790 updates
[2025-07-04 18:48:54,914][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint24.pt
[2025-07-04 18:48:56,357][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint24.pt
[2025-07-04 18:48:57,068][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint24.pt (epoch 24 @ 5790 updates, score None) (writing took 2.1480468999998266 seconds)
[2025-07-04 18:48:57,068][fairseq_cli.train][INFO] - end of epoch 24 (average epoch stats below)
[2025-07-04 18:48:57,068][train][INFO] - {"epoch": 24, "train_loss": "7.432", "train_nll_loss": "0.02", "train_loss_recon": "0.126", "train_loss_info_nce": "6.169", "train_ppl": "1.01", "train_wps": "1177.6", "train_ups": "0.4", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "5790", "train_lr": "9.61087e-05", "train_gnorm": "3.805", "train_clip": "2.9", "train_loss_scale": "512", "train_train_wall": "597", "train_gb_free": "2.9", "train_wall": "5146"}
[2025-07-04 18:48:57,462][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 18:48:57,462][fairseq.trainer][INFO] - begin training epoch 25
[2025-07-04 18:48:57,462][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 18:49:26,023][train_inner][INFO] - {"epoch": 25, "update": 24.041, "loss": "7.398", "nll_loss": "0.02", "loss_recon": "0.126", "loss_info_nce": "6.138", "ppl": "1.01", "wps": "1177.5", "ups": "0.4", "wpb": "2957.4", "bsz": "384.8", "num_updates": "5800", "lr": "9.6087e-05", "gnorm": "3.257", "clip": "2", "loss_scale": "512", "train_wall": "493", "gb_free": "2.9", "wall": "5174"}
[2025-07-04 18:57:45,533][train_inner][INFO] - {"epoch": 25, "update": 24.868, "loss": "7.203", "nll_loss": "0.019", "loss_recon": "0.124", "loss_info_nce": "5.96", "ppl": "1.01", "wps": "1186.7", "ups": "0.4", "wpb": "2963.9", "bsz": "386.8", "num_updates": "6000", "lr": "9.56522e-05", "gnorm": "3.302", "clip": "0", "loss_scale": "512", "train_wall": "496", "gb_free": "2.9", "wall": "5674"}
[2025-07-04 18:59:02,842][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-04 18:59:47,721][valid][INFO] - {"epoch": 25, "valid_loss": "6.385", "valid_nll_loss": "0.017", "valid_loss_recon": "0.114", "valid_loss_info_nce": "5.244", "valid_ppl": "1.01", "valid_wps": "1828.7", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "6032", "valid_best_loss": "6.385"}
[2025-07-04 18:59:47,721][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 25 @ 6032 updates
[2025-07-04 18:59:47,721][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint25.pt
[2025-07-04 18:59:49,232][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint25.pt
[2025-07-04 18:59:50,666][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint25.pt (epoch 25 @ 6032 updates, score 6.385) (writing took 2.9334457000004477 seconds)
[2025-07-04 18:59:50,667][fairseq_cli.train][INFO] - end of epoch 25 (average epoch stats below)
[2025-07-04 18:59:50,667][train][INFO] - {"epoch": 25, "train_loss": "7.197", "train_nll_loss": "0.019", "train_loss_recon": "0.124", "train_loss_info_nce": "5.954", "train_ppl": "1.01", "train_wps": "1094.8", "train_ups": "0.37", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "6032", "train_lr": "9.55826e-05", "train_gnorm": "3.192", "train_clip": "0", "train_loss_scale": "512", "train_train_wall": "598", "train_gb_free": "2.9", "train_wall": "5799"}
[2025-07-04 18:59:50,972][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 18:59:50,988][fairseq.trainer][INFO] - begin training epoch 26
[2025-07-04 18:59:50,988][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 19:06:53,409][train_inner][INFO] - {"epoch": 26, "update": 25.694, "loss": "7.074", "nll_loss": "0.019", "loss_recon": "0.123", "loss_info_nce": "5.84", "ppl": "1.01", "wps": "1079.6", "ups": "0.37", "wpb": "2957.4", "bsz": "385.1", "num_updates": "6200", "lr": "9.52174e-05", "gnorm": "3.351", "clip": "3", "loss_scale": "512", "train_wall": "494", "gb_free": "2.9", "wall": "6222"}
[2025-07-04 19:09:55,151][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 26 @ 6274 updates
[2025-07-04 19:09:55,167][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint26.pt
[2025-07-04 19:09:56,655][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint26.pt
[2025-07-04 19:09:57,501][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint26.pt (epoch 26 @ 6274 updates, score None) (writing took 2.339687099999537 seconds)
[2025-07-04 19:09:57,501][fairseq_cli.train][INFO] - end of epoch 26 (average epoch stats below)
[2025-07-04 19:09:57,517][train][INFO] - {"epoch": 26, "train_loss": "7.034", "train_nll_loss": "0.019", "train_loss_recon": "0.123", "train_loss_info_nce": "5.806", "train_ppl": "1.01", "train_wps": "1179.1", "train_ups": "0.4", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "6274", "train_lr": "9.50565e-05", "train_gnorm": "3.311", "train_clip": "2.5", "train_loss_scale": "512", "train_train_wall": "598", "train_gb_free": "2.9", "train_wall": "6406"}
[2025-07-04 19:09:57,823][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 19:09:57,823][fairseq.trainer][INFO] - begin training epoch 27
[2025-07-04 19:09:57,823][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 19:15:16,379][train_inner][INFO] - {"epoch": 27, "update": 26.521, "loss": "6.932", "nll_loss": "0.019", "loss_recon": "0.122", "loss_info_nce": "5.716", "ppl": "1.01", "wps": "1174.5", "ups": "0.4", "wpb": "2953.7", "bsz": "386.4", "num_updates": "6400", "lr": "9.47826e-05", "gnorm": "2.937", "clip": "0.5", "loss_scale": "512", "train_wall": "494", "gb_free": "2.9", "wall": "6725"}
[2025-07-04 19:20:04,073][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 27 @ 6516 updates
[2025-07-04 19:20:04,073][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint27.pt
[2025-07-04 19:20:05,556][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint27.pt
[2025-07-04 19:20:06,385][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint27.pt (epoch 27 @ 6516 updates, score None) (writing took 2.316550300000017 seconds)
[2025-07-04 19:20:06,385][fairseq_cli.train][INFO] - end of epoch 27 (average epoch stats below)
[2025-07-04 19:20:06,401][train][INFO] - {"epoch": 27, "train_loss": "6.871", "train_nll_loss": "0.019", "train_loss_recon": "0.121", "train_loss_info_nce": "5.657", "train_ppl": "1.01", "train_wps": "1175.2", "train_ups": "0.4", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "6516", "train_lr": "9.45304e-05", "train_gnorm": "2.796", "train_clip": "0.4", "train_loss_scale": "512", "train_train_wall": "599", "train_gb_free": "2.9", "train_wall": "7015"}
[2025-07-04 19:20:06,722][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 19:20:06,722][fairseq.trainer][INFO] - begin training epoch 28
[2025-07-04 19:20:06,722][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 19:23:40,850][train_inner][INFO] - {"epoch": 28, "update": 27.347, "loss": "6.811", "nll_loss": "0.018", "loss_recon": "0.121", "loss_info_nce": "5.599", "ppl": "1.01", "wps": "1172.1", "ups": "0.4", "wpb": "2956.5", "bsz": "384.8", "num_updates": "6600", "lr": "9.43478e-05", "gnorm": "2.747", "clip": "0", "loss_scale": "512", "train_wall": "496", "gb_free": "2.9", "wall": "7229"}
[2025-07-04 19:30:17,256][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 28 @ 6758 updates
[2025-07-04 19:30:17,256][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint28.pt
[2025-07-04 19:30:19,233][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint28.pt
[2025-07-04 19:30:20,080][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint28.pt (epoch 28 @ 6758 updates, score None) (writing took 2.831087999999909 seconds)
[2025-07-04 19:30:20,080][fairseq_cli.train][INFO] - end of epoch 28 (average epoch stats below)
[2025-07-04 19:30:20,095][train][INFO] - {"epoch": 28, "train_loss": "6.739", "train_nll_loss": "0.018", "train_loss_recon": "0.121", "train_loss_info_nce": "5.533", "train_ppl": "1.01", "train_wps": "1166", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "6758", "train_lr": "9.40043e-05", "train_gnorm": "2.892", "train_clip": "0", "train_loss_scale": "512", "train_train_wall": "604", "train_gb_free": "2.9", "train_wall": "7629"}
[2025-07-04 19:30:20,405][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 19:30:20,415][fairseq.trainer][INFO] - begin training epoch 29
[2025-07-04 19:30:20,415][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 19:32:07,686][train_inner][INFO] - {"epoch": 29, "update": 28.174, "loss": "6.704", "nll_loss": "0.018", "loss_recon": "0.12", "loss_info_nce": "5.502", "ppl": "1.01", "wps": "1164.8", "ups": "0.39", "wpb": "2951.8", "bsz": "386.5", "num_updates": "6800", "lr": "9.3913e-05", "gnorm": "2.871", "clip": "0", "loss_scale": "512", "train_wall": "498", "gb_free": "2.9", "wall": "7736"}
[2025-07-04 19:40:23,605][train_inner][INFO] - {"epoch": 29, "update": 29.0, "loss": "6.604", "nll_loss": "0.018", "loss_recon": "0.119", "loss_info_nce": "5.418", "ppl": "1.01", "wps": "1192.3", "ups": "0.4", "wpb": "2956.5", "bsz": "385.4", "num_updates": "7000", "lr": "9.34783e-05", "gnorm": "2.872", "clip": "0", "loss_scale": "512", "train_wall": "493", "gb_free": "2.9", "wall": "8232"}
[2025-07-04 19:40:23,605][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 29 @ 7000 updates
[2025-07-04 19:40:23,605][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint29.pt
[2025-07-04 19:40:25,119][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint29.pt
[2025-07-04 19:40:25,842][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint29.pt (epoch 29 @ 7000 updates, score None) (writing took 2.24260890000005 seconds)
[2025-07-04 19:40:25,842][fairseq_cli.train][INFO] - end of epoch 29 (average epoch stats below)
[2025-07-04 19:40:25,868][train][INFO] - {"epoch": 29, "train_loss": "6.617", "train_nll_loss": "0.018", "train_loss_recon": "0.119", "train_loss_info_nce": "5.427", "train_ppl": "1.01", "train_wps": "1181.2", "train_ups": "0.4", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "7000", "train_lr": "9.34783e-05", "train_gnorm": "2.841", "train_clip": "0", "train_loss_scale": "512", "train_train_wall": "597", "train_gb_free": "2.9", "train_wall": "8234"}
[2025-07-04 19:40:26,236][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 19:40:26,252][fairseq.trainer][INFO] - begin training epoch 30
[2025-07-04 19:40:26,252][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 19:48:49,683][train_inner][INFO] - {"epoch": 30, "update": 29.826, "loss": "6.524", "nll_loss": "0.018", "loss_recon": "0.119", "loss_info_nce": "5.339", "ppl": "1.01", "wps": "1171.3", "ups": "0.4", "wpb": "2963.9", "bsz": "387.3", "num_updates": "7200", "lr": "9.30435e-05", "gnorm": "2.649", "clip": "0.5", "loss_scale": "512", "train_wall": "497", "gb_free": "2.9", "wall": "8738"}
[2025-07-04 19:50:31,224][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-04 19:51:15,030][valid][INFO] - {"epoch": 30, "valid_loss": "5.884", "valid_nll_loss": "0.016", "valid_loss_recon": "0.107", "valid_loss_info_nce": "4.815", "valid_ppl": "1.01", "valid_wps": "1879.4", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "7242", "valid_best_loss": "5.884"}
[2025-07-04 19:51:15,030][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 30 @ 7242 updates
[2025-07-04 19:51:15,030][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint30.pt
[2025-07-04 19:51:16,475][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint30.pt
[2025-07-04 19:51:18,085][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint30.pt (epoch 30 @ 7242 updates, score 5.884) (writing took 3.064004700001533 seconds)
[2025-07-04 19:51:18,085][fairseq_cli.train][INFO] - end of epoch 30 (average epoch stats below)
[2025-07-04 19:51:18,116][train][INFO] - {"epoch": 30, "train_loss": "6.512", "train_nll_loss": "0.018", "train_loss_recon": "0.118", "train_loss_info_nce": "5.329", "train_ppl": "1.01", "train_wps": "1097.1", "train_ups": "0.37", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "7242", "train_lr": "9.29522e-05", "train_gnorm": "2.456", "train_clip": "0.4", "train_loss_scale": "512", "train_train_wall": "598", "train_gb_free": "2.9", "train_wall": "8887"}
[2025-07-04 19:51:18,414][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 19:51:18,424][fairseq.trainer][INFO] - begin training epoch 31
[2025-07-04 19:51:18,424][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 19:57:56,250][train_inner][INFO] - {"epoch": 31, "update": 30.653, "loss": "6.432", "nll_loss": "0.017", "loss_recon": "0.117", "loss_info_nce": "5.258", "ppl": "1.01", "wps": "1082.3", "ups": "0.37", "wpb": "2957.4", "bsz": "384.6", "num_updates": "7400", "lr": "9.26087e-05", "gnorm": "2.329", "clip": "0", "loss_scale": "512", "train_wall": "492", "gb_free": "2.9", "wall": "9285"}
[2025-07-04 20:01:23,543][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 31 @ 7484 updates
[2025-07-04 20:01:23,543][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint31.pt
[2025-07-04 20:01:25,047][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint31.pt
[2025-07-04 20:01:25,764][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint31.pt (epoch 31 @ 7484 updates, score None) (writing took 2.2232285999998567 seconds)
[2025-07-04 20:01:25,764][fairseq_cli.train][INFO] - end of epoch 31 (average epoch stats below)
[2025-07-04 20:01:25,780][train][INFO] - {"epoch": 31, "train_loss": "6.418", "train_nll_loss": "0.017", "train_loss_recon": "0.117", "train_loss_info_nce": "5.244", "train_ppl": "1.01", "train_wps": "1177.5", "train_ups": "0.4", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "7484", "train_lr": "9.24261e-05", "train_gnorm": "2.655", "train_clip": "0", "train_loss_scale": "512", "train_train_wall": "597", "train_gb_free": "2.9", "train_wall": "9494"}
[2025-07-04 20:01:26,164][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 20:01:26,174][fairseq.trainer][INFO] - begin training epoch 32
[2025-07-04 20:01:26,174][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 20:06:19,042][train_inner][INFO] - {"epoch": 32, "update": 31.479, "loss": "6.371", "nll_loss": "0.017", "loss_recon": "0.117", "loss_info_nce": "5.205", "ppl": "1.01", "wps": "1173.8", "ups": "0.4", "wpb": "2950.9", "bsz": "386.4", "num_updates": "7600", "lr": "9.21739e-05", "gnorm": "2.422", "clip": "0", "loss_scale": "512", "train_wall": "494", "gb_free": "2.9", "wall": "9787"}
[2025-07-04 20:11:30,091][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 32 @ 7726 updates
[2025-07-04 20:11:30,091][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint32.pt
[2025-07-04 20:11:31,637][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint32.pt
[2025-07-04 20:11:32,360][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint32.pt (epoch 32 @ 7726 updates, score None) (writing took 2.262162699998953 seconds)
[2025-07-04 20:11:32,360][fairseq_cli.train][INFO] - end of epoch 32 (average epoch stats below)
[2025-07-04 20:11:32,370][train][INFO] - {"epoch": 32, "train_loss": "6.341", "train_nll_loss": "0.017", "train_loss_recon": "0.116", "train_loss_info_nce": "5.177", "train_ppl": "1.01", "train_wps": "1179.6", "train_ups": "0.4", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "7726", "train_lr": "9.19e-05", "train_gnorm": "2.143", "train_clip": "0", "train_loss_scale": "1024", "train_train_wall": "597", "train_gb_free": "2.9", "train_wall": "10101"}
[2025-07-04 20:11:32,682][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 20:11:32,682][fairseq.trainer][INFO] - begin training epoch 33
[2025-07-04 20:11:32,682][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 20:14:40,496][train_inner][INFO] - {"epoch": 33, "update": 32.306, "loss": "6.323", "nll_loss": "0.017", "loss_recon": "0.116", "loss_info_nce": "5.16", "ppl": "1.01", "wps": "1180.3", "ups": "0.4", "wpb": "2959.3", "bsz": "385", "num_updates": "7800", "lr": "9.17391e-05", "gnorm": "2.318", "clip": "0", "loss_scale": "1024", "train_wall": "492", "gb_free": "2.9", "wall": "10289"}
[2025-07-04 20:21:41,216][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 33 @ 7968 updates
[2025-07-04 20:21:41,231][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint33.pt
[2025-07-04 20:21:42,704][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint33.pt
[2025-07-04 20:21:43,440][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint33.pt (epoch 33 @ 7968 updates, score None) (writing took 2.2117760000001 seconds)
[2025-07-04 20:21:43,440][fairseq_cli.train][INFO] - end of epoch 33 (average epoch stats below)
[2025-07-04 20:21:43,440][train][INFO] - {"epoch": 33, "train_loss": "6.276", "train_nll_loss": "0.017", "train_loss_recon": "0.116", "train_loss_info_nce": "5.118", "train_ppl": "1.01", "train_wps": "1171", "train_ups": "0.4", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "7968", "train_lr": "9.13739e-05", "train_gnorm": "2.347", "train_clip": "0", "train_loss_scale": "1024", "train_train_wall": "601", "train_gb_free": "2.9", "train_wall": "10712"}
[2025-07-04 20:21:43,834][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 20:21:43,834][fairseq.trainer][INFO] - begin training epoch 34
[2025-07-04 20:21:43,834][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 20:23:07,383][train_inner][INFO] - {"epoch": 34, "update": 33.132, "loss": "6.257", "nll_loss": "0.017", "loss_recon": "0.116", "loss_info_nce": "5.101", "ppl": "1.01", "wps": "1165.4", "ups": "0.39", "wpb": "2953.7", "bsz": "385.9", "num_updates": "8000", "lr": "9.13043e-05", "gnorm": "2.291", "clip": "0", "loss_scale": "1024", "train_wall": "497", "gb_free": "2.9", "wall": "10796"}
[2025-07-04 20:31:27,367][train_inner][INFO] - {"epoch": 34, "update": 33.959, "loss": "6.208", "nll_loss": "0.017", "loss_recon": "0.115", "loss_info_nce": "5.058", "ppl": "1.01", "wps": "1186", "ups": "0.4", "wpb": "2964.8", "bsz": "386.6", "num_updates": "8200", "lr": "9.08696e-05", "gnorm": "2.489", "clip": "0.5", "loss_scale": "1024", "train_wall": "494", "gb_free": "2.9", "wall": "11296"}
[2025-07-04 20:31:49,500][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 34 @ 8210 updates
[2025-07-04 20:31:49,500][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint34.pt
[2025-07-04 20:31:51,082][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint34.pt
[2025-07-04 20:31:51,825][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint34.pt (epoch 34 @ 8210 updates, score None) (writing took 2.3138873999996576 seconds)
[2025-07-04 20:31:51,825][fairseq_cli.train][INFO] - end of epoch 34 (average epoch stats below)
[2025-07-04 20:31:51,825][train][INFO] - {"epoch": 34, "train_loss": "6.208", "train_nll_loss": "0.017", "train_loss_recon": "0.115", "train_loss_info_nce": "5.058", "train_ppl": "1.01", "train_wps": "1176.1", "train_ups": "0.4", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "8210", "train_lr": "9.08478e-05", "train_gnorm": "2.488", "train_clip": "0.4", "train_loss_scale": "1024", "train_train_wall": "596", "train_gb_free": "2.9", "train_wall": "11320"}
[2025-07-04 20:31:52,139][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 20:31:52,139][fairseq.trainer][INFO] - begin training epoch 35
[2025-07-04 20:31:52,155][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 20:39:52,935][train_inner][INFO] - {"epoch": 35, "update": 34.785, "loss": "6.151", "nll_loss": "0.017", "loss_recon": "0.114", "loss_info_nce": "5.008", "ppl": "1.01", "wps": "1168.8", "ups": "0.4", "wpb": "2954.6", "bsz": "386.2", "num_updates": "8400", "lr": "9.04348e-05", "gnorm": "2.061", "clip": "0", "loss_scale": "1024", "train_wall": "496", "gb_free": "2.9", "wall": "11801"}
[2025-07-04 20:42:00,543][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-04 20:42:46,414][valid][INFO] - {"epoch": 35, "valid_loss": "5.686", "valid_nll_loss": "0.015", "valid_loss_recon": "0.107", "valid_loss_info_nce": "4.617", "valid_ppl": "1.01", "valid_wps": "1794.6", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "8452", "valid_best_loss": "5.686"}
[2025-07-04 20:42:46,414][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 35 @ 8452 updates
[2025-07-04 20:42:46,414][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint35.pt
[2025-07-04 20:42:47,956][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint35.pt
[2025-07-04 20:42:50,122][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint35.pt (epoch 35 @ 8452 updates, score 5.686) (writing took 3.7149362999989535 seconds)
[2025-07-04 20:42:50,122][fairseq_cli.train][INFO] - end of epoch 35 (average epoch stats below)
[2025-07-04 20:42:50,637][train][INFO] - {"epoch": 35, "train_loss": "6.148", "train_nll_loss": "0.017", "train_loss_recon": "0.114", "train_loss_info_nce": "5.004", "train_ppl": "1.01", "train_wps": "1087", "train_ups": "0.37", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "8452", "train_lr": "9.03217e-05", "train_gnorm": "1.98", "train_clip": "0", "train_loss_scale": "1024", "train_train_wall": "601", "train_gb_free": "2.9", "train_wall": "11979"}
[2025-07-04 20:42:50,949][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 20:42:50,965][fairseq.trainer][INFO] - begin training epoch 36
[2025-07-04 20:42:50,965][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 20:48:43,148][train_inner][INFO] - {"epoch": 36, "update": 35.612, "loss": "6.107", "nll_loss": "0.016", "loss_recon": "0.114", "loss_info_nce": "4.966", "ppl": "1.01", "wps": "1114.2", "ups": "0.38", "wpb": "2953.7", "bsz": "385.7", "num_updates": "8600", "lr": "9e-05", "gnorm": "2.026", "clip": "0", "loss_scale": "1024", "train_wall": "475", "gb_free": "2.9", "wall": "12332"}
[2025-07-04 20:51:57,655][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 36 @ 8694 updates
[2025-07-04 20:51:57,655][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint36.pt
[2025-07-04 20:51:59,045][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint36.pt
[2025-07-04 20:51:59,541][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint36.pt (epoch 36 @ 8694 updates, score None) (writing took 1.8952913000011904 seconds)
[2025-07-04 20:51:59,541][fairseq_cli.train][INFO] - end of epoch 36 (average epoch stats below)
[2025-07-04 20:51:59,557][train][INFO] - {"epoch": 36, "train_loss": "6.085", "train_nll_loss": "0.016", "train_loss_recon": "0.114", "train_loss_info_nce": "4.949", "train_ppl": "1.01", "train_wps": "1303.6", "train_ups": "0.44", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "8694", "train_lr": "8.97957e-05", "train_gnorm": "2.084", "train_clip": "0", "train_loss_scale": "1024", "train_train_wall": "542", "train_gb_free": "2.9", "train_wall": "12528"}
[2025-07-04 20:51:59,722][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 20:51:59,722][fairseq.trainer][INFO] - begin training epoch 37
[2025-07-04 20:51:59,722][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 20:55:42,836][train_inner][INFO] - {"epoch": 37, "update": 36.438, "loss": "6.065", "nll_loss": "0.016", "loss_recon": "0.113", "loss_info_nce": "4.93", "ppl": "1.01", "wps": "1408.4", "ups": "0.48", "wpb": "2955.5", "bsz": "385", "num_updates": "8800", "lr": "8.95652e-05", "gnorm": "2.166", "clip": "0", "loss_scale": "1024", "train_wall": "415", "gb_free": "2.9", "wall": "12751"}
[2025-07-04 21:00:25,887][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 37 @ 8936 updates
[2025-07-04 21:00:25,887][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint37.pt
[2025-07-04 21:00:27,331][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint37.pt
[2025-07-04 21:00:27,827][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint37.pt (epoch 37 @ 8936 updates, score None) (writing took 1.937330099999599 seconds)
[2025-07-04 21:00:27,827][fairseq_cli.train][INFO] - end of epoch 37 (average epoch stats below)
[2025-07-04 21:00:27,842][train][INFO] - {"epoch": 37, "train_loss": "6.049", "train_nll_loss": "0.016", "train_loss_recon": "0.114", "train_loss_info_nce": "4.915", "train_ppl": "1.01", "train_wps": "1407.8", "train_ups": "0.48", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "8936", "train_lr": "8.92696e-05", "train_gnorm": "2.116", "train_clip": "0", "train_loss_scale": "1024", "train_train_wall": "503", "train_gb_free": "3", "train_wall": "13036"}
[2025-07-04 21:00:28,067][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 21:00:28,067][fairseq.trainer][INFO] - begin training epoch 38
[2025-07-04 21:00:28,067][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 21:02:44,202][train_inner][INFO] - {"epoch": 38, "update": 37.264, "loss": "6.027", "nll_loss": "0.016", "loss_recon": "0.113", "loss_info_nce": "4.896", "ppl": "1.01", "wps": "1402.9", "ups": "0.47", "wpb": "2955.5", "bsz": "386.1", "num_updates": "9000", "lr": "8.91304e-05", "gnorm": "1.9", "clip": "0", "loss_scale": "1024", "train_wall": "416", "gb_free": "2.9", "wall": "13173"}
[2025-07-04 21:08:55,179][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 38 @ 9178 updates
[2025-07-04 21:08:55,179][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint38.pt
[2025-07-04 21:08:56,611][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint38.pt
[2025-07-04 21:08:57,099][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint38.pt (epoch 38 @ 9178 updates, score None) (writing took 1.9202463000001444 seconds)
[2025-07-04 21:08:57,099][fairseq_cli.train][INFO] - end of epoch 38 (average epoch stats below)
[2025-07-04 21:08:57,115][train][INFO] - {"epoch": 38, "train_loss": "5.985", "train_nll_loss": "0.016", "train_loss_recon": "0.113", "train_loss_info_nce": "4.859", "train_ppl": "1.01", "train_wps": "1405.1", "train_ups": "0.48", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "9178", "train_lr": "8.87435e-05", "train_gnorm": "2.07", "train_clip": "0", "train_loss_scale": "1024", "train_train_wall": "504", "train_gb_free": "2.9", "train_wall": "13546"}
[2025-07-04 21:08:57,280][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 21:08:57,280][fairseq.trainer][INFO] - begin training epoch 39
[2025-07-04 21:08:57,280][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 21:09:44,921][train_inner][INFO] - {"epoch": 39, "update": 38.091, "loss": "5.977", "nll_loss": "0.016", "loss_recon": "0.113", "loss_info_nce": "4.85", "ppl": "1.01", "wps": "1405.4", "ups": "0.48", "wpb": "2956.5", "bsz": "385.1", "num_updates": "9200", "lr": "8.86957e-05", "gnorm": "2.25", "clip": "0", "loss_scale": "1024", "train_wall": "416", "gb_free": "2.9", "wall": "13593"}
[2025-07-04 21:16:42,397][train_inner][INFO] - {"epoch": 39, "update": 38.917, "loss": "5.937", "nll_loss": "0.016", "loss_recon": "0.112", "loss_info_nce": "4.815", "ppl": "1.01", "wps": "1419.9", "ups": "0.48", "wpb": "2963.9", "bsz": "386.8", "num_updates": "9400", "lr": "8.82609e-05", "gnorm": "2.134", "clip": "0", "loss_scale": "1024", "train_wall": "416", "gb_free": "2.9", "wall": "14011"}
[2025-07-04 21:17:22,796][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 39 @ 9420 updates
[2025-07-04 21:17:22,796][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint39.pt
[2025-07-04 21:17:24,266][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint39.pt
[2025-07-04 21:17:24,708][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint39.pt (epoch 39 @ 9420 updates, score None) (writing took 1.9105933999999252 seconds)
[2025-07-04 21:17:24,708][fairseq_cli.train][INFO] - end of epoch 39 (average epoch stats below)
[2025-07-04 21:17:24,708][train][INFO] - {"epoch": 39, "train_loss": "5.936", "train_nll_loss": "0.016", "train_loss_recon": "0.112", "train_loss_info_nce": "4.814", "train_ppl": "1.01", "train_wps": "1409.7", "train_ups": "0.48", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "9420", "train_lr": "8.82174e-05", "train_gnorm": "2.199", "train_clip": "0", "train_loss_scale": "1024", "train_train_wall": "502", "train_gb_free": "2.9", "train_wall": "14053"}
[2025-07-04 21:17:24,890][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 21:17:24,905][fairseq.trainer][INFO] - begin training epoch 40
[2025-07-04 21:17:24,905][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 21:23:41,524][train_inner][INFO] - {"epoch": 40, "update": 39.744, "loss": "5.909", "nll_loss": "0.016", "loss_recon": "0.112", "loss_info_nce": "4.792", "ppl": "1.01", "wps": "1410.4", "ups": "0.48", "wpb": "2955.5", "bsz": "385.8", "num_updates": "9600", "lr": "8.78261e-05", "gnorm": "1.857", "clip": "0", "loss_scale": "1024", "train_wall": "414", "gb_free": "2.9", "wall": "14430"}
[2025-07-04 21:25:48,157][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-04 21:26:17,222][valid][INFO] - {"epoch": 40, "valid_loss": "5.469", "valid_nll_loss": "0.015", "valid_loss_recon": "0.102", "valid_loss_info_nce": "4.449", "valid_ppl": "1.01", "valid_wps": "2843.8", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "9662", "valid_best_loss": "5.469"}
[2025-07-04 21:26:17,222][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 40 @ 9662 updates
[2025-07-04 21:26:17,222][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint40.pt
[2025-07-04 21:26:24,838][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint40.pt
[2025-07-04 21:26:25,728][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint40.pt (epoch 40 @ 9662 updates, score 5.469) (writing took 8.495829199999207 seconds)
[2025-07-04 21:26:25,728][fairseq_cli.train][INFO] - end of epoch 40 (average epoch stats below)
[2025-07-04 21:26:25,728][train][INFO] - {"epoch": 40, "train_loss": "5.907", "train_nll_loss": "0.016", "train_loss_recon": "0.112", "train_loss_info_nce": "4.787", "train_ppl": "1.01", "train_wps": "1322.6", "train_ups": "0.45", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "9662", "train_lr": "8.76913e-05", "train_gnorm": "1.95", "train_clip": "0", "train_loss_scale": "1024", "train_train_wall": "500", "train_gb_free": "2.9", "train_wall": "14594"}
[2025-07-04 21:26:25,900][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 21:26:25,916][fairseq.trainer][INFO] - begin training epoch 41
[2025-07-04 21:26:25,916][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 21:31:15,901][train_inner][INFO] - {"epoch": 41, "update": 40.57, "loss": "5.879", "nll_loss": "0.016", "loss_recon": "0.112", "loss_info_nce": "4.761", "ppl": "1.01", "wps": "1299.7", "ups": "0.44", "wpb": "2952.8", "bsz": "386.2", "num_updates": "9800", "lr": "8.73913e-05", "gnorm": "2.089", "clip": "0", "loss_scale": "2048", "train_wall": "413", "gb_free": "2.9", "wall": "14884"}
[2025-07-04 21:34:55,015][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 41 @ 9904 updates
[2025-07-04 21:34:55,015][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint41.pt
[2025-07-04 21:34:56,578][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint41.pt
[2025-07-04 21:34:57,121][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint41.pt (epoch 41 @ 9904 updates, score None) (writing took 2.0943322999992233 seconds)
[2025-07-04 21:34:57,122][fairseq_cli.train][INFO] - end of epoch 41 (average epoch stats below)
[2025-07-04 21:34:57,127][train][INFO] - {"epoch": 41, "train_loss": "5.863", "train_nll_loss": "0.016", "train_loss_recon": "0.111", "train_loss_info_nce": "4.75", "train_ppl": "1.01", "train_wps": "1399.2", "train_ups": "0.47", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "9904", "train_lr": "8.71652e-05", "train_gnorm": "2.108", "train_clip": "0", "train_loss_scale": "2048", "train_train_wall": "506", "train_gb_free": "2.9", "train_wall": "15106"}
[2025-07-04 21:34:57,430][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 21:34:57,434][fairseq.trainer][INFO] - begin training epoch 42
[2025-07-04 21:34:57,435][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 21:38:25,457][train_inner][INFO] - {"epoch": 42, "update": 41.397, "loss": "5.843", "nll_loss": "0.016", "loss_recon": "0.111", "loss_info_nce": "4.734", "ppl": "1.01", "wps": "1377", "ups": "0.47", "wpb": "2957.4", "bsz": "384.8", "num_updates": "10000", "lr": "8.69565e-05", "gnorm": "2.251", "clip": "0", "loss_scale": "2048", "train_wall": "424", "gb_free": "2.9", "wall": "15314"}
[2025-07-04 21:38:25,457][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-04 21:38:52,136][valid][INFO] - {"epoch": 42, "valid_loss": "5.437", "valid_nll_loss": "0.015", "valid_loss_recon": "0.103", "valid_loss_info_nce": "4.409", "valid_ppl": "1.01", "valid_wps": "3093.7", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "10000", "valid_best_loss": "5.437"}
[2025-07-04 21:43:55,556][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 42 @ 10146 updates
[2025-07-04 21:43:55,556][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint42.pt
[2025-07-04 21:43:56,982][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint42.pt
[2025-07-04 21:43:57,420][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint42.pt (epoch 42 @ 10146 updates, score None) (writing took 1.8718984999995882 seconds)
[2025-07-04 21:43:57,435][fairseq_cli.train][INFO] - end of epoch 42 (average epoch stats below)
[2025-07-04 21:43:57,435][train][INFO] - {"epoch": 42, "train_loss": "5.826", "train_nll_loss": "0.016", "train_loss_recon": "0.111", "train_loss_info_nce": "4.716", "train_ppl": "1.01", "train_wps": "1324.3", "train_ups": "0.45", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "10146", "train_lr": "8.66391e-05", "train_gnorm": "1.9", "train_clip": "0", "train_loss_scale": "2048", "train_train_wall": "507", "train_gb_free": "2.9", "train_wall": "15646"}
[2025-07-04 21:43:57,609][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 21:43:57,609][fairseq.trainer][INFO] - begin training epoch 43
[2025-07-04 21:43:57,609][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 21:45:52,514][train_inner][INFO] - {"epoch": 43, "update": 42.223, "loss": "5.817", "nll_loss": "0.016", "loss_recon": "0.111", "loss_info_nce": "4.707", "ppl": "1.01", "wps": "1322.2", "ups": "0.45", "wpb": "2955.5", "bsz": "385.6", "num_updates": "10200", "lr": "8.65217e-05", "gnorm": "1.811", "clip": "0", "loss_scale": "2048", "train_wall": "415", "gb_free": "2.9", "wall": "15761"}
[2025-07-04 21:52:24,068][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 43 @ 10388 updates
[2025-07-04 21:52:24,068][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint43.pt
[2025-07-04 21:52:25,570][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint43.pt
[2025-07-04 21:52:26,013][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint43.pt (epoch 43 @ 10388 updates, score None) (writing took 1.941153600000689 seconds)
[2025-07-04 21:52:26,013][fairseq_cli.train][INFO] - end of epoch 43 (average epoch stats below)
[2025-07-04 21:52:26,013][train][INFO] - {"epoch": 43, "train_loss": "5.794", "train_nll_loss": "0.016", "train_loss_recon": "0.111", "train_loss_info_nce": "4.688", "train_ppl": "1.01", "train_wps": "1407", "train_ups": "0.48", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "10388", "train_lr": "8.6113e-05", "train_gnorm": "1.944", "train_clip": "0", "train_loss_scale": "2048", "train_train_wall": "504", "train_gb_free": "2.9", "train_wall": "16154"}
[2025-07-04 21:52:26,232][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 21:52:26,232][fairseq.trainer][INFO] - begin training epoch 44
[2025-07-04 21:52:26,232][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 21:52:53,372][train_inner][INFO] - {"epoch": 44, "update": 43.05, "loss": "5.791", "nll_loss": "0.016", "loss_recon": "0.111", "loss_info_nce": "4.684", "ppl": "1.01", "wps": "1404.1", "ups": "0.48", "wpb": "2954.6", "bsz": "385.6", "num_updates": "10400", "lr": "8.6087e-05", "gnorm": "1.971", "clip": "0", "loss_scale": "2048", "train_wall": "416", "gb_free": "2.9", "wall": "16182"}
[2025-07-04 22:01:11,402][train_inner][INFO] - {"epoch": 44, "update": 43.876, "loss": "5.754", "nll_loss": "0.016", "loss_recon": "0.11", "loss_info_nce": "4.654", "ppl": "1.01", "wps": "1191.7", "ups": "0.4", "wpb": "2967.6", "bsz": "386", "num_updates": "10600", "lr": "8.56522e-05", "gnorm": "1.718", "clip": "0", "loss_scale": "2048", "train_wall": "494", "gb_free": "2.9", "wall": "16680"}
[2025-07-04 22:02:24,317][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 44 @ 10630 updates
[2025-07-04 22:02:24,317][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint44.pt
[2025-07-04 22:02:25,835][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint44.pt
[2025-07-04 22:02:26,570][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint44.pt (epoch 44 @ 10630 updates, score None) (writing took 2.25104680000004 seconds)
[2025-07-04 22:02:26,570][fairseq_cli.train][INFO] - end of epoch 44 (average epoch stats below)
[2025-07-04 22:02:26,585][train][INFO] - {"epoch": 44, "train_loss": "5.751", "train_nll_loss": "0.016", "train_loss_recon": "0.11", "train_loss_info_nce": "4.651", "train_ppl": "1.01", "train_wps": "1191.5", "train_ups": "0.4", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "10630", "train_lr": "8.5587e-05", "train_gnorm": "1.757", "train_clip": "0", "train_loss_scale": "2048", "train_train_wall": "592", "train_gb_free": "2.9", "train_wall": "16755"}
[2025-07-04 22:02:26,897][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 22:02:26,897][fairseq.trainer][INFO] - begin training epoch 45
[2025-07-04 22:02:26,897][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 22:08:55,359][train_inner][INFO] - {"epoch": 45, "update": 44.702, "loss": "5.721", "nll_loss": "0.015", "loss_recon": "0.11", "loss_info_nce": "4.623", "ppl": "1.01", "wps": "1272.5", "ups": "0.43", "wpb": "2951.8", "bsz": "386.2", "num_updates": "10800", "lr": "8.52174e-05", "gnorm": "1.758", "clip": "0", "loss_scale": "2048", "train_wall": "457", "gb_free": "2.9", "wall": "17144"}
[2025-07-04 22:11:54,321][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-04 22:12:38,746][valid][INFO] - {"epoch": 45, "valid_loss": "5.345", "valid_nll_loss": "0.015", "valid_loss_recon": "0.102", "valid_loss_info_nce": "4.324", "valid_ppl": "1.01", "valid_wps": "1850.7", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "10872", "valid_best_loss": "5.345"}
[2025-07-04 22:12:38,746][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 45 @ 10872 updates
[2025-07-04 22:12:38,746][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint45.pt
[2025-07-04 22:12:40,180][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint45.pt
[2025-07-04 22:12:41,765][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint45.pt (epoch 45 @ 10872 updates, score 5.345) (writing took 3.0148350999988907 seconds)
[2025-07-04 22:12:41,765][fairseq_cli.train][INFO] - end of epoch 45 (average epoch stats below)
[2025-07-04 22:12:41,765][train][INFO] - {"epoch": 45, "train_loss": "5.712", "train_nll_loss": "0.015", "train_loss_recon": "0.11", "train_loss_info_nce": "4.616", "train_ppl": "1.01", "train_wps": "1163.1", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "10872", "train_lr": "8.50609e-05", "train_gnorm": "1.892", "train_clip": "0", "train_loss_scale": "2048", "train_train_wall": "562", "train_gb_free": "2.9", "train_wall": "17370"}
[2025-07-04 22:12:42,076][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 22:12:42,092][fairseq.trainer][INFO] - begin training epoch 46
[2025-07-04 22:12:42,092][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 22:18:08,486][train_inner][INFO] - {"epoch": 46, "update": 45.529, "loss": "5.693", "nll_loss": "0.015", "loss_recon": "0.109", "loss_info_nce": "4.601", "ppl": "1.01", "wps": "1067.3", "ups": "0.36", "wpb": "2951.8", "bsz": "386.9", "num_updates": "11000", "lr": "8.47826e-05", "gnorm": "1.775", "clip": "0", "loss_scale": "2048", "train_wall": "499", "gb_free": "2.9", "wall": "17697"}
[2025-07-04 22:22:34,890][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 46 @ 11114 updates
[2025-07-04 22:22:34,890][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint46.pt
[2025-07-04 22:22:36,250][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint46.pt
[2025-07-04 22:22:36,727][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint46.pt (epoch 46 @ 11114 updates, score None) (writing took 1.8420761999987008 seconds)
[2025-07-04 22:22:36,727][fairseq_cli.train][INFO] - end of epoch 46 (average epoch stats below)
[2025-07-04 22:22:36,727][train][INFO] - {"epoch": 46, "train_loss": "5.69", "train_nll_loss": "0.015", "train_loss_recon": "0.109", "train_loss_info_nce": "4.596", "train_ppl": "1.01", "train_wps": "1202.7", "train_ups": "0.41", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "11114", "train_lr": "8.45348e-05", "train_gnorm": "1.672", "train_clip": "0", "train_loss_scale": "2048", "train_train_wall": "586", "train_gb_free": "2.9", "train_wall": "17965"}
[2025-07-04 22:22:36,902][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 22:22:36,902][fairseq.trainer][INFO] - begin training epoch 47
[2025-07-04 22:22:36,902][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 22:25:42,627][train_inner][INFO] - {"epoch": 47, "update": 46.355, "loss": "5.678", "nll_loss": "0.015", "loss_recon": "0.11", "loss_info_nce": "4.583", "ppl": "1.01", "wps": "1302.8", "ups": "0.44", "wpb": "2958.3", "bsz": "384.4", "num_updates": "11200", "lr": "8.43478e-05", "gnorm": "1.681", "clip": "0", "loss_scale": "2048", "train_wall": "449", "gb_free": "2.9", "wall": "18151"}
[2025-07-04 22:31:56,645][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 47 @ 11356 updates
[2025-07-04 22:31:56,645][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint47.pt
[2025-07-04 22:31:58,141][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint47.pt
[2025-07-04 22:31:58,931][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint47.pt (epoch 47 @ 11356 updates, score None) (writing took 2.290343500000745 seconds)
[2025-07-04 22:31:58,931][fairseq_cli.train][INFO] - end of epoch 47 (average epoch stats below)
[2025-07-04 22:31:58,931][train][INFO] - {"epoch": 47, "train_loss": "5.657", "train_nll_loss": "0.015", "train_loss_recon": "0.109", "train_loss_info_nce": "4.566", "train_ppl": "1.01", "train_wps": "1272.8", "train_ups": "0.43", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "11356", "train_lr": "8.40087e-05", "train_gnorm": "1.74", "train_clip": "0", "train_loss_scale": "2048", "train_train_wall": "555", "train_gb_free": "2.9", "train_wall": "18527"}
[2025-07-04 22:31:59,315][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 22:31:59,315][fairseq.trainer][INFO] - begin training epoch 48
[2025-07-04 22:31:59,325][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 22:33:38,180][train_inner][INFO] - {"epoch": 48, "update": 47.182, "loss": "5.648", "nll_loss": "0.015", "loss_recon": "0.109", "loss_info_nce": "4.56", "ppl": "1.01", "wps": "1243", "ups": "0.42", "wpb": "2955.5", "bsz": "385.8", "num_updates": "11400", "lr": "8.3913e-05", "gnorm": "1.73", "clip": "0", "loss_scale": "2048", "train_wall": "468", "gb_free": "2.9", "wall": "18627"}
[2025-07-04 22:41:30,197][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 48 @ 11598 updates
[2025-07-04 22:41:30,197][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint48.pt
[2025-07-04 22:41:31,640][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint48.pt
[2025-07-04 22:41:32,457][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint48.pt (epoch 48 @ 11598 updates, score None) (writing took 2.261637999999948 seconds)
[2025-07-04 22:41:32,457][fairseq_cli.train][INFO] - end of epoch 48 (average epoch stats below)
[2025-07-04 22:41:32,467][train][INFO] - {"epoch": 48, "train_loss": "5.633", "train_nll_loss": "0.015", "train_loss_recon": "0.109", "train_loss_info_nce": "4.545", "train_ppl": "1.01", "train_wps": "1247.6", "train_ups": "0.42", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "11598", "train_lr": "8.34826e-05", "train_gnorm": "1.797", "train_clip": "0", "train_loss_scale": "2048", "train_train_wall": "566", "train_gb_free": "2.9", "train_wall": "19101"}
[2025-07-04 22:41:32,827][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 22:41:32,827][fairseq.trainer][INFO] - begin training epoch 49
[2025-07-04 22:41:32,827][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 22:41:41,231][train_inner][INFO] - {"epoch": 49, "update": 48.008, "loss": "5.632", "nll_loss": "0.015", "loss_recon": "0.109", "loss_info_nce": "4.545", "ppl": "1.01", "wps": "1223.7", "ups": "0.41", "wpb": "2955.5", "bsz": "385.4", "num_updates": "11600", "lr": "8.34783e-05", "gnorm": "1.891", "clip": "0", "loss_scale": "2048", "train_wall": "475", "gb_free": "2.9", "wall": "19110"}
[2025-07-04 22:50:09,813][train_inner][INFO] - {"epoch": 49, "update": 48.835, "loss": "5.611", "nll_loss": "0.015", "loss_recon": "0.109", "loss_info_nce": "4.525", "ppl": "1.01", "wps": "1166.3", "ups": "0.39", "wpb": "2965.8", "bsz": "386.8", "num_updates": "11800", "lr": "8.30435e-05", "gnorm": "1.549", "clip": "0", "loss_scale": "2048", "train_wall": "505", "gb_free": "2.9", "wall": "19618"}
[2025-07-04 22:51:47,691][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 49 @ 11840 updates
[2025-07-04 22:51:47,693][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint49.pt
[2025-07-04 22:51:49,133][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint49.pt
[2025-07-04 22:51:49,892][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint49.pt (epoch 49 @ 11840 updates, score None) (writing took 2.202199899998959 seconds)
[2025-07-04 22:51:49,892][fairseq_cli.train][INFO] - end of epoch 49 (average epoch stats below)
[2025-07-04 22:51:49,892][train][INFO] - {"epoch": 49, "train_loss": "5.608", "train_nll_loss": "0.015", "train_loss_recon": "0.108", "train_loss_info_nce": "4.524", "train_ppl": "1.01", "train_wps": "1158.9", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "11840", "train_lr": "8.29565e-05", "train_gnorm": "1.516", "train_clip": "0", "train_loss_scale": "4096", "train_train_wall": "608", "train_gb_free": "2.9", "train_wall": "19718"}
[2025-07-04 22:51:50,276][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 22:51:50,276][fairseq.trainer][INFO] - begin training epoch 50
[2025-07-04 22:51:50,276][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 22:58:35,114][train_inner][INFO] - {"epoch": 50, "update": 49.661, "loss": "5.588", "nll_loss": "0.015", "loss_recon": "0.108", "loss_info_nce": "4.507", "ppl": "1.01", "wps": "1168", "ups": "0.4", "wpb": "2950.9", "bsz": "386.3", "num_updates": "12000", "lr": "8.26087e-05", "gnorm": "1.615", "clip": "0", "loss_scale": "4096", "train_wall": "497", "gb_free": "2.9", "wall": "20124"}
[2025-07-04 23:01:28,201][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-04 23:01:55,764][valid][INFO] - {"epoch": 50, "valid_loss": "5.226", "valid_nll_loss": "0.014", "valid_loss_recon": "0.1", "valid_loss_info_nce": "4.225", "valid_ppl": "1.01", "valid_wps": "3000.4", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "12082", "valid_best_loss": "5.226"}
[2025-07-04 23:01:55,764][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 50 @ 12082 updates
[2025-07-04 23:01:55,764][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint50.pt
[2025-07-04 23:01:57,118][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint50.pt
[2025-07-04 23:01:58,095][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint50.pt (epoch 50 @ 12082 updates, score 5.226) (writing took 2.3371060000026773 seconds)
[2025-07-04 23:01:58,095][fairseq_cli.train][INFO] - end of epoch 50 (average epoch stats below)
[2025-07-04 23:01:58,111][train][INFO] - {"epoch": 50, "train_loss": "5.585", "train_nll_loss": "0.015", "train_loss_recon": "0.108", "train_loss_info_nce": "4.504", "train_ppl": "1.01", "train_wps": "1176.5", "train_ups": "0.4", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "12082", "train_lr": "8.24304e-05", "train_gnorm": "1.656", "train_clip": "0", "train_loss_scale": "4096", "train_train_wall": "572", "train_gb_free": "2.9", "train_wall": "20327"}
[2025-07-04 23:01:58,296][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 23:01:58,307][fairseq.trainer][INFO] - begin training epoch 51
[2025-07-04 23:01:58,308][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 23:06:12,363][train_inner][INFO] - {"epoch": 51, "update": 50.488, "loss": "5.578", "nll_loss": "0.015", "loss_recon": "0.108", "loss_info_nce": "4.498", "ppl": "1.01", "wps": "1295.6", "ups": "0.44", "wpb": "2962.1", "bsz": "384.2", "num_updates": "12200", "lr": "8.21739e-05", "gnorm": "1.609", "clip": "0", "loss_scale": "4096", "train_wall": "424", "gb_free": "2.9", "wall": "20581"}
[2025-07-04 23:10:29,963][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 51 @ 12324 updates
[2025-07-04 23:10:29,973][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint51.pt
[2025-07-04 23:10:31,290][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint51.pt
[2025-07-04 23:10:31,792][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint51.pt (epoch 51 @ 12324 updates, score None) (writing took 1.8263205000002927 seconds)
[2025-07-04 23:10:31,792][fairseq_cli.train][INFO] - end of epoch 51 (average epoch stats below)
[2025-07-04 23:10:31,808][train][INFO] - {"epoch": 51, "train_loss": "5.567", "train_nll_loss": "0.015", "train_loss_recon": "0.108", "train_loss_info_nce": "4.487", "train_ppl": "1.01", "train_wps": "1393", "train_ups": "0.47", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "12324", "train_lr": "8.19043e-05", "train_gnorm": "1.635", "train_clip": "0", "train_loss_scale": "4096", "train_train_wall": "508", "train_gb_free": "2.9", "train_wall": "20840"}
[2025-07-04 23:10:31,976][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 23:10:31,976][fairseq.trainer][INFO] - begin training epoch 52
[2025-07-04 23:10:31,976][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 23:13:14,676][train_inner][INFO] - {"epoch": 52, "update": 51.314, "loss": "5.552", "nll_loss": "0.015", "loss_recon": "0.107", "loss_info_nce": "4.477", "ppl": "1.01", "wps": "1398.6", "ups": "0.47", "wpb": "2952.8", "bsz": "385.9", "num_updates": "12400", "lr": "8.17391e-05", "gnorm": "1.678", "clip": "0", "loss_scale": "4096", "train_wall": "417", "gb_free": "2.9", "wall": "21003"}
[2025-07-04 23:19:03,763][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 52 @ 12566 updates
[2025-07-04 23:19:03,763][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint52.pt
[2025-07-04 23:19:05,221][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint52.pt
[2025-07-04 23:19:05,655][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint52.pt (epoch 52 @ 12566 updates, score None) (writing took 1.8948103999973682 seconds)
[2025-07-04 23:19:05,665][fairseq_cli.train][INFO] - end of epoch 52 (average epoch stats below)
[2025-07-04 23:19:05,665][train][INFO] - {"epoch": 52, "train_loss": "5.538", "train_nll_loss": "0.015", "train_loss_recon": "0.108", "train_loss_info_nce": "4.462", "train_ppl": "1.01", "train_wps": "1392.5", "train_ups": "0.47", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "12566", "train_lr": "8.13783e-05", "train_gnorm": "1.557", "train_clip": "0", "train_loss_scale": "4096", "train_train_wall": "509", "train_gb_free": "2.9", "train_wall": "21354"}
[2025-07-04 23:19:05,881][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 23:19:05,881][fairseq.trainer][INFO] - begin training epoch 53
[2025-07-04 23:19:05,881][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 23:20:19,168][train_inner][INFO] - {"epoch": 53, "update": 52.14, "loss": "5.532", "nll_loss": "0.015", "loss_recon": "0.108", "loss_info_nce": "4.456", "ppl": "1.01", "wps": "1391.6", "ups": "0.47", "wpb": "2953.7", "bsz": "386.2", "num_updates": "12600", "lr": "8.13043e-05", "gnorm": "1.622", "clip": "0", "loss_scale": "4096", "train_wall": "420", "gb_free": "2.9", "wall": "21428"}
[2025-07-04 23:27:18,323][train_inner][INFO] - {"epoch": 53, "update": 52.967, "loss": "5.524", "nll_loss": "0.015", "loss_recon": "0.107", "loss_info_nce": "4.449", "ppl": "1.01", "wps": "1414.3", "ups": "0.48", "wpb": "2963.9", "bsz": "386.8", "num_updates": "12800", "lr": "8.08696e-05", "gnorm": "1.578", "clip": "0", "loss_scale": "4096", "train_wall": "418", "gb_free": "2.9", "wall": "21847"}
[2025-07-04 23:27:33,599][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 53 @ 12808 updates
[2025-07-04 23:27:33,599][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint53.pt
[2025-07-04 23:27:34,968][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint53.pt
[2025-07-04 23:27:35,516][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint53.pt (epoch 53 @ 12808 updates, score None) (writing took 1.9158805000006396 seconds)
[2025-07-04 23:27:35,516][fairseq_cli.train][INFO] - end of epoch 53 (average epoch stats below)
[2025-07-04 23:27:35,526][train][INFO] - {"epoch": 53, "train_loss": "5.524", "train_nll_loss": "0.015", "train_loss_recon": "0.107", "train_loss_info_nce": "4.45", "train_ppl": "1.01", "train_wps": "1403.4", "train_ups": "0.47", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "12808", "train_lr": "8.08522e-05", "train_gnorm": "1.691", "train_clip": "0", "train_loss_scale": "4096", "train_train_wall": "504", "train_gb_free": "2.9", "train_wall": "21864"}
[2025-07-04 23:27:35,743][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 23:27:35,743][fairseq.trainer][INFO] - begin training epoch 54
[2025-07-04 23:27:35,743][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 23:35:38,901][train_inner][INFO] - {"epoch": 54, "update": 53.793, "loss": "5.502", "nll_loss": "0.015", "loss_recon": "0.107", "loss_info_nce": "4.43", "ppl": "1.01", "wps": "1181.6", "ups": "0.4", "wpb": "2957.4", "bsz": "384.8", "num_updates": "13000", "lr": "8.04348e-05", "gnorm": "1.609", "clip": "0", "loss_scale": "4096", "train_wall": "494", "gb_free": "2.9", "wall": "22347"}
[2025-07-04 23:37:44,799][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 54 @ 13050 updates
[2025-07-04 23:37:44,800][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint54.pt
[2025-07-04 23:37:46,274][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint54.pt
[2025-07-04 23:37:47,105][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint54.pt (epoch 54 @ 13050 updates, score None) (writing took 2.3060248000001593 seconds)
[2025-07-04 23:37:47,105][fairseq_cli.train][INFO] - end of epoch 54 (average epoch stats below)
[2025-07-04 23:37:47,115][train][INFO] - {"epoch": 54, "train_loss": "5.499", "train_nll_loss": "0.015", "train_loss_recon": "0.107", "train_loss_info_nce": "4.428", "train_ppl": "1.01", "train_wps": "1170", "train_ups": "0.4", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "13050", "train_lr": "8.03261e-05", "train_gnorm": "1.601", "train_clip": "0", "train_loss_scale": "4096", "train_train_wall": "605", "train_gb_free": "2.9", "train_wall": "22476"}
[2025-07-04 23:37:47,490][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 23:37:47,490][fairseq.trainer][INFO] - begin training epoch 55
[2025-07-04 23:37:47,490][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 23:43:08,072][train_inner][INFO] - {"epoch": 55, "update": 54.62, "loss": "5.486", "nll_loss": "0.015", "loss_recon": "0.107", "loss_info_nce": "4.414", "ppl": "1.01", "wps": "1316", "ups": "0.45", "wpb": "2955.5", "bsz": "385.6", "num_updates": "13200", "lr": "8e-05", "gnorm": "1.54", "clip": "0", "loss_scale": "4096", "train_wall": "442", "gb_free": "2.9", "wall": "22797"}
[2025-07-04 23:46:47,251][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-04 23:47:39,577][valid][INFO] - {"epoch": 55, "valid_loss": "5.147", "valid_nll_loss": "0.014", "valid_loss_recon": "0.1", "valid_loss_info_nce": "4.149", "valid_ppl": "1.01", "valid_wps": "1570.7", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "13292", "valid_best_loss": "5.147"}
[2025-07-04 23:47:39,585][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 55 @ 13292 updates
[2025-07-04 23:47:39,587][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint55.pt
[2025-07-04 23:47:41,069][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint55.pt
[2025-07-04 23:47:42,864][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint55.pt (epoch 55 @ 13292 updates, score 5.147) (writing took 3.2864283000017167 seconds)
[2025-07-04 23:47:42,864][fairseq_cli.train][INFO] - end of epoch 55 (average epoch stats below)
[2025-07-04 23:47:42,883][train][INFO] - {"epoch": 55, "train_loss": "5.478", "train_nll_loss": "0.015", "train_loss_recon": "0.107", "train_loss_info_nce": "4.41", "train_ppl": "1.01", "train_wps": "1201.1", "train_ups": "0.41", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "13292", "train_lr": "7.98e-05", "train_gnorm": "1.56", "train_clip": "0", "train_loss_scale": "4096", "train_train_wall": "535", "train_gb_free": "2.9", "train_wall": "23071"}
[2025-07-04 23:47:43,286][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 23:47:43,286][fairseq.trainer][INFO] - begin training epoch 56
[2025-07-04 23:47:43,286][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 23:52:43,508][train_inner][INFO] - {"epoch": 56, "update": 55.446, "loss": "5.469", "nll_loss": "0.015", "loss_recon": "0.106", "loss_info_nce": "4.406", "ppl": "1.01", "wps": "1026.3", "ups": "0.35", "wpb": "2952.8", "bsz": "386.2", "num_updates": "13400", "lr": "7.95652e-05", "gnorm": "1.699", "clip": "0", "loss_scale": "4096", "train_wall": "515", "gb_free": "2.9", "wall": "23372"}
[2025-07-04 23:58:29,622][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 56 @ 13534 updates
[2025-07-04 23:58:29,622][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint56.pt
[2025-07-04 23:58:31,130][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint56.pt
[2025-07-04 23:58:31,978][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint56.pt (epoch 56 @ 13534 updates, score None) (writing took 2.3523731999994197 seconds)
[2025-07-04 23:58:31,978][fairseq_cli.train][INFO] - end of epoch 56 (average epoch stats below)
[2025-07-04 23:58:31,978][train][INFO] - {"epoch": 56, "train_loss": "5.466", "train_nll_loss": "0.015", "train_loss_recon": "0.107", "train_loss_info_nce": "4.4", "train_ppl": "1.01", "train_wps": "1102.4", "train_ups": "0.37", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "13534", "train_lr": "7.92739e-05", "train_gnorm": "1.509", "train_clip": "0", "train_loss_scale": "4096", "train_train_wall": "642", "train_gb_free": "2.9", "train_wall": "23720"}
[2025-07-04 23:58:32,347][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 23:58:32,347][fairseq.trainer][INFO] - begin training epoch 57
[2025-07-04 23:58:32,347][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 00:01:24,083][train_inner][INFO] - {"epoch": 57, "update": 56.273, "loss": "5.462", "nll_loss": "0.015", "loss_recon": "0.107", "loss_info_nce": "4.394", "ppl": "1.01", "wps": "1134.4", "ups": "0.38", "wpb": "2952.8", "bsz": "386.6", "num_updates": "13600", "lr": "7.91304e-05", "gnorm": "1.379", "clip": "0", "loss_scale": "4096", "train_wall": "513", "gb_free": "2.9", "wall": "23893"}
[2025-07-05 00:08:52,705][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 57 @ 13776 updates
[2025-07-05 00:08:52,705][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint57.pt
[2025-07-05 00:08:54,207][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint57.pt
[2025-07-05 00:08:55,058][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint57.pt (epoch 57 @ 13776 updates, score None) (writing took 2.3481159999973897 seconds)
[2025-07-05 00:08:55,058][fairseq_cli.train][INFO] - end of epoch 57 (average epoch stats below)
[2025-07-05 00:08:55,066][train][INFO] - {"epoch": 57, "train_loss": "5.457", "train_nll_loss": "0.015", "train_loss_recon": "0.107", "train_loss_info_nce": "4.392", "train_ppl": "1.01", "train_wps": "1148.4", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "13776", "train_lr": "7.87478e-05", "train_gnorm": "1.493", "train_clip": "0", "train_loss_scale": "4096", "train_train_wall": "614", "train_gb_free": "2.9", "train_wall": "24344"}
[2025-07-05 00:08:55,434][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 00:08:55,440][fairseq.trainer][INFO] - begin training epoch 58
[2025-07-05 00:08:55,441][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 00:10:00,465][train_inner][INFO] - {"epoch": 58, "update": 57.099, "loss": "5.454", "nll_loss": "0.015", "loss_recon": "0.106", "loss_info_nce": "4.389", "ppl": "1.01", "wps": "1145.4", "ups": "0.39", "wpb": "2957.4", "bsz": "384.8", "num_updates": "13800", "lr": "7.86957e-05", "gnorm": "1.481", "clip": "0", "loss_scale": "4096", "train_wall": "508", "gb_free": "2.9", "wall": "24409"}
[2025-07-05 00:18:08,182][train_inner][INFO] - {"epoch": 58, "update": 57.926, "loss": "5.444", "nll_loss": "0.015", "loss_recon": "0.106", "loss_info_nce": "4.379", "ppl": "1.01", "wps": "1216.2", "ups": "0.41", "wpb": "2965.8", "bsz": "386.3", "num_updates": "14000", "lr": "7.82609e-05", "gnorm": "1.621", "clip": "0", "loss_scale": "8192", "train_wall": "485", "gb_free": "2.9", "wall": "24897"}
[2025-07-05 00:18:51,416][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 58 @ 14018 updates
[2025-07-05 00:18:51,417][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint58.pt
[2025-07-05 00:18:52,917][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint58.pt
[2025-07-05 00:18:53,765][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint58.pt (epoch 58 @ 14018 updates, score None) (writing took 2.3484625999990385 seconds)
[2025-07-05 00:18:53,765][fairseq_cli.train][INFO] - end of epoch 58 (average epoch stats below)
[2025-07-05 00:18:53,765][train][INFO] - {"epoch": 58, "train_loss": "5.442", "train_nll_loss": "0.015", "train_loss_recon": "0.106", "train_loss_info_nce": "4.378", "train_ppl": "1.01", "train_wps": "1195.2", "train_ups": "0.4", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "14018", "train_lr": "7.82217e-05", "train_gnorm": "1.589", "train_clip": "0", "train_loss_scale": "8192", "train_train_wall": "590", "train_gb_free": "2.9", "train_wall": "24942"}
[2025-07-05 00:18:54,125][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 00:18:54,140][fairseq.trainer][INFO] - begin training epoch 59
[2025-07-05 00:18:54,140][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 00:26:42,478][train_inner][INFO] - {"epoch": 59, "update": 58.752, "loss": "5.432", "nll_loss": "0.015", "loss_recon": "0.106", "loss_info_nce": "4.37", "ppl": "1.01", "wps": "1148.8", "ups": "0.39", "wpb": "2953.7", "bsz": "386", "num_updates": "14200", "lr": "7.78261e-05", "gnorm": "1.513", "clip": "0", "loss_scale": "8192", "train_wall": "505", "gb_free": "2.9", "wall": "25411"}
[2025-07-05 00:28:59,799][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 59 @ 14260 updates
[2025-07-05 00:28:59,799][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint59.pt
[2025-07-05 00:29:01,317][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint59.pt
[2025-07-05 00:29:02,178][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint59.pt (epoch 59 @ 14260 updates, score None) (writing took 2.3775307000032626 seconds)
[2025-07-05 00:29:02,178][fairseq_cli.train][INFO] - end of epoch 59 (average epoch stats below)
[2025-07-05 00:29:02,178][train][INFO] - {"epoch": 59, "train_loss": "5.433", "train_nll_loss": "0.015", "train_loss_recon": "0.106", "train_loss_info_nce": "4.371", "train_ppl": "1.01", "train_wps": "1176.1", "train_ups": "0.4", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "14260", "train_lr": "7.76957e-05", "train_gnorm": "1.609", "train_clip": "0", "train_loss_scale": "8192", "train_train_wall": "599", "train_gb_free": "2.9", "train_wall": "25551"}
[2025-07-05 00:29:02,563][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 00:29:02,573][fairseq.trainer][INFO] - begin training epoch 60
[2025-07-05 00:29:02,573][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 00:35:06,762][train_inner][INFO] - {"epoch": 60, "update": 59.579, "loss": "5.422", "nll_loss": "0.015", "loss_recon": "0.106", "loss_info_nce": "4.361", "ppl": "1.01", "wps": "1172.2", "ups": "0.4", "wpb": "2955.5", "bsz": "385.3", "num_updates": "14400", "lr": "7.73913e-05", "gnorm": "1.51", "clip": "0", "loss_scale": "8192", "train_wall": "496", "gb_free": "2.9", "wall": "25915"}
[2025-07-05 00:39:25,065][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-05 00:40:17,930][valid][INFO] - {"epoch": 60, "valid_loss": "5.125", "valid_nll_loss": "0.014", "valid_loss_recon": "0.102", "valid_loss_info_nce": "4.108", "valid_ppl": "1.01", "valid_wps": "1553.8", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "14502", "valid_best_loss": "5.125"}
[2025-07-05 00:40:17,930][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 60 @ 14502 updates
[2025-07-05 00:40:17,930][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint60.pt
[2025-07-05 00:40:19,442][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint60.pt
[2025-07-05 00:40:21,014][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint60.pt (epoch 60 @ 14502 updates, score 5.125) (writing took 3.0798620999994455 seconds)
[2025-07-05 00:40:21,014][fairseq_cli.train][INFO] - end of epoch 60 (average epoch stats below)
[2025-07-05 00:40:21,024][train][INFO] - {"epoch": 60, "train_loss": "5.419", "train_nll_loss": "0.015", "train_loss_recon": "0.106", "train_loss_info_nce": "4.359", "train_ppl": "1.01", "train_wps": "1054.1", "train_ups": "0.36", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "14502", "train_lr": "7.71696e-05", "train_gnorm": "1.545", "train_clip": "0", "train_loss_scale": "8192", "train_train_wall": "616", "train_gb_free": "2.9", "train_wall": "26229"}
[2025-07-05 00:40:21,368][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 00:40:21,376][fairseq.trainer][INFO] - begin training epoch 61
[2025-07-05 00:40:21,376][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 00:44:36,231][train_inner][INFO] - {"epoch": 61, "update": 60.405, "loss": "5.418", "nll_loss": "0.015", "loss_recon": "0.106", "loss_info_nce": "4.359", "ppl": "1.01", "wps": "1037.7", "ups": "0.35", "wpb": "2954.6", "bsz": "386.2", "num_updates": "14600", "lr": "7.69565e-05", "gnorm": "1.622", "clip": "0", "loss_scale": "8192", "train_wall": "507", "gb_free": "2.9", "wall": "26485"}
[2025-07-05 00:50:40,534][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 61 @ 14744 updates
[2025-07-05 00:50:40,534][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint61.pt
[2025-07-05 00:50:41,989][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint61.pt
[2025-07-05 00:50:42,836][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint61.pt (epoch 61 @ 14744 updates, score None) (writing took 2.3100664999983564 seconds)
[2025-07-05 00:50:42,836][fairseq_cli.train][INFO] - end of epoch 61 (average epoch stats below)
[2025-07-05 00:50:42,852][train][INFO] - {"epoch": 61, "train_loss": "5.409", "train_nll_loss": "0.015", "train_loss_recon": "0.106", "train_loss_info_nce": "4.35", "train_ppl": "1.01", "train_wps": "1150.7", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "14744", "train_lr": "7.66435e-05", "train_gnorm": "1.414", "train_clip": "0", "train_loss_scale": "8192", "train_train_wall": "612", "train_gb_free": "2.9", "train_wall": "26851"}
[2025-07-05 00:50:43,246][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 00:50:43,246][fairseq.trainer][INFO] - begin training epoch 62
[2025-07-05 00:50:43,246][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 00:53:09,879][train_inner][INFO] - {"epoch": 62, "update": 61.231, "loss": "5.402", "nll_loss": "0.015", "loss_recon": "0.106", "loss_info_nce": "4.344", "ppl": "1.01", "wps": "1151.6", "ups": "0.39", "wpb": "2957.4", "bsz": "385.1", "num_updates": "14800", "lr": "7.65217e-05", "gnorm": "1.454", "clip": "0", "loss_scale": "8192", "train_wall": "505", "gb_free": "2.9", "wall": "26998"}
[2025-07-05 01:01:01,764][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 62 @ 14986 updates
[2025-07-05 01:01:01,764][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint62.pt
[2025-07-05 01:01:03,317][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint62.pt
[2025-07-05 01:01:04,101][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint62.pt (epoch 62 @ 14986 updates, score None) (writing took 2.331068800001958 seconds)
[2025-07-05 01:01:04,101][fairseq_cli.train][INFO] - end of epoch 62 (average epoch stats below)
[2025-07-05 01:01:04,142][train][INFO] - {"epoch": 62, "train_loss": "5.39", "train_nll_loss": "0.015", "train_loss_recon": "0.106", "train_loss_info_nce": "4.335", "train_ppl": "1.01", "train_wps": "1151.8", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "14986", "train_lr": "7.61174e-05", "train_gnorm": "1.442", "train_clip": "0", "train_loss_scale": "8192", "train_train_wall": "611", "train_gb_free": "2.9", "train_wall": "27473"}
[2025-07-05 01:01:04,538][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 01:01:04,538][fairseq.trainer][INFO] - begin training epoch 63
[2025-07-05 01:01:04,538][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 01:01:44,313][train_inner][INFO] - {"epoch": 63, "update": 62.058, "loss": "5.389", "nll_loss": "0.015", "loss_recon": "0.105", "loss_info_nce": "4.335", "ppl": "1.01", "wps": "1148.7", "ups": "0.39", "wpb": "2954.6", "bsz": "385.6", "num_updates": "15000", "lr": "7.6087e-05", "gnorm": "1.442", "clip": "0", "loss_scale": "8192", "train_wall": "505", "gb_free": "2.9", "wall": "27513"}
[2025-07-05 01:10:12,990][train_inner][INFO] - {"epoch": 63, "update": 62.884, "loss": "5.382", "nll_loss": "0.015", "loss_recon": "0.105", "loss_info_nce": "4.328", "ppl": "1.01", "wps": "1164.6", "ups": "0.39", "wpb": "2962.1", "bsz": "387.4", "num_updates": "15200", "lr": "7.56522e-05", "gnorm": "1.45", "clip": "0", "loss_scale": "8192", "train_wall": "506", "gb_free": "2.9", "wall": "28021"}
[2025-07-05 01:11:21,165][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 63 @ 15228 updates
[2025-07-05 01:11:21,165][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint63.pt
[2025-07-05 01:11:22,645][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint63.pt
[2025-07-05 01:11:23,510][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint63.pt (epoch 63 @ 15228 updates, score None) (writing took 2.3504279000007955 seconds)
[2025-07-05 01:11:23,510][fairseq_cli.train][INFO] - end of epoch 63 (average epoch stats below)
[2025-07-05 01:11:23,520][train][INFO] - {"epoch": 63, "train_loss": "5.385", "train_nll_loss": "0.015", "train_loss_recon": "0.105", "train_loss_info_nce": "4.33", "train_ppl": "1.01", "train_wps": "1155.3", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "15228", "train_lr": "7.55913e-05", "train_gnorm": "1.46", "train_clip": "0", "train_loss_scale": "8192", "train_train_wall": "610", "train_gb_free": "2.9", "train_wall": "28092"}
[2025-07-05 01:11:23,916][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 01:11:23,916][fairseq.trainer][INFO] - begin training epoch 64
[2025-07-05 01:11:23,916][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 01:18:43,573][train_inner][INFO] - {"epoch": 64, "update": 63.711, "loss": "5.379", "nll_loss": "0.015", "loss_recon": "0.106", "loss_info_nce": "4.324", "ppl": "1.01", "wps": "1158.5", "ups": "0.39", "wpb": "2957.4", "bsz": "385.1", "num_updates": "15400", "lr": "7.52174e-05", "gnorm": "1.491", "clip": "0", "loss_scale": "8192", "train_wall": "502", "gb_free": "2.9", "wall": "28532"}
[2025-07-05 01:21:37,341][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 64 @ 15470 updates
[2025-07-05 01:21:37,341][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint64.pt
[2025-07-05 01:21:38,794][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint64.pt
[2025-07-05 01:21:39,641][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint64.pt (epoch 64 @ 15470 updates, score None) (writing took 2.2977107999977306 seconds)
[2025-07-05 01:21:39,641][fairseq_cli.train][INFO] - end of epoch 64 (average epoch stats below)
[2025-07-05 01:21:39,641][train][INFO] - {"epoch": 64, "train_loss": "5.372", "train_nll_loss": "0.014", "train_loss_recon": "0.105", "train_loss_info_nce": "4.319", "train_ppl": "1.01", "train_wps": "1161.4", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "15470", "train_lr": "7.50652e-05", "train_gnorm": "1.492", "train_clip": "0", "train_loss_scale": "8192", "train_train_wall": "607", "train_gb_free": "2.9", "train_wall": "28708"}
[2025-07-05 01:21:39,963][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 01:21:39,963][fairseq.trainer][INFO] - begin training epoch 65
[2025-07-05 01:21:39,963][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 01:27:12,694][train_inner][INFO] - {"epoch": 65, "update": 64.537, "loss": "5.367", "nll_loss": "0.014", "loss_recon": "0.105", "loss_info_nce": "4.316", "ppl": "1.01", "wps": "1161.8", "ups": "0.39", "wpb": "2957.4", "bsz": "385", "num_updates": "15600", "lr": "7.47826e-05", "gnorm": "1.561", "clip": "0", "loss_scale": "8192", "train_wall": "501", "gb_free": "2.9", "wall": "29041"}
[2025-07-05 01:31:55,351][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-05 01:32:43,615][valid][INFO] - {"epoch": 65, "valid_loss": "5.057", "valid_nll_loss": "0.014", "valid_loss_recon": "0.099", "valid_loss_info_nce": "4.067", "valid_ppl": "1.01", "valid_wps": "1700.4", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "15712", "valid_best_loss": "5.057"}
[2025-07-05 01:32:43,615][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 65 @ 15712 updates
[2025-07-05 01:32:43,625][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint65.pt
[2025-07-05 01:32:45,163][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint65.pt
[2025-07-05 01:32:46,992][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint65.pt (epoch 65 @ 15712 updates, score 5.057) (writing took 3.3755062999989605 seconds)
[2025-07-05 01:32:46,992][fairseq_cli.train][INFO] - end of epoch 65 (average epoch stats below)
[2025-07-05 01:32:46,992][train][INFO] - {"epoch": 65, "train_loss": "5.362", "train_nll_loss": "0.014", "train_loss_recon": "0.105", "train_loss_info_nce": "4.311", "train_ppl": "1.01", "train_wps": "1072.2", "train_ups": "0.36", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "15712", "train_lr": "7.45391e-05", "train_gnorm": "1.409", "train_clip": "0", "train_loss_scale": "8192", "train_train_wall": "609", "train_gb_free": "2.9", "train_wall": "29375"}
[2025-07-05 01:32:47,351][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 01:32:47,351][fairseq.trainer][INFO] - begin training epoch 66
[2025-07-05 01:32:47,351][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 01:36:35,933][train_inner][INFO] - {"epoch": 66, "update": 65.364, "loss": "5.353", "nll_loss": "0.014", "loss_recon": "0.105", "loss_info_nce": "4.305", "ppl": "1.01", "wps": "1048.8", "ups": "0.36", "wpb": "2953.7", "bsz": "386.2", "num_updates": "15800", "lr": "7.43478e-05", "gnorm": "1.193", "clip": "0", "loss_scale": "8192", "train_wall": "505", "gb_free": "2.9", "wall": "29604"}
[2025-07-05 01:43:06,942][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 66 @ 15954 updates
[2025-07-05 01:43:06,957][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint66.pt
[2025-07-05 01:43:08,434][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint66.pt
[2025-07-05 01:43:09,225][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint66.pt (epoch 66 @ 15954 updates, score None) (writing took 2.272004200000083 seconds)
[2025-07-05 01:43:09,225][fairseq_cli.train][INFO] - end of epoch 66 (average epoch stats below)
[2025-07-05 01:43:09,225][train][INFO] - {"epoch": 66, "train_loss": "5.351", "train_nll_loss": "0.014", "train_loss_recon": "0.105", "train_loss_info_nce": "4.301", "train_ppl": "1.01", "train_wps": "1150", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "15954", "train_lr": "7.4013e-05", "train_gnorm": "1.346", "train_clip": "0", "train_loss_scale": "16384", "train_train_wall": "613", "train_gb_free": "2.9", "train_wall": "29998"}
[2025-07-05 01:43:09,635][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 01:43:09,635][fairseq.trainer][INFO] - begin training epoch 67
[2025-07-05 01:43:09,635][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 01:45:11,195][train_inner][INFO] - {"epoch": 67, "update": 66.19, "loss": "5.352", "nll_loss": "0.014", "loss_recon": "0.105", "loss_info_nce": "4.301", "ppl": "1.01", "wps": "1146.5", "ups": "0.39", "wpb": "2953.7", "bsz": "385.9", "num_updates": "16000", "lr": "7.3913e-05", "gnorm": "1.423", "clip": "0", "loss_scale": "16384", "train_wall": "506", "gb_free": "2.9", "wall": "30120"}
[2025-07-05 01:53:29,148][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 67 @ 16196 updates
[2025-07-05 01:53:29,164][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint67.pt
[2025-07-05 01:53:30,668][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint67.pt
[2025-07-05 01:53:31,489][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint67.pt (epoch 67 @ 16196 updates, score None) (writing took 2.3317982999978994 seconds)
[2025-07-05 01:53:31,489][fairseq_cli.train][INFO] - end of epoch 67 (average epoch stats below)
[2025-07-05 01:53:31,504][train][INFO] - {"epoch": 67, "train_loss": "5.344", "train_nll_loss": "0.014", "train_loss_recon": "0.105", "train_loss_info_nce": "4.297", "train_ppl": "1.01", "train_wps": "1149.9", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "16196", "train_lr": "7.3487e-05", "train_gnorm": "1.398", "train_clip": "0", "train_loss_scale": "16384", "train_train_wall": "612", "train_gb_free": "2.9", "train_wall": "30620"}
[2025-07-05 01:53:31,883][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 01:53:31,898][fairseq.trainer][INFO] - begin training epoch 68
[2025-07-05 01:53:31,898][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 01:53:45,990][train_inner][INFO] - {"epoch": 68, "update": 67.017, "loss": "5.34", "nll_loss": "0.014", "loss_recon": "0.105", "loss_info_nce": "4.293", "ppl": "1.01", "wps": "1148.3", "ups": "0.39", "wpb": "2955.5", "bsz": "385.4", "num_updates": "16200", "lr": "7.34783e-05", "gnorm": "1.368", "clip": "0", "loss_scale": "16384", "train_wall": "506", "gb_free": "2.9", "wall": "30634"}
[2025-07-05 02:02:16,249][train_inner][INFO] - {"epoch": 68, "update": 67.843, "loss": "5.336", "nll_loss": "0.014", "loss_recon": "0.105", "loss_info_nce": "4.29", "ppl": "1.01", "wps": "1161.4", "ups": "0.39", "wpb": "2963", "bsz": "387.1", "num_updates": "16400", "lr": "7.30435e-05", "gnorm": "1.384", "clip": "0", "loss_scale": "16384", "train_wall": "507", "gb_free": "2.9", "wall": "31145"}
[2025-07-05 02:03:50,008][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 68 @ 16438 updates
[2025-07-05 02:03:50,008][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint68.pt
[2025-07-05 02:03:51,455][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint68.pt
[2025-07-05 02:03:52,289][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint68.pt (epoch 68 @ 16438 updates, score None) (writing took 2.2852180000008957 seconds)
[2025-07-05 02:03:52,289][fairseq_cli.train][INFO] - end of epoch 68 (average epoch stats below)
[2025-07-05 02:03:52,289][train][INFO] - {"epoch": 68, "train_loss": "5.335", "train_nll_loss": "0.014", "train_loss_recon": "0.105", "train_loss_info_nce": "4.289", "train_ppl": "1.01", "train_wps": "1152.6", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "16438", "train_lr": "7.29609e-05", "train_gnorm": "1.369", "train_clip": "0", "train_loss_scale": "16384", "train_train_wall": "612", "train_gb_free": "2.9", "train_wall": "31241"}
[2025-07-05 02:03:52,626][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 02:03:52,642][fairseq.trainer][INFO] - begin training epoch 69
[2025-07-05 02:03:52,642][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 02:10:50,990][train_inner][INFO] - {"epoch": 69, "update": 68.669, "loss": "5.326", "nll_loss": "0.014", "loss_recon": "0.104", "loss_info_nce": "4.283", "ppl": "1.01", "wps": "1147.6", "ups": "0.39", "wpb": "2953.7", "bsz": "386", "num_updates": "16600", "lr": "7.26087e-05", "gnorm": "1.487", "clip": "0", "loss_scale": "16384", "train_wall": "506", "gb_free": "2.9", "wall": "31659"}
[2025-07-05 02:14:11,554][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 69 @ 16680 updates
[2025-07-05 02:14:11,564][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint69.pt
[2025-07-05 02:14:13,073][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint69.pt
[2025-07-05 02:14:13,856][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint69.pt (epoch 69 @ 16680 updates, score None) (writing took 2.2982271000000765 seconds)
[2025-07-05 02:14:13,856][fairseq_cli.train][INFO] - end of epoch 69 (average epoch stats below)
[2025-07-05 02:14:13,866][train][INFO] - {"epoch": 69, "train_loss": "5.328", "train_nll_loss": "0.014", "train_loss_recon": "0.105", "train_loss_info_nce": "4.283", "train_ppl": "1.01", "train_wps": "1151.2", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "16680", "train_lr": "7.24348e-05", "train_gnorm": "1.383", "train_clip": "0", "train_loss_scale": "16384", "train_train_wall": "612", "train_gb_free": "2.9", "train_wall": "31862"}
[2025-07-05 02:14:14,234][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 02:14:14,250][fairseq.trainer][INFO] - begin training epoch 70
[2025-07-05 02:14:14,250][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 02:19:23,975][train_inner][INFO] - {"epoch": 70, "update": 69.496, "loss": "5.324", "nll_loss": "0.014", "loss_recon": "0.104", "loss_info_nce": "4.28", "ppl": "1.01", "wps": "1153.7", "ups": "0.39", "wpb": "2959.3", "bsz": "384.7", "num_updates": "16800", "lr": "7.21739e-05", "gnorm": "1.502", "clip": "0", "loss_scale": "16384", "train_wall": "504", "gb_free": "2.9", "wall": "32172"}
[2025-07-05 02:24:33,262][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-05 02:25:24,963][valid][INFO] - {"epoch": 70, "valid_loss": "5.049", "valid_nll_loss": "0.014", "valid_loss_recon": "0.101", "valid_loss_info_nce": "4.04", "valid_ppl": "1.01", "valid_wps": "1588.2", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "16922", "valid_best_loss": "5.049"}
[2025-07-05 02:25:24,979][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 70 @ 16922 updates
[2025-07-05 02:25:24,979][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint70.pt
[2025-07-05 02:25:26,463][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint70.pt
[2025-07-05 02:25:28,106][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint70.pt (epoch 70 @ 16922 updates, score 5.049) (writing took 3.1278960999989067 seconds)
[2025-07-05 02:25:28,106][fairseq_cli.train][INFO] - end of epoch 70 (average epoch stats below)
[2025-07-05 02:25:28,132][train][INFO] - {"epoch": 70, "train_loss": "5.319", "train_nll_loss": "0.014", "train_loss_recon": "0.104", "train_loss_info_nce": "4.276", "train_ppl": "1.01", "train_wps": "1061.3", "train_ups": "0.36", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "16922", "train_lr": "7.19087e-05", "train_gnorm": "1.465", "train_clip": "0", "train_loss_scale": "16384", "train_train_wall": "612", "train_gb_free": "2.9", "train_wall": "32537"}
[2025-07-05 02:25:28,485][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 02:25:28,485][fairseq.trainer][INFO] - begin training epoch 71
[2025-07-05 02:25:28,485][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 02:28:51,282][train_inner][INFO] - {"epoch": 71, "update": 70.322, "loss": "5.32", "nll_loss": "0.014", "loss_recon": "0.105", "loss_info_nce": "4.274", "ppl": "1.01", "wps": "1040.3", "ups": "0.35", "wpb": "2950.9", "bsz": "386.3", "num_updates": "17000", "lr": "7.17391e-05", "gnorm": "1.422", "clip": "0", "loss_scale": "16384", "train_wall": "506", "gb_free": "2.9", "wall": "32740"}
[2025-07-05 02:35:46,823][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 71 @ 17164 updates
[2025-07-05 02:35:46,823][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint71.pt
[2025-07-05 02:35:48,365][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint71.pt
[2025-07-05 02:35:49,265][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint71.pt (epoch 71 @ 17164 updates, score None) (writing took 2.4356850999974995 seconds)
[2025-07-05 02:35:49,265][fairseq_cli.train][INFO] - end of epoch 71 (average epoch stats below)
[2025-07-05 02:35:49,265][train][INFO] - {"epoch": 71, "train_loss": "5.316", "train_nll_loss": "0.014", "train_loss_recon": "0.104", "train_loss_info_nce": "4.273", "train_ppl": "1.01", "train_wps": "1152", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "17164", "train_lr": "7.13826e-05", "train_gnorm": "1.396", "train_clip": "0", "train_loss_scale": "16384", "train_train_wall": "612", "train_gb_free": "2.9", "train_wall": "33158"}
[2025-07-05 02:35:49,649][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 02:35:49,649][fairseq.trainer][INFO] - begin training epoch 72
[2025-07-05 02:35:49,649][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 02:37:25,140][train_inner][INFO] - {"epoch": 72, "update": 71.149, "loss": "5.312", "nll_loss": "0.014", "loss_recon": "0.104", "loss_info_nce": "4.27", "ppl": "1.01", "wps": "1150.7", "ups": "0.39", "wpb": "2956.5", "bsz": "385.4", "num_updates": "17200", "lr": "7.13043e-05", "gnorm": "1.169", "clip": "0", "loss_scale": "16384", "train_wall": "505", "gb_free": "2.9", "wall": "33254"}
[2025-07-05 02:45:56,871][train_inner][INFO] - {"epoch": 72, "update": 71.975, "loss": "5.307", "nll_loss": "0.014", "loss_recon": "0.104", "loss_info_nce": "4.264", "ppl": "1.01", "wps": "1159.1", "ups": "0.39", "wpb": "2965.8", "bsz": "386.8", "num_updates": "17400", "lr": "7.08696e-05", "gnorm": "1.532", "clip": "0", "loss_scale": "16384", "train_wall": "509", "gb_free": "2.9", "wall": "33765"}
[2025-07-05 02:46:09,199][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 72 @ 17406 updates
[2025-07-05 02:46:09,199][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint72.pt
[2025-07-05 02:46:10,638][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint72.pt
[2025-07-05 02:46:11,417][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint72.pt (epoch 72 @ 17406 updates, score None) (writing took 2.2143340000038734 seconds)
[2025-07-05 02:46:11,417][fairseq_cli.train][INFO] - end of epoch 72 (average epoch stats below)
[2025-07-05 02:46:11,417][train][INFO] - {"epoch": 72, "train_loss": "5.305", "train_nll_loss": "0.014", "train_loss_recon": "0.104", "train_loss_info_nce": "4.264", "train_ppl": "1.01", "train_wps": "1150.1", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "17406", "train_lr": "7.08565e-05", "train_gnorm": "1.458", "train_clip": "0", "train_loss_scale": "16384", "train_train_wall": "614", "train_gb_free": "2.9", "train_wall": "33780"}
[2025-07-05 02:46:11,821][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 02:46:11,821][fairseq.trainer][INFO] - begin training epoch 73
[2025-07-05 02:46:11,821][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 02:54:30,282][train_inner][INFO] - {"epoch": 73, "update": 72.802, "loss": "5.296", "nll_loss": "0.014", "loss_recon": "0.104", "loss_info_nce": "4.258", "ppl": "1.01", "wps": "1151.7", "ups": "0.39", "wpb": "2956.5", "bsz": "385.6", "num_updates": "17600", "lr": "7.04348e-05", "gnorm": "1.202", "clip": "0", "loss_scale": "16384", "train_wall": "505", "gb_free": "2.9", "wall": "34279"}
[2025-07-05 02:56:29,812][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 73 @ 17648 updates
[2025-07-05 02:56:29,812][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint73.pt
[2025-07-05 02:56:31,314][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint73.pt
[2025-07-05 02:56:32,171][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint73.pt (epoch 73 @ 17648 updates, score None) (writing took 2.3695418999996036 seconds)
[2025-07-05 02:56:32,171][fairseq_cli.train][INFO] - end of epoch 73 (average epoch stats below)
[2025-07-05 02:56:32,187][train][INFO] - {"epoch": 73, "train_loss": "5.296", "train_nll_loss": "0.014", "train_loss_recon": "0.104", "train_loss_info_nce": "4.257", "train_ppl": "1.01", "train_wps": "1152.7", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "17648", "train_lr": "7.03304e-05", "train_gnorm": "1.244", "train_clip": "0", "train_loss_scale": "16384", "train_train_wall": "612", "train_gb_free": "2.9", "train_wall": "34401"}
[2025-07-05 02:56:32,568][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 02:56:32,583][fairseq.trainer][INFO] - begin training epoch 74
[2025-07-05 02:56:32,583][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 03:03:05,006][train_inner][INFO] - {"epoch": 74, "update": 73.628, "loss": "5.298", "nll_loss": "0.014", "loss_recon": "0.104", "loss_info_nce": "4.255", "ppl": "1.01", "wps": "1148.4", "ups": "0.39", "wpb": "2955.5", "bsz": "385.2", "num_updates": "17800", "lr": "7e-05", "gnorm": "1.307", "clip": "0", "loss_scale": "16384", "train_wall": "506", "gb_free": "2.9", "wall": "34793"}
[2025-07-05 03:06:51,565][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 74 @ 17890 updates
[2025-07-05 03:06:51,565][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint74.pt
[2025-07-05 03:06:53,047][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint74.pt
[2025-07-05 03:06:53,881][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint74.pt (epoch 74 @ 17890 updates, score None) (writing took 2.3278911000015796 seconds)
[2025-07-05 03:06:53,881][fairseq_cli.train][INFO] - end of epoch 74 (average epoch stats below)
[2025-07-05 03:06:53,897][train][INFO] - {"epoch": 74, "train_loss": "5.292", "train_nll_loss": "0.014", "train_loss_recon": "0.104", "train_loss_info_nce": "4.253", "train_ppl": "1.01", "train_wps": "1150.9", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "17890", "train_lr": "6.98043e-05", "train_gnorm": "1.331", "train_clip": "0", "train_loss_scale": "16384", "train_train_wall": "612", "train_gb_free": "2.9", "train_wall": "35022"}
[2025-07-05 03:06:54,291][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 03:06:54,291][fairseq.trainer][INFO] - begin training epoch 75
[2025-07-05 03:06:54,291][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 03:11:37,593][train_inner][INFO] - {"epoch": 75, "update": 74.455, "loss": "5.284", "nll_loss": "0.014", "loss_recon": "0.104", "loss_info_nce": "4.249", "ppl": "1.01", "wps": "1152.5", "ups": "0.39", "wpb": "2953.7", "bsz": "386.4", "num_updates": "18000", "lr": "6.95652e-05", "gnorm": "1.335", "clip": "0", "loss_scale": "32768", "train_wall": "503", "gb_free": "2.9", "wall": "35306"}
[2025-07-05 03:17:11,527][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-05 03:18:03,495][valid][INFO] - {"epoch": 75, "valid_loss": "5.005", "valid_nll_loss": "0.014", "valid_loss_recon": "0.099", "valid_loss_info_nce": "4.017", "valid_ppl": "1.01", "valid_wps": "1580.1", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "18132", "valid_best_loss": "5.005"}
[2025-07-05 03:18:03,495][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 75 @ 18132 updates
[2025-07-05 03:18:03,495][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint75.pt
[2025-07-05 03:18:04,931][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint75.pt
[2025-07-05 03:18:06,498][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint75.pt (epoch 75 @ 18132 updates, score 5.005) (writing took 3.0010505999962334 seconds)
[2025-07-05 03:18:06,498][fairseq_cli.train][INFO] - end of epoch 75 (average epoch stats below)
[2025-07-05 03:18:06,514][train][INFO] - {"epoch": 75, "train_loss": "5.282", "train_nll_loss": "0.014", "train_loss_recon": "0.104", "train_loss_info_nce": "4.245", "train_ppl": "1.01", "train_wps": "1063.8", "train_ups": "0.36", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "18132", "train_lr": "6.92783e-05", "train_gnorm": "1.169", "train_clip": "0", "train_loss_scale": "32768", "train_train_wall": "610", "train_gb_free": "2.9", "train_wall": "35695"}
[2025-07-05 03:18:06,900][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 03:18:06,900][fairseq.trainer][INFO] - begin training epoch 76
[2025-07-05 03:18:06,900][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 03:21:04,512][train_inner][INFO] - {"epoch": 76, "update": 75.281, "loss": "5.277", "nll_loss": "0.014", "loss_recon": "0.103", "loss_info_nce": "4.242", "ppl": "1.01", "wps": "1042.4", "ups": "0.35", "wpb": "2954.6", "bsz": "385.1", "num_updates": "18200", "lr": "6.91304e-05", "gnorm": "1.151", "clip": "0", "loss_scale": "32768", "train_wall": "506", "gb_free": "2.9", "wall": "35873"}
[2025-07-05 03:28:26,056][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 76 @ 18374 updates
[2025-07-05 03:28:26,056][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint76.pt
[2025-07-05 03:28:28,970][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint76.pt
[2025-07-05 03:28:29,914][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint76.pt (epoch 76 @ 18374 updates, score None) (writing took 3.8642881999985548 seconds)
[2025-07-05 03:28:29,914][fairseq_cli.train][INFO] - end of epoch 76 (average epoch stats below)
[2025-07-05 03:28:29,930][train][INFO] - {"epoch": 76, "train_loss": "5.278", "train_nll_loss": "0.014", "train_loss_recon": "0.104", "train_loss_info_nce": "4.242", "train_ppl": "1.01", "train_wps": "1147.8", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "18374", "train_lr": "6.87522e-05", "train_gnorm": "1.281", "train_clip": "0", "train_loss_scale": "32768", "train_train_wall": "614", "train_gb_free": "2.9", "train_wall": "36318"}
[2025-07-05 03:28:30,298][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 03:28:30,298][fairseq.trainer][INFO] - begin training epoch 77
[2025-07-05 03:28:30,314][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 03:29:40,425][train_inner][INFO] - {"epoch": 77, "update": 76.107, "loss": "5.279", "nll_loss": "0.014", "loss_recon": "0.104", "loss_info_nce": "4.24", "ppl": "1.01", "wps": "1146.5", "ups": "0.39", "wpb": "2957.4", "bsz": "385.3", "num_updates": "18400", "lr": "6.86957e-05", "gnorm": "1.275", "clip": "0", "loss_scale": "32768", "train_wall": "507", "gb_free": "2.9", "wall": "36389"}
[2025-07-05 03:38:14,321][train_inner][INFO] - {"epoch": 77, "update": 76.934, "loss": "5.273", "nll_loss": "0.014", "loss_recon": "0.104", "loss_info_nce": "4.237", "ppl": "1.01", "wps": "1153.1", "ups": "0.39", "wpb": "2963", "bsz": "387.1", "num_updates": "18600", "lr": "6.82609e-05", "gnorm": "1.296", "clip": "0", "loss_scale": "32768", "train_wall": "513", "gb_free": "2.9", "wall": "36903"}
[2025-07-05 03:38:52,335][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 77 @ 18616 updates
[2025-07-05 03:38:52,335][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint77.pt
[2025-07-05 03:38:53,828][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint77.pt
[2025-07-05 03:38:54,664][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint77.pt (epoch 77 @ 18616 updates, score None) (writing took 2.3270186000008835 seconds)
[2025-07-05 03:38:54,664][fairseq_cli.train][INFO] - end of epoch 77 (average epoch stats below)
[2025-07-05 03:38:54,716][train][INFO] - {"epoch": 77, "train_loss": "5.272", "train_nll_loss": "0.014", "train_loss_recon": "0.104", "train_loss_info_nce": "4.236", "train_ppl": "1.01", "train_wps": "1145.3", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "18616", "train_lr": "6.82261e-05", "train_gnorm": "1.284", "train_clip": "0", "train_loss_scale": "32768", "train_train_wall": "618", "train_gb_free": "2.9", "train_wall": "36943"}
[2025-07-05 03:38:55,100][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 03:38:55,100][fairseq.trainer][INFO] - begin training epoch 78
[2025-07-05 03:38:55,100][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 03:46:48,016][train_inner][INFO] - {"epoch": 78, "update": 77.76, "loss": "5.264", "nll_loss": "0.014", "loss_recon": "0.103", "loss_info_nce": "4.229", "ppl": "1.01", "wps": "1150", "ups": "0.39", "wpb": "2953.7", "bsz": "385.9", "num_updates": "18800", "lr": "6.78261e-05", "gnorm": "1.277", "clip": "0", "loss_scale": "32768", "train_wall": "506", "gb_free": "2.9", "wall": "37416"}
[2025-07-05 03:49:13,569][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 78 @ 18858 updates
[2025-07-05 03:49:13,571][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint78.pt
[2025-07-05 03:49:15,063][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint78.pt
[2025-07-05 03:49:16,056][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint78.pt (epoch 78 @ 18858 updates, score None) (writing took 2.494905799998378 seconds)
[2025-07-05 03:49:16,066][fairseq_cli.train][INFO] - end of epoch 78 (average epoch stats below)
[2025-07-05 03:49:16,066][train][INFO] - {"epoch": 78, "train_loss": "5.263", "train_nll_loss": "0.014", "train_loss_recon": "0.103", "train_loss_info_nce": "4.229", "train_ppl": "1.01", "train_wps": "1151.6", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "18858", "train_lr": "6.77e-05", "train_gnorm": "1.265", "train_clip": "0", "train_loss_scale": "32768", "train_train_wall": "613", "train_gb_free": "2.9", "train_wall": "37565"}
[2025-07-05 03:49:16,451][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 03:49:16,466][fairseq.trainer][INFO] - begin training epoch 79
[2025-07-05 03:49:16,466][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 03:55:23,460][train_inner][INFO] - {"epoch": 79, "update": 78.587, "loss": "5.267", "nll_loss": "0.014", "loss_recon": "0.104", "loss_info_nce": "4.232", "ppl": "1.01", "wps": "1147.5", "ups": "0.39", "wpb": "2957.4", "bsz": "385.3", "num_updates": "19000", "lr": "6.73913e-05", "gnorm": "1.18", "clip": "0", "loss_scale": "32768", "train_wall": "507", "gb_free": "2.9", "wall": "37932"}
[2025-07-05 03:59:35,419][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 79 @ 19100 updates
[2025-07-05 03:59:35,419][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint79.pt
[2025-07-05 03:59:36,847][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint79.pt
[2025-07-05 03:59:37,751][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint79.pt (epoch 79 @ 19100 updates, score None) (writing took 2.3249062999966554 seconds)
[2025-07-05 03:59:37,751][fairseq_cli.train][INFO] - end of epoch 79 (average epoch stats below)
[2025-07-05 03:59:37,751][train][INFO] - {"epoch": 79, "train_loss": "5.263", "train_nll_loss": "0.014", "train_loss_recon": "0.103", "train_loss_info_nce": "4.229", "train_ppl": "1.01", "train_wps": "1151", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "19100", "train_lr": "6.71739e-05", "train_gnorm": "1.165", "train_clip": "0", "train_loss_scale": "32768", "train_train_wall": "611", "train_gb_free": "2.9", "train_wall": "38186"}
[2025-07-05 03:59:38,130][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 03:59:38,145][fairseq.trainer][INFO] - begin training epoch 80
[2025-07-05 03:59:38,145][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 04:03:56,721][train_inner][INFO] - {"epoch": 80, "update": 79.413, "loss": "5.253", "nll_loss": "0.014", "loss_recon": "0.103", "loss_info_nce": "4.222", "ppl": "1.01", "wps": "1151", "ups": "0.39", "wpb": "2953.7", "bsz": "386.2", "num_updates": "19200", "lr": "6.69565e-05", "gnorm": "1.165", "clip": "0", "loss_scale": "32768", "train_wall": "505", "gb_free": "2.9", "wall": "38445"}
[2025-07-05 04:09:55,628][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-05 04:10:48,043][valid][INFO] - {"epoch": 80, "valid_loss": "4.96", "valid_nll_loss": "0.013", "valid_loss_recon": "0.097", "valid_loss_info_nce": "3.992", "valid_ppl": "1.01", "valid_wps": "1571.4", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "19342", "valid_best_loss": "4.96"}
[2025-07-05 04:10:48,043][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 80 @ 19342 updates
[2025-07-05 04:10:48,043][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint80.pt
[2025-07-05 04:10:49,649][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint80.pt
[2025-07-05 04:10:51,271][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint80.pt (epoch 80 @ 19342 updates, score 4.96) (writing took 3.228868900005182 seconds)
[2025-07-05 04:10:51,271][fairseq_cli.train][INFO] - end of epoch 80 (average epoch stats below)
[2025-07-05 04:10:51,286][train][INFO] - {"epoch": 80, "train_loss": "5.252", "train_nll_loss": "0.014", "train_loss_recon": "0.103", "train_loss_info_nce": "4.221", "train_ppl": "1.01", "train_wps": "1062.4", "train_ups": "0.36", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "19342", "train_lr": "6.66478e-05", "train_gnorm": "1.217", "train_clip": "0", "train_loss_scale": "32768", "train_train_wall": "612", "train_gb_free": "2.9", "train_wall": "38860"}
[2025-07-05 04:10:51,702][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 04:10:51,702][fairseq.trainer][INFO] - begin training epoch 81
[2025-07-05 04:10:51,702][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 04:13:23,768][train_inner][INFO] - {"epoch": 81, "update": 80.24, "loss": "5.248", "nll_loss": "0.014", "loss_recon": "0.103", "loss_info_nce": "4.221", "ppl": "1.01", "wps": "1043.1", "ups": "0.35", "wpb": "2957.4", "bsz": "384.8", "num_updates": "19400", "lr": "6.65217e-05", "gnorm": "1.247", "clip": "0", "loss_scale": "32768", "train_wall": "505", "gb_free": "2.9", "wall": "39012"}
[2025-07-05 04:21:09,846][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 81 @ 19584 updates
[2025-07-05 04:21:09,862][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint81.pt
[2025-07-05 04:21:11,314][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint81.pt
[2025-07-05 04:21:12,114][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint81.pt (epoch 81 @ 19584 updates, score None) (writing took 2.2590094999977737 seconds)
[2025-07-05 04:21:12,114][fairseq_cli.train][INFO] - end of epoch 81 (average epoch stats below)
[2025-07-05 04:21:12,114][train][INFO] - {"epoch": 81, "train_loss": "5.246", "train_nll_loss": "0.014", "train_loss_recon": "0.103", "train_loss_info_nce": "4.216", "train_ppl": "1.01", "train_wps": "1152.6", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "19584", "train_lr": "6.61217e-05", "train_gnorm": "1.306", "train_clip": "0", "train_loss_scale": "32768", "train_train_wall": "611", "train_gb_free": "2.9", "train_wall": "39481"}
[2025-07-05 04:21:12,523][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 04:21:12,523][fairseq.trainer][INFO] - begin training epoch 82
[2025-07-05 04:21:12,523][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 04:21:57,180][train_inner][INFO] - {"epoch": 82, "update": 81.066, "loss": "5.247", "nll_loss": "0.014", "loss_recon": "0.103", "loss_info_nce": "4.214", "ppl": "1.01", "wps": "1151", "ups": "0.39", "wpb": "2954.6", "bsz": "385.6", "num_updates": "19600", "lr": "6.6087e-05", "gnorm": "1.372", "clip": "0", "loss_scale": "32768", "train_wall": "505", "gb_free": "2.9", "wall": "39526"}
[2025-07-05 04:30:27,888][train_inner][INFO] - {"epoch": 82, "update": 81.893, "loss": "5.241", "nll_loss": "0.014", "loss_recon": "0.103", "loss_info_nce": "4.21", "ppl": "1.01", "wps": "1160", "ups": "0.39", "wpb": "2962.1", "bsz": "387.4", "num_updates": "19800", "lr": "6.56522e-05", "gnorm": "1.141", "clip": "0", "loss_scale": "32768", "train_wall": "507", "gb_free": "2.9", "wall": "40036"}
[2025-07-05 04:31:31,463][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 82 @ 19826 updates
[2025-07-05 04:31:31,463][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint82.pt
[2025-07-05 04:31:32,967][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint82.pt
[2025-07-05 04:31:33,866][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint82.pt (epoch 82 @ 19826 updates, score None) (writing took 2.3976134000040474 seconds)
[2025-07-05 04:31:33,866][fairseq_cli.train][INFO] - end of epoch 82 (average epoch stats below)
[2025-07-05 04:31:33,876][train][INFO] - {"epoch": 82, "train_loss": "5.242", "train_nll_loss": "0.014", "train_loss_recon": "0.103", "train_loss_info_nce": "4.212", "train_ppl": "1.01", "train_wps": "1150.9", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "19826", "train_lr": "6.55957e-05", "train_gnorm": "1.231", "train_clip": "0", "train_loss_scale": "32768", "train_train_wall": "612", "train_gb_free": "2.9", "train_wall": "40102"}
[2025-07-05 04:31:34,316][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 04:31:34,326][fairseq.trainer][INFO] - begin training epoch 83
[2025-07-05 04:31:34,326][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 04:39:03,117][train_inner][INFO] - {"epoch": 83, "update": 82.719, "loss": "5.239", "nll_loss": "0.014", "loss_recon": "0.103", "loss_info_nce": "4.211", "ppl": "1.01", "wps": "1148", "ups": "0.39", "wpb": "2957.4", "bsz": "385.3", "num_updates": "20000", "lr": "6.52174e-05", "gnorm": "1.242", "clip": "0", "loss_scale": "65536", "train_wall": "506", "gb_free": "2.9", "wall": "40552"}
[2025-07-05 04:39:03,117][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-05 04:39:55,600][valid][INFO] - {"epoch": 83, "valid_loss": "4.93", "valid_nll_loss": "0.013", "valid_loss_recon": "0.094", "valid_loss_info_nce": "3.986", "valid_ppl": "1.01", "valid_wps": "1571.7", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "20000", "valid_best_loss": "4.93"}
[2025-07-05 04:42:46,818][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 83 @ 20068 updates
[2025-07-05 04:42:46,818][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint83.pt
[2025-07-05 04:42:48,360][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint83.pt
[2025-07-05 04:42:49,216][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint83.pt (epoch 83 @ 20068 updates, score None) (writing took 2.3890287000031094 seconds)
[2025-07-05 04:42:49,216][fairseq_cli.train][INFO] - end of epoch 83 (average epoch stats below)
[2025-07-05 04:42:49,216][train][INFO] - {"epoch": 83, "train_loss": "5.239", "train_nll_loss": "0.014", "train_loss_recon": "0.103", "train_loss_info_nce": "4.209", "train_ppl": "1.01", "train_wps": "1059.5", "train_ups": "0.36", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "20068", "train_lr": "6.50696e-05", "train_gnorm": "1.163", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "613", "train_gb_free": "2.9", "train_wall": "40778"}
[2025-07-05 04:42:49,585][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 04:42:49,600][fairseq.trainer][INFO] - begin training epoch 84
[2025-07-05 04:42:49,600][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 04:48:30,494][train_inner][INFO] - {"epoch": 84, "update": 83.545, "loss": "5.235", "nll_loss": "0.014", "loss_recon": "0.103", "loss_info_nce": "4.206", "ppl": "1.01", "wps": "1041.5", "ups": "0.35", "wpb": "2954.6", "bsz": "386", "num_updates": "20200", "lr": "6.47826e-05", "gnorm": "1.347", "clip": "0", "loss_scale": "65536", "train_wall": "506", "gb_free": "2.9", "wall": "41119"}
[2025-07-05 04:53:07,422][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 84 @ 20310 updates
[2025-07-05 04:53:07,422][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint84.pt
[2025-07-05 04:53:08,883][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint84.pt
[2025-07-05 04:53:09,766][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint84.pt (epoch 84 @ 20310 updates, score None) (writing took 2.350788800002192 seconds)
[2025-07-05 04:53:09,766][fairseq_cli.train][INFO] - end of epoch 84 (average epoch stats below)
[2025-07-05 04:53:09,782][train][INFO] - {"epoch": 84, "train_loss": "5.233", "train_nll_loss": "0.014", "train_loss_recon": "0.103", "train_loss_info_nce": "4.204", "train_ppl": "1.01", "train_wps": "1153.1", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "20310", "train_lr": "6.45435e-05", "train_gnorm": "1.255", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "611", "train_gb_free": "2.9", "train_wall": "41398"}
[2025-07-05 04:53:10,152][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 04:53:10,167][fairseq.trainer][INFO] - begin training epoch 85
[2025-07-05 04:53:10,167][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 04:57:02,271][train_inner][INFO] - {"epoch": 85, "update": 84.372, "loss": "5.234", "nll_loss": "0.014", "loss_recon": "0.103", "loss_info_nce": "4.205", "ppl": "1.01", "wps": "1154.6", "ups": "0.39", "wpb": "2954.6", "bsz": "385.4", "num_updates": "20400", "lr": "6.43478e-05", "gnorm": "1.127", "clip": "0", "loss_scale": "65536", "train_wall": "503", "gb_free": "2.9", "wall": "41631"}
[2025-07-05 05:03:28,877][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-05 05:04:20,663][valid][INFO] - {"epoch": 85, "valid_loss": "4.935", "valid_nll_loss": "0.013", "valid_loss_recon": "0.095", "valid_loss_info_nce": "3.981", "valid_ppl": "1.01", "valid_wps": "1585.5", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "20552", "valid_best_loss": "4.935"}
[2025-07-05 05:04:20,663][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 85 @ 20552 updates
[2025-07-05 05:04:20,673][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint85.pt
[2025-07-05 05:04:22,163][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint85.pt
[2025-07-05 05:04:23,668][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint85.pt (epoch 85 @ 20552 updates, score 4.935) (writing took 3.0061012000005576 seconds)
[2025-07-05 05:04:23,668][fairseq_cli.train][INFO] - end of epoch 85 (average epoch stats below)
[2025-07-05 05:04:23,684][train][INFO] - {"epoch": 85, "train_loss": "5.226", "train_nll_loss": "0.014", "train_loss_recon": "0.103", "train_loss_info_nce": "4.199", "train_ppl": "1.01", "train_wps": "1061.8", "train_ups": "0.36", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "20552", "train_lr": "6.40174e-05", "train_gnorm": "1.249", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "612", "train_gb_free": "2.9", "train_wall": "42072"}
[2025-07-05 05:04:24,062][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 05:04:24,078][fairseq.trainer][INFO] - begin training epoch 86
[2025-07-05 05:04:24,078][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 05:06:30,248][train_inner][INFO] - {"epoch": 86, "update": 85.198, "loss": "5.22", "nll_loss": "0.014", "loss_recon": "0.103", "loss_info_nce": "4.194", "ppl": "1.01", "wps": "1040.7", "ups": "0.35", "wpb": "2955.5", "bsz": "385.6", "num_updates": "20600", "lr": "6.3913e-05", "gnorm": "1.192", "clip": "0", "loss_scale": "65536", "train_wall": "507", "gb_free": "2.9", "wall": "42199"}
[2025-07-05 05:14:42,479][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 86 @ 20794 updates
[2025-07-05 05:14:42,479][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint86.pt
[2025-07-05 05:14:44,017][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint86.pt
[2025-07-05 05:14:44,873][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint86.pt (epoch 86 @ 20794 updates, score None) (writing took 2.386941700002353 seconds)
[2025-07-05 05:14:44,873][fairseq_cli.train][INFO] - end of epoch 86 (average epoch stats below)
[2025-07-05 05:14:44,873][train][INFO] - {"epoch": 86, "train_loss": "5.223", "train_nll_loss": "0.014", "train_loss_recon": "0.103", "train_loss_info_nce": "4.197", "train_ppl": "1.01", "train_wps": "1151.9", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "20794", "train_lr": "6.34913e-05", "train_gnorm": "1.076", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "612", "train_gb_free": "2.9", "train_wall": "42693"}
[2025-07-05 05:14:45,268][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 05:14:45,283][fairseq.trainer][INFO] - begin training epoch 87
[2025-07-05 05:14:45,283][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 05:15:04,329][train_inner][INFO] - {"epoch": 87, "update": 86.025, "loss": "5.223", "nll_loss": "0.014", "loss_recon": "0.103", "loss_info_nce": "4.197", "ppl": "1.01", "wps": "1149.5", "ups": "0.39", "wpb": "2954.6", "bsz": "385.6", "num_updates": "20800", "lr": "6.34783e-05", "gnorm": "1.068", "clip": "0", "loss_scale": "65536", "train_wall": "505", "gb_free": "2.9", "wall": "42713"}
[2025-07-05 05:23:35,098][train_inner][INFO] - {"epoch": 87, "update": 86.851, "loss": "5.22", "nll_loss": "0.014", "loss_recon": "0.103", "loss_info_nce": "4.193", "ppl": "1.01", "wps": "1160.2", "ups": "0.39", "wpb": "2963", "bsz": "387.1", "num_updates": "21000", "lr": "6.30435e-05", "gnorm": "1.166", "clip": "0", "loss_scale": "65536", "train_wall": "508", "gb_free": "2.9", "wall": "43224"}
[2025-07-05 05:25:03,745][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 87 @ 21036 updates
[2025-07-05 05:25:03,745][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint87.pt
[2025-07-05 05:25:05,186][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint87.pt
[2025-07-05 05:25:06,037][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint87.pt (epoch 87 @ 21036 updates, score None) (writing took 2.2878659999987576 seconds)
[2025-07-05 05:25:06,037][fairseq_cli.train][INFO] - end of epoch 87 (average epoch stats below)
[2025-07-05 05:25:06,037][train][INFO] - {"epoch": 87, "train_loss": "5.22", "train_nll_loss": "0.014", "train_loss_recon": "0.103", "train_loss_info_nce": "4.193", "train_ppl": "1.01", "train_wps": "1152", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "21036", "train_lr": "6.29652e-05", "train_gnorm": "1.157", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "612", "train_gb_free": "2.9", "train_wall": "43314"}
[2025-07-05 05:25:06,431][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 05:25:06,447][fairseq.trainer][INFO] - begin training epoch 88
[2025-07-05 05:25:06,447][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 05:32:11,970][train_inner][INFO] - {"epoch": 88, "update": 87.678, "loss": "5.22", "nll_loss": "0.014", "loss_recon": "0.103", "loss_info_nce": "4.194", "ppl": "1.01", "wps": "1144.4", "ups": "0.39", "wpb": "2957.4", "bsz": "385.1", "num_updates": "21200", "lr": "6.26087e-05", "gnorm": "1.224", "clip": "0", "loss_scale": "65536", "train_wall": "507", "gb_free": "2.9", "wall": "43740"}
[2025-07-05 05:35:28,164][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 88 @ 21278 updates
[2025-07-05 05:35:28,164][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint88.pt
[2025-07-05 05:35:29,622][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint88.pt
[2025-07-05 05:35:30,527][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint88.pt (epoch 88 @ 21278 updates, score None) (writing took 2.370018399997207 seconds)
[2025-07-05 05:35:30,527][fairseq_cli.train][INFO] - end of epoch 88 (average epoch stats below)
[2025-07-05 05:35:30,558][train][INFO] - {"epoch": 88, "train_loss": "5.216", "train_nll_loss": "0.014", "train_loss_recon": "0.102", "train_loss_info_nce": "4.191", "train_ppl": "1.01", "train_wps": "1145.8", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "21278", "train_lr": "6.24391e-05", "train_gnorm": "1.206", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "614", "train_gb_free": "2.9", "train_wall": "43939"}
[2025-07-05 05:35:30,937][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 05:35:30,937][fairseq.trainer][INFO] - begin training epoch 89
[2025-07-05 05:35:30,937][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 05:40:46,915][train_inner][INFO] - {"epoch": 89, "update": 88.504, "loss": "5.205", "nll_loss": "0.014", "loss_recon": "0.102", "loss_info_nce": "4.184", "ppl": "1.01", "wps": "1147.6", "ups": "0.39", "wpb": "2954.6", "bsz": "385.4", "num_updates": "21400", "lr": "6.21739e-05", "gnorm": "1.091", "clip": "0", "loss_scale": "65536", "train_wall": "506", "gb_free": "2.9", "wall": "44255"}
[2025-07-05 05:45:50,046][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 89 @ 21520 updates
[2025-07-05 05:45:50,046][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint89.pt
[2025-07-05 05:45:51,541][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint89.pt
[2025-07-05 05:45:52,446][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint89.pt (epoch 89 @ 21520 updates, score None) (writing took 2.394768700003624 seconds)
[2025-07-05 05:45:52,446][fairseq_cli.train][INFO] - end of epoch 89 (average epoch stats below)
[2025-07-05 05:45:52,462][train][INFO] - {"epoch": 89, "train_loss": "5.206", "train_nll_loss": "0.014", "train_loss_recon": "0.102", "train_loss_info_nce": "4.183", "train_ppl": "1.01", "train_wps": "1150.6", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "21520", "train_lr": "6.1913e-05", "train_gnorm": "1.095", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "612", "train_gb_free": "2.9", "train_wall": "44561"}
[2025-07-05 05:45:52,824][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 05:45:52,840][fairseq.trainer][INFO] - begin training epoch 90
[2025-07-05 05:45:52,840][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 05:49:20,662][train_inner][INFO] - {"epoch": 90, "update": 89.331, "loss": "5.207", "nll_loss": "0.014", "loss_recon": "0.102", "loss_info_nce": "4.183", "ppl": "1.01", "wps": "1151", "ups": "0.39", "wpb": "2956.5", "bsz": "385.8", "num_updates": "21600", "lr": "6.17391e-05", "gnorm": "1.068", "clip": "0", "loss_scale": "65536", "train_wall": "504", "gb_free": "2.9", "wall": "44769"}
[2025-07-05 05:56:13,015][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-05 05:57:05,468][valid][INFO] - {"epoch": 90, "valid_loss": "4.912", "valid_nll_loss": "0.013", "valid_loss_recon": "0.095", "valid_loss_info_nce": "3.966", "valid_ppl": "1.01", "valid_wps": "1566", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "21762", "valid_best_loss": "4.912"}
[2025-07-05 05:57:05,468][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 90 @ 21762 updates
[2025-07-05 05:57:05,468][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint90.pt
[2025-07-05 05:57:06,963][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint90.pt
[2025-07-05 05:57:08,617][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint90.pt (epoch 90 @ 21762 updates, score 4.912) (writing took 3.1457728000023053 seconds)
[2025-07-05 05:57:08,617][fairseq_cli.train][INFO] - end of epoch 90 (average epoch stats below)
[2025-07-05 05:57:08,617][train][INFO] - {"epoch": 90, "train_loss": "5.204", "train_nll_loss": "0.014", "train_loss_recon": "0.102", "train_loss_info_nce": "4.181", "train_ppl": "1.01", "train_wps": "1058.3", "train_ups": "0.36", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "21762", "train_lr": "6.1387e-05", "train_gnorm": "1.066", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "613", "train_gb_free": "2.9", "train_wall": "45237"}
[2025-07-05 05:57:08,987][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 05:57:08,987][fairseq.trainer][INFO] - begin training epoch 91
[2025-07-05 05:57:08,987][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 05:58:49,766][train_inner][INFO] - {"epoch": 91, "update": 90.157, "loss": "5.202", "nll_loss": "0.014", "loss_recon": "0.102", "loss_info_nce": "4.181", "ppl": "1.01", "wps": "1039", "ups": "0.35", "wpb": "2956.5", "bsz": "385.1", "num_updates": "21800", "lr": "6.13043e-05", "gnorm": "1.153", "clip": "0", "loss_scale": "65536", "train_wall": "506", "gb_free": "2.9", "wall": "45338"}
[2025-07-05 06:07:19,652][train_inner][INFO] - {"epoch": 91, "update": 90.983, "loss": "5.198", "nll_loss": "0.014", "loss_recon": "0.102", "loss_info_nce": "4.176", "ppl": "1.01", "wps": "1161.9", "ups": "0.39", "wpb": "2962.1", "bsz": "387.4", "num_updates": "22000", "lr": "6.08696e-05", "gnorm": "1.093", "clip": "0", "loss_scale": "65536", "train_wall": "506", "gb_free": "2.9", "wall": "45848"}
[2025-07-05 06:07:26,880][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 91 @ 22004 updates
[2025-07-05 06:07:26,880][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint91.pt
[2025-07-05 06:07:28,377][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint91.pt
[2025-07-05 06:07:29,183][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint91.pt (epoch 91 @ 22004 updates, score None) (writing took 2.3100618999960716 seconds)
[2025-07-05 06:07:29,183][fairseq_cli.train][INFO] - end of epoch 91 (average epoch stats below)
[2025-07-05 06:07:29,199][train][INFO] - {"epoch": 91, "train_loss": "5.199", "train_nll_loss": "0.014", "train_loss_recon": "0.102", "train_loss_info_nce": "4.177", "train_ppl": "1.01", "train_wps": "1153.1", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "22004", "train_lr": "6.08609e-05", "train_gnorm": "1.143", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "610", "train_gb_free": "2.9", "train_wall": "45858"}
[2025-07-05 06:07:29,593][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 06:07:29,593][fairseq.trainer][INFO] - begin training epoch 92
[2025-07-05 06:07:29,593][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 06:12:03,700][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 65536.0
[2025-07-05 06:15:56,326][train_inner][INFO] - {"epoch": 92, "update": 91.814, "loss": "5.194", "nll_loss": "0.014", "loss_recon": "0.102", "loss_info_nce": "4.175", "ppl": "1.01", "wps": "1143.7", "ups": "0.39", "wpb": "2954.6", "bsz": "386.1", "num_updates": "22200", "lr": "6.04348e-05", "gnorm": "1.094", "clip": "0", "loss_scale": "65536", "train_wall": "508", "gb_free": "2.9", "wall": "46365"}
[2025-07-05 06:17:47,837][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 92 @ 22245 updates
[2025-07-05 06:17:47,837][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint92.pt
[2025-07-05 06:17:49,281][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint92.pt
[2025-07-05 06:17:50,030][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint92.pt (epoch 92 @ 22245 updates, score None) (writing took 2.1906724000000395 seconds)
[2025-07-05 06:17:50,030][fairseq_cli.train][INFO] - end of epoch 92 (average epoch stats below)
[2025-07-05 06:17:50,046][train][INFO] - {"epoch": 92, "train_loss": "5.195", "train_nll_loss": "0.014", "train_loss_recon": "0.102", "train_loss_info_nce": "4.174", "train_ppl": "1.01", "train_wps": "1147.7", "train_ups": "0.39", "train_wpb": "2956.7", "train_bsz": "385.8", "train_num_updates": "22245", "train_lr": "6.0337e-05", "train_gnorm": "1.092", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "612", "train_gb_free": "2.9", "train_wall": "46478"}
[2025-07-05 06:17:50,425][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 06:17:50,425][fairseq.trainer][INFO] - begin training epoch 93
[2025-07-05 06:17:50,425][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 06:24:29,782][train_inner][INFO] - {"epoch": 93, "update": 92.64, "loss": "5.195", "nll_loss": "0.014", "loss_recon": "0.102", "loss_info_nce": "4.173", "ppl": "1.01", "wps": "1152", "ups": "0.39", "wpb": "2957.4", "bsz": "385.1", "num_updates": "22400", "lr": "6e-05", "gnorm": "1.098", "clip": "0", "loss_scale": "65536", "train_wall": "504", "gb_free": "2.9", "wall": "46878"}
[2025-07-05 06:28:08,135][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 93 @ 22487 updates
[2025-07-05 06:28:08,135][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint93.pt
[2025-07-05 06:28:09,682][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint93.pt
[2025-07-05 06:28:10,512][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint93.pt (epoch 93 @ 22487 updates, score None) (writing took 2.375468300000648 seconds)
[2025-07-05 06:28:10,512][fairseq_cli.train][INFO] - end of epoch 93 (average epoch stats below)
[2025-07-05 06:28:10,512][train][INFO] - {"epoch": 93, "train_loss": "5.19", "train_nll_loss": "0.014", "train_loss_recon": "0.102", "train_loss_info_nce": "4.17", "train_ppl": "1.01", "train_wps": "1153.2", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "22487", "train_lr": "5.98109e-05", "train_gnorm": "1.113", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "610", "train_gb_free": "2.9", "train_wall": "47099"}
[2025-07-05 06:28:10,897][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 06:28:10,897][fairseq.trainer][INFO] - begin training epoch 94
[2025-07-05 06:28:10,897][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 06:33:02,902][train_inner][INFO] - {"epoch": 94, "update": 93.467, "loss": "5.191", "nll_loss": "0.014", "loss_recon": "0.102", "loss_info_nce": "4.173", "ppl": "1.01", "wps": "1152.7", "ups": "0.39", "wpb": "2957.4", "bsz": "384.3", "num_updates": "22600", "lr": "5.95652e-05", "gnorm": "1.09", "clip": "0", "loss_scale": "65536", "train_wall": "505", "gb_free": "2.9", "wall": "47391"}
[2025-07-05 06:38:29,040][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 94 @ 22729 updates
[2025-07-05 06:38:29,056][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint94.pt
[2025-07-05 06:38:30,510][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint94.pt
[2025-07-05 06:38:31,358][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint94.pt (epoch 94 @ 22729 updates, score None) (writing took 2.3044238000002224 seconds)
[2025-07-05 06:38:31,358][fairseq_cli.train][INFO] - end of epoch 94 (average epoch stats below)
[2025-07-05 06:38:31,368][train][INFO] - {"epoch": 94, "train_loss": "5.191", "train_nll_loss": "0.014", "train_loss_recon": "0.102", "train_loss_info_nce": "4.17", "train_ppl": "1.01", "train_wps": "1152.5", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "22729", "train_lr": "5.92848e-05", "train_gnorm": "1.073", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "612", "train_gb_free": "2.9", "train_wall": "47720"}
[2025-07-05 06:38:31,752][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 06:38:31,752][fairseq.trainer][INFO] - begin training epoch 95
[2025-07-05 06:38:31,752][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 06:41:37,107][train_inner][INFO] - {"epoch": 95, "update": 94.293, "loss": "5.183", "nll_loss": "0.014", "loss_recon": "0.102", "loss_info_nce": "4.164", "ppl": "1.01", "wps": "1147.8", "ups": "0.39", "wpb": "2950.9", "bsz": "387", "num_updates": "22800", "lr": "5.91304e-05", "gnorm": "1.082", "clip": "0", "loss_scale": "65536", "train_wall": "505", "gb_free": "2.9", "wall": "47906"}
[2025-07-05 06:48:52,071][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-05 06:49:44,064][valid][INFO] - {"epoch": 95, "valid_loss": "4.935", "valid_nll_loss": "0.013", "valid_loss_recon": "0.098", "valid_loss_info_nce": "3.952", "valid_ppl": "1.01", "valid_wps": "1584.6", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "22971", "valid_best_loss": "4.912"}
[2025-07-05 06:49:44,064][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 95 @ 22971 updates
[2025-07-05 06:49:44,064][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint95.pt
[2025-07-05 06:49:45,574][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint95.pt
[2025-07-05 06:49:46,423][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint95.pt (epoch 95 @ 22971 updates, score 4.935) (writing took 2.349437199998647 seconds)
[2025-07-05 06:49:46,423][fairseq_cli.train][INFO] - end of epoch 95 (average epoch stats below)
[2025-07-05 06:49:46,423][train][INFO] - {"epoch": 95, "train_loss": "5.181", "train_nll_loss": "0.014", "train_loss_recon": "0.102", "train_loss_info_nce": "4.163", "train_ppl": "1.01", "train_wps": "1060", "train_ups": "0.36", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "22971", "train_lr": "5.87587e-05", "train_gnorm": "1.16", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "613", "train_gb_free": "2.9", "train_wall": "48395"}
[2025-07-05 06:49:46,791][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 06:49:46,791][fairseq.trainer][INFO] - begin training epoch 96
[2025-07-05 06:49:46,791][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 06:51:04,621][train_inner][INFO] - {"epoch": 96, "update": 95.12, "loss": "5.181", "nll_loss": "0.014", "loss_recon": "0.102", "loss_info_nce": "4.163", "ppl": "1.01", "wps": "1041.6", "ups": "0.35", "wpb": "2955.5", "bsz": "385.6", "num_updates": "23000", "lr": "5.86957e-05", "gnorm": "1.163", "clip": "0", "loss_scale": "65536", "train_wall": "506", "gb_free": "2.9", "wall": "48473"}
[2025-07-05 06:59:33,615][train_inner][INFO] - {"epoch": 96, "update": 95.946, "loss": "5.179", "nll_loss": "0.014", "loss_recon": "0.102", "loss_info_nce": "4.162", "ppl": "1.01", "wps": "1164.6", "ups": "0.39", "wpb": "2963.9", "bsz": "386.8", "num_updates": "23200", "lr": "5.82609e-05", "gnorm": "0.961", "clip": "0", "loss_scale": "65536", "train_wall": "506", "gb_free": "2.9", "wall": "48982"}
[2025-07-05 07:00:04,393][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 96 @ 23213 updates
[2025-07-05 07:00:04,393][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint96.pt
[2025-07-05 07:00:05,928][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint96.pt
[2025-07-05 07:00:06,805][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint96.pt (epoch 96 @ 23213 updates, score None) (writing took 2.406721400002425 seconds)
[2025-07-05 07:00:06,805][fairseq_cli.train][INFO] - end of epoch 96 (average epoch stats below)
[2025-07-05 07:00:06,815][train][INFO] - {"epoch": 96, "train_loss": "5.18", "train_nll_loss": "0.014", "train_loss_recon": "0.102", "train_loss_info_nce": "4.162", "train_ppl": "1.01", "train_wps": "1153.4", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "23213", "train_lr": "5.82326e-05", "train_gnorm": "0.95", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "611", "train_gb_free": "2.9", "train_wall": "49015"}
[2025-07-05 07:00:07,214][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 07:00:07,224][fairseq.trainer][INFO] - begin training epoch 97
[2025-07-05 07:00:07,224][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 07:08:09,447][train_inner][INFO] - {"epoch": 97, "update": 96.773, "loss": "5.177", "nll_loss": "0.014", "loss_recon": "0.102", "loss_info_nce": "4.162", "ppl": "1.01", "wps": "1147", "ups": "0.39", "wpb": "2958.3", "bsz": "384.8", "num_updates": "23400", "lr": "5.78261e-05", "gnorm": "1.032", "clip": "0", "loss_scale": "65536", "train_wall": "507", "gb_free": "2.9", "wall": "49498"}
[2025-07-05 07:10:27,833][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 97 @ 23455 updates
[2025-07-05 07:10:27,833][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint97.pt
[2025-07-05 07:10:29,294][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint97.pt
[2025-07-05 07:10:30,084][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint97.pt (epoch 97 @ 23455 updates, score None) (writing took 2.2482605000041076 seconds)
[2025-07-05 07:10:30,084][fairseq_cli.train][INFO] - end of epoch 97 (average epoch stats below)
[2025-07-05 07:10:30,084][train][INFO] - {"epoch": 97, "train_loss": "5.178", "train_nll_loss": "0.014", "train_loss_recon": "0.102", "train_loss_info_nce": "4.16", "train_ppl": "1.01", "train_wps": "1148.1", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "23455", "train_lr": "5.77065e-05", "train_gnorm": "1.053", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "614", "train_gb_free": "3.2", "train_wall": "49639"}
[2025-07-05 07:10:30,478][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 07:10:30,493][fairseq.trainer][INFO] - begin training epoch 98
[2025-07-05 07:10:30,493][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 07:16:42,966][train_inner][INFO] - {"epoch": 98, "update": 97.599, "loss": "5.175", "nll_loss": "0.014", "loss_recon": "0.102", "loss_info_nce": "4.157", "ppl": "1.01", "wps": "1151.2", "ups": "0.39", "wpb": "2955.5", "bsz": "385.4", "num_updates": "23600", "lr": "5.73913e-05", "gnorm": "1.027", "clip": "0", "loss_scale": "65536", "train_wall": "505", "gb_free": "2.9", "wall": "50011"}
[2025-07-05 07:20:47,464][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 98 @ 23697 updates
[2025-07-05 07:20:47,464][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint98.pt
[2025-07-05 07:20:49,094][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint98.pt
[2025-07-05 07:20:49,919][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint98.pt (epoch 98 @ 23697 updates, score None) (writing took 2.447298299994145 seconds)
[2025-07-05 07:20:49,920][fairseq_cli.train][INFO] - end of epoch 98 (average epoch stats below)
[2025-07-05 07:20:49,927][train][INFO] - {"epoch": 98, "train_loss": "5.172", "train_nll_loss": "0.014", "train_loss_recon": "0.102", "train_loss_info_nce": "4.156", "train_ppl": "1.01", "train_wps": "1154.4", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "23697", "train_lr": "5.71804e-05", "train_gnorm": "1.012", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "610", "train_gb_free": "2.9", "train_wall": "50258"}
[2025-07-05 07:20:50,337][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 07:20:50,343][fairseq.trainer][INFO] - begin training epoch 99
[2025-07-05 07:20:50,344][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 07:25:16,752][train_inner][INFO] - {"epoch": 99, "update": 98.426, "loss": "5.173", "nll_loss": "0.014", "loss_recon": "0.102", "loss_info_nce": "4.156", "ppl": "1.01", "wps": "1149.8", "ups": "0.39", "wpb": "2953.7", "bsz": "385.9", "num_updates": "23800", "lr": "5.69565e-05", "gnorm": "1.162", "clip": "0", "loss_scale": "65536", "train_wall": "504", "gb_free": "2.9", "wall": "50525"}
[2025-07-05 07:31:08,652][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 99 @ 23939 updates
[2025-07-05 07:31:08,652][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint99.pt
[2025-07-05 07:31:10,105][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint99.pt
[2025-07-05 07:31:10,895][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint99.pt (epoch 99 @ 23939 updates, score None) (writing took 2.2478946999981417 seconds)
[2025-07-05 07:31:10,895][fairseq_cli.train][INFO] - end of epoch 99 (average epoch stats below)
[2025-07-05 07:31:10,895][train][INFO] - {"epoch": 99, "train_loss": "5.169", "train_nll_loss": "0.014", "train_loss_recon": "0.102", "train_loss_info_nce": "4.154", "train_ppl": "1.01", "train_wps": "1152.3", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "23939", "train_lr": "5.66543e-05", "train_gnorm": "1.172", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "611", "train_gb_free": "2.9", "train_wall": "50879"}
[2025-07-05 07:31:11,290][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 07:31:11,290][fairseq.trainer][INFO] - begin training epoch 100
[2025-07-05 07:31:11,290][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 07:33:50,784][train_inner][INFO] - {"epoch": 100, "update": 99.252, "loss": "5.167", "nll_loss": "0.014", "loss_recon": "0.102", "loss_info_nce": "4.152", "ppl": "1.01", "wps": "1149.6", "ups": "0.39", "wpb": "2954.6", "bsz": "385.7", "num_updates": "24000", "lr": "5.65217e-05", "gnorm": "1.05", "clip": "0", "loss_scale": "65536", "train_wall": "506", "gb_free": "2.9", "wall": "51039"}
[2025-07-05 07:41:26,523][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 65536.0
[2025-07-05 07:41:31,576][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-05 07:42:24,027][valid][INFO] - {"epoch": 100, "valid_loss": "4.871", "valid_nll_loss": "0.013", "valid_loss_recon": "0.092", "valid_loss_info_nce": "3.948", "valid_ppl": "1.01", "valid_wps": "1562.3", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "24180", "valid_best_loss": "4.871"}
[2025-07-05 07:42:24,027][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 100 @ 24180 updates
[2025-07-05 07:42:24,027][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint100.pt
[2025-07-05 07:42:25,520][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint100.pt
[2025-07-05 07:42:27,147][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint100.pt (epoch 100 @ 24180 updates, score 4.871) (writing took 3.114201000003959 seconds)
[2025-07-05 07:42:27,147][fairseq_cli.train][INFO] - end of epoch 100 (average epoch stats below)
[2025-07-05 07:42:27,147][train][INFO] - {"epoch": 100, "train_loss": "5.164", "train_nll_loss": "0.014", "train_loss_recon": "0.101", "train_loss_info_nce": "4.149", "train_ppl": "1.01", "train_wps": "1053.7", "train_ups": "0.36", "train_wpb": "2956.7", "train_bsz": "385.8", "train_num_updates": "24180", "train_lr": "5.61304e-05", "train_gnorm": "1.061", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "614", "train_gb_free": "2.9", "train_wall": "51556"}
[2025-07-05 07:42:27,526][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 07:42:27,541][fairseq.trainer][INFO] - begin training epoch 101
[2025-07-05 07:42:27,541][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 07:43:22,392][train_inner][INFO] - {"epoch": 101, "update": 100.083, "loss": "5.164", "nll_loss": "0.014", "loss_recon": "0.102", "loss_info_nce": "4.149", "ppl": "1.01", "wps": "1034.1", "ups": "0.35", "wpb": "2955.5", "bsz": "385.8", "num_updates": "24200", "lr": "5.6087e-05", "gnorm": "1.075", "clip": "0", "loss_scale": "65536", "train_wall": "510", "gb_free": "2.9", "wall": "51611"}
[2025-07-05 07:51:50,678][train_inner][INFO] - {"epoch": 101, "update": 100.909, "loss": "5.16", "nll_loss": "0.014", "loss_recon": "0.101", "loss_info_nce": "4.148", "ppl": "1.01", "wps": "1165.5", "ups": "0.39", "wpb": "2962.1", "bsz": "387.4", "num_updates": "24400", "lr": "5.56522e-05", "gnorm": "1.071", "clip": "0", "loss_scale": "65536", "train_wall": "505", "gb_free": "2.9", "wall": "52119"}
[2025-07-05 07:52:43,640][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 101 @ 24422 updates
[2025-07-05 07:52:43,640][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint101.pt
[2025-07-05 07:52:45,092][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint101.pt
[2025-07-05 07:52:45,968][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint101.pt (epoch 101 @ 24422 updates, score None) (writing took 2.3266394000020227 seconds)
[2025-07-05 07:52:45,968][fairseq_cli.train][INFO] - end of epoch 101 (average epoch stats below)
[2025-07-05 07:52:45,982][train][INFO] - {"epoch": 101, "train_loss": "5.163", "train_nll_loss": "0.014", "train_loss_recon": "0.101", "train_loss_info_nce": "4.149", "train_ppl": "1.01", "train_wps": "1156.3", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "24422", "train_lr": "5.56043e-05", "train_gnorm": "1.035", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "610", "train_gb_free": "2.9", "train_wall": "52174"}
[2025-07-05 07:52:46,351][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 07:52:46,366][fairseq.trainer][INFO] - begin training epoch 102
[2025-07-05 07:52:46,366][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 08:00:23,738][train_inner][INFO] - {"epoch": 102, "update": 101.736, "loss": "5.157", "nll_loss": "0.014", "loss_recon": "0.101", "loss_info_nce": "4.144", "ppl": "1.01", "wps": "1152.9", "ups": "0.39", "wpb": "2957.4", "bsz": "384.8", "num_updates": "24600", "lr": "5.52174e-05", "gnorm": "1.112", "clip": "0", "loss_scale": "65536", "train_wall": "505", "gb_free": "2.9", "wall": "52632"}
[2025-07-05 08:03:03,765][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 102 @ 24664 updates
[2025-07-05 08:03:03,765][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint102.pt
[2025-07-05 08:03:05,299][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint102.pt
[2025-07-05 08:03:06,163][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint102.pt (epoch 102 @ 24664 updates, score None) (writing took 2.3963182999941637 seconds)
[2025-07-05 08:03:06,163][fairseq_cli.train][INFO] - end of epoch 102 (average epoch stats below)
[2025-07-05 08:03:06,173][train][INFO] - {"epoch": 102, "train_loss": "5.159", "train_nll_loss": "0.014", "train_loss_recon": "0.101", "train_loss_info_nce": "4.145", "train_ppl": "1.01", "train_wps": "1153.8", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "24664", "train_lr": "5.50783e-05", "train_gnorm": "1.138", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "611", "train_gb_free": "2.9", "train_wall": "52795"}
[2025-07-05 08:03:06,526][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 08:03:06,526][fairseq.trainer][INFO] - begin training epoch 103
[2025-07-05 08:03:06,526][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 08:08:54,669][train_inner][INFO] - {"epoch": 103, "update": 102.562, "loss": "5.159", "nll_loss": "0.014", "loss_recon": "0.102", "loss_info_nce": "4.143", "ppl": "1.01", "wps": "1156.6", "ups": "0.39", "wpb": "2954.6", "bsz": "385.9", "num_updates": "24800", "lr": "5.47826e-05", "gnorm": "1.061", "clip": "0", "loss_scale": "65536", "train_wall": "503", "gb_free": "2.9", "wall": "53143"}
[2025-07-05 08:13:20,738][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 103 @ 24906 updates
[2025-07-05 08:13:20,739][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint103.pt
[2025-07-05 08:13:22,238][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint103.pt
[2025-07-05 08:13:23,200][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint103.pt (epoch 103 @ 24906 updates, score None) (writing took 2.461249799998768 seconds)
[2025-07-05 08:13:23,200][fairseq_cli.train][INFO] - end of epoch 103 (average epoch stats below)
[2025-07-05 08:13:23,209][train][INFO] - {"epoch": 103, "train_loss": "5.153", "train_nll_loss": "0.014", "train_loss_recon": "0.101", "train_loss_info_nce": "4.14", "train_ppl": "1.01", "train_wps": "1159.7", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "24906", "train_lr": "5.45522e-05", "train_gnorm": "1.032", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "608", "train_gb_free": "2.9", "train_wall": "53412"}
[2025-07-05 08:13:23,617][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 08:13:23,627][fairseq.trainer][INFO] - begin training epoch 104
[2025-07-05 08:13:23,627][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 08:17:26,237][train_inner][INFO] - {"epoch": 104, "update": 103.388, "loss": "5.156", "nll_loss": "0.014", "loss_recon": "0.101", "loss_info_nce": "4.142", "ppl": "1.01", "wps": "1155.9", "ups": "0.39", "wpb": "2956.5", "bsz": "385.6", "num_updates": "25000", "lr": "5.43478e-05", "gnorm": "0.967", "clip": "0", "loss_scale": "65536", "train_wall": "503", "gb_free": "2.9", "wall": "53655"}
[2025-07-05 08:17:26,237][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-05 08:18:18,701][valid][INFO] - {"epoch": 104, "valid_loss": "4.875", "valid_nll_loss": "0.013", "valid_loss_recon": "0.094", "valid_loss_info_nce": "3.934", "valid_ppl": "1.01", "valid_wps": "1570.8", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "25000", "valid_best_loss": "4.871"}
[2025-07-05 08:18:18,701][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 104 @ 25000 updates
[2025-07-05 08:18:18,701][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint_104_25000.pt
[2025-07-05 08:18:20,169][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint_104_25000.pt
[2025-07-05 08:18:21,007][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint_104_25000.pt (epoch 104 @ 25000 updates, score 4.875) (writing took 2.3023764999961713 seconds)
[2025-07-05 08:24:34,244][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 104 @ 25148 updates
[2025-07-05 08:24:34,244][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint104.pt
[2025-07-05 08:24:35,715][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint104.pt
[2025-07-05 08:24:36,536][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint104.pt (epoch 104 @ 25148 updates, score None) (writing took 2.3032136999972863 seconds)
[2025-07-05 08:24:36,536][fairseq_cli.train][INFO] - end of epoch 104 (average epoch stats below)
[2025-07-05 08:24:36,551][train][INFO] - {"epoch": 104, "train_loss": "5.154", "train_nll_loss": "0.014", "train_loss_recon": "0.101", "train_loss_info_nce": "4.141", "train_ppl": "1.01", "train_wps": "1062.7", "train_ups": "0.36", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "25148", "train_lr": "5.40261e-05", "train_gnorm": "0.947", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "609", "train_gb_free": "2.9", "train_wall": "54085"}
[2025-07-05 08:24:36,946][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 08:24:36,962][fairseq.trainer][INFO] - begin training epoch 105
[2025-07-05 08:24:36,962][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 08:26:53,689][train_inner][INFO] - {"epoch": 105, "update": 104.215, "loss": "5.154", "nll_loss": "0.014", "loss_recon": "0.101", "loss_info_nce": "4.142", "ppl": "1.01", "wps": "1041", "ups": "0.35", "wpb": "2953.7", "bsz": "385.9", "num_updates": "25200", "lr": "5.3913e-05", "gnorm": "1.021", "clip": "0", "loss_scale": "65536", "train_wall": "505", "gb_free": "2.9", "wall": "54222"}
[2025-07-05 08:33:38,670][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-05 08:34:09,224][valid][INFO] - {"epoch": 105, "valid_loss": "4.892", "valid_nll_loss": "0.013", "valid_loss_recon": "0.096", "valid_loss_info_nce": "3.928", "valid_ppl": "1.01", "valid_wps": "2696.8", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "25390", "valid_best_loss": "4.871"}
[2025-07-05 08:34:09,224][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 105 @ 25390 updates
[2025-07-05 08:34:09,224][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint105.pt
[2025-07-05 08:34:10,648][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint105.pt
[2025-07-05 08:34:11,146][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint105.pt (epoch 105 @ 25390 updates, score 4.892) (writing took 1.9183823999992455 seconds)
[2025-07-05 08:34:11,146][fairseq_cli.train][INFO] - end of epoch 105 (average epoch stats below)
[2025-07-05 08:34:11,207][train][INFO] - {"epoch": 105, "train_loss": "5.151", "train_nll_loss": "0.014", "train_loss_recon": "0.101", "train_loss_info_nce": "4.138", "train_ppl": "1.01", "train_wps": "1245.3", "train_ups": "0.42", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "25390", "train_lr": "5.35e-05", "train_gnorm": "1.049", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "538", "train_gb_free": "2.9", "train_wall": "54660"}
[2025-07-05 08:34:11,400][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 08:34:11,411][fairseq.trainer][INFO] - begin training epoch 106
[2025-07-05 08:34:11,411][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 08:34:34,767][train_inner][INFO] - {"epoch": 106, "update": 105.041, "loss": "5.147", "nll_loss": "0.014", "loss_recon": "0.101", "loss_info_nce": "4.135", "ppl": "1.01", "wps": "1282", "ups": "0.43", "wpb": "2955.5", "bsz": "385.4", "num_updates": "25400", "lr": "5.34783e-05", "gnorm": "1.001", "clip": "0", "loss_scale": "65536", "train_wall": "425", "gb_free": "2.9", "wall": "54683"}
[2025-07-05 08:41:33,591][train_inner][INFO] - {"epoch": 106, "update": 105.868, "loss": "5.149", "nll_loss": "0.014", "loss_recon": "0.101", "loss_info_nce": "4.136", "ppl": "1.01", "wps": "1415.3", "ups": "0.48", "wpb": "2963.9", "bsz": "386.8", "num_updates": "25600", "lr": "5.30435e-05", "gnorm": "1.044", "clip": "0", "loss_scale": "65536", "train_wall": "418", "gb_free": "2.9", "wall": "55102"}
[2025-07-05 08:42:40,080][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 106 @ 25632 updates
[2025-07-05 08:42:40,080][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint106.pt
[2025-07-05 08:42:41,567][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint106.pt
[2025-07-05 08:42:42,543][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint106.pt (epoch 106 @ 25632 updates, score None) (writing took 2.471437500003958 seconds)
[2025-07-05 08:42:42,543][fairseq_cli.train][INFO] - end of epoch 106 (average epoch stats below)
[2025-07-05 08:42:42,922][train][INFO] - {"epoch": 106, "train_loss": "5.147", "train_nll_loss": "0.014", "train_loss_recon": "0.101", "train_loss_info_nce": "4.135", "train_ppl": "1.01", "train_wps": "1399.3", "train_ups": "0.47", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "25632", "train_lr": "5.29739e-05", "train_gnorm": "0.995", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "505", "train_gb_free": "2.9", "train_wall": "55171"}
[2025-07-05 08:42:43,243][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 08:42:43,243][fairseq.trainer][INFO] - begin training epoch 107
[2025-07-05 08:42:43,243][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 08:48:43,387][train_inner][INFO] - {"epoch": 107, "update": 106.694, "loss": "5.144", "nll_loss": "0.014", "loss_recon": "0.101", "loss_info_nce": "4.134", "ppl": "1.01", "wps": "1374.9", "ups": "0.47", "wpb": "2954.6", "bsz": "385.7", "num_updates": "25800", "lr": "5.26087e-05", "gnorm": "0.97", "clip": "0", "loss_scale": "65536", "train_wall": "423", "gb_free": "2.9", "wall": "55532"}
[2025-07-05 08:51:15,895][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 107 @ 25874 updates
[2025-07-05 08:51:15,895][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint107.pt
[2025-07-05 08:51:17,318][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint107.pt
[2025-07-05 08:51:17,796][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint107.pt (epoch 107 @ 25874 updates, score None) (writing took 1.9100963000018965 seconds)
[2025-07-05 08:51:17,796][fairseq_cli.train][INFO] - end of epoch 107 (average epoch stats below)
[2025-07-05 08:51:17,812][train][INFO] - {"epoch": 107, "train_loss": "5.149", "train_nll_loss": "0.014", "train_loss_recon": "0.101", "train_loss_info_nce": "4.137", "train_ppl": "1.01", "train_wps": "1389.7", "train_ups": "0.47", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "25874", "train_lr": "5.24478e-05", "train_gnorm": "1.164", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "509", "train_gb_free": "2.9", "train_wall": "55686"}
[2025-07-05 08:51:18,011][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 08:51:18,021][fairseq.trainer][INFO] - begin training epoch 108
[2025-07-05 08:51:18,021][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 08:55:42,152][train_inner][INFO] - {"epoch": 108, "update": 107.521, "loss": "5.148", "nll_loss": "0.014", "loss_recon": "0.101", "loss_info_nce": "4.136", "ppl": "1.01", "wps": "1411.1", "ups": "0.48", "wpb": "2954.6", "bsz": "386.1", "num_updates": "26000", "lr": "5.21739e-05", "gnorm": "1.118", "clip": "0", "loss_scale": "65536", "train_wall": "414", "gb_free": "2.9", "wall": "55951"}
[2025-07-05 08:59:41,164][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 108 @ 26116 updates
[2025-07-05 08:59:41,164][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint108.pt
[2025-07-05 08:59:42,615][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint108.pt
[2025-07-05 08:59:43,125][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint108.pt (epoch 108 @ 26116 updates, score None) (writing took 1.964280600004713 seconds)
[2025-07-05 08:59:43,125][fairseq_cli.train][INFO] - end of epoch 108 (average epoch stats below)
[2025-07-05 08:59:43,156][train][INFO] - {"epoch": 108, "train_loss": "5.143", "train_nll_loss": "0.014", "train_loss_recon": "0.101", "train_loss_info_nce": "4.132", "train_ppl": "1.01", "train_wps": "1416", "train_ups": "0.48", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "26116", "train_lr": "5.19217e-05", "train_gnorm": "1.037", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "500", "train_gb_free": "2.9", "train_wall": "56192"}
[2025-07-05 08:59:43,339][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 08:59:43,339][fairseq.trainer][INFO] - begin training epoch 109
[2025-07-05 08:59:43,339][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 09:02:38,879][train_inner][INFO] - {"epoch": 109, "update": 108.347, "loss": "5.144", "nll_loss": "0.014", "loss_recon": "0.101", "loss_info_nce": "4.132", "ppl": "1.01", "wps": "1419.3", "ups": "0.48", "wpb": "2957.4", "bsz": "384.8", "num_updates": "26200", "lr": "5.17391e-05", "gnorm": "1.049", "clip": "0", "loss_scale": "65536", "train_wall": "412", "gb_free": "2.9", "wall": "56367"}
[2025-07-05 09:04:26,562][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 65536.0
[2025-07-05 09:08:04,724][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 109 @ 26357 updates
[2025-07-05 09:08:04,724][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint109.pt
[2025-07-05 09:08:06,060][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint109.pt
[2025-07-05 09:08:06,538][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint109.pt (epoch 109 @ 26357 updates, score None) (writing took 1.813099200000579 seconds)
[2025-07-05 09:08:06,538][fairseq_cli.train][INFO] - end of epoch 109 (average epoch stats below)
[2025-07-05 09:08:06,554][train][INFO] - {"epoch": 109, "train_loss": "5.137", "train_nll_loss": "0.014", "train_loss_recon": "0.101", "train_loss_info_nce": "4.128", "train_ppl": "1.01", "train_wps": "1415.5", "train_ups": "0.48", "train_wpb": "2956.7", "train_bsz": "385.8", "train_num_updates": "26357", "train_lr": "5.13978e-05", "train_gnorm": "0.939", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "498", "train_gb_free": "2.9", "train_wall": "56695"}
[2025-07-05 09:08:06,735][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 09:08:06,751][fairseq.trainer][INFO] - begin training epoch 110
[2025-07-05 09:08:06,751][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 09:09:37,967][train_inner][INFO] - {"epoch": 110, "update": 109.178, "loss": "5.134", "nll_loss": "0.014", "loss_recon": "0.101", "loss_info_nce": "4.126", "ppl": "1.01", "wps": "1408.7", "ups": "0.48", "wpb": "2951.8", "bsz": "386.7", "num_updates": "26400", "lr": "5.13043e-05", "gnorm": "1.01", "clip": "0", "loss_scale": "65536", "train_wall": "414", "gb_free": "2.9", "wall": "56786"}
[2025-07-05 09:16:28,534][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-05 09:16:58,096][valid][INFO] - {"epoch": 110, "valid_loss": "4.839", "valid_nll_loss": "0.013", "valid_loss_recon": "0.092", "valid_loss_info_nce": "3.922", "valid_ppl": "1.01", "valid_wps": "2788.5", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "26599", "valid_best_loss": "4.839"}
[2025-07-05 09:16:58,096][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 110 @ 26599 updates
[2025-07-05 09:16:58,096][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint110.pt
[2025-07-05 09:16:59,630][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint110.pt
[2025-07-05 09:17:00,537][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint110.pt (epoch 110 @ 26599 updates, score 4.839) (writing took 2.4416103000039584 seconds)
[2025-07-05 09:17:00,537][fairseq_cli.train][INFO] - end of epoch 110 (average epoch stats below)
[2025-07-05 09:17:00,553][train][INFO] - {"epoch": 110, "train_loss": "5.137", "train_nll_loss": "0.014", "train_loss_recon": "0.101", "train_loss_info_nce": "4.128", "train_ppl": "1.01", "train_wps": "1340", "train_ups": "0.45", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "26599", "train_lr": "5.08717e-05", "train_gnorm": "0.963", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "498", "train_gb_free": "2.9", "train_wall": "57229"}
[2025-07-05 09:17:00,734][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 09:17:00,734][fairseq.trainer][INFO] - begin training epoch 111
[2025-07-05 09:17:00,734][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 09:17:04,920][train_inner][INFO] - {"epoch": 111, "update": 110.004, "loss": "5.137", "nll_loss": "0.014", "loss_recon": "0.101", "loss_info_nce": "4.128", "ppl": "1.01", "wps": "1323.4", "ups": "0.45", "wpb": "2957.4", "bsz": "384.8", "num_updates": "26600", "lr": "5.08696e-05", "gnorm": "0.938", "clip": "0", "loss_scale": "65536", "train_wall": "412", "gb_free": "2.9", "wall": "57233"}
[2025-07-05 09:23:58,837][train_inner][INFO] - {"epoch": 111, "update": 110.831, "loss": "5.132", "nll_loss": "0.014", "loss_recon": "0.101", "loss_info_nce": "4.123", "ppl": "1.01", "wps": "1432.6", "ups": "0.48", "wpb": "2964.8", "bsz": "386.6", "num_updates": "26800", "lr": "5.04348e-05", "gnorm": "0.917", "clip": "0", "loss_scale": "65536", "train_wall": "412", "gb_free": "2.9", "wall": "57647"}
[2025-07-05 09:25:22,312][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 111 @ 26841 updates
[2025-07-05 09:25:22,312][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint111.pt
[2025-07-05 09:25:23,716][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint111.pt
[2025-07-05 09:25:24,211][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint111.pt (epoch 111 @ 26841 updates, score None) (writing took 1.895497899997281 seconds)
[2025-07-05 09:25:24,211][fairseq_cli.train][INFO] - end of epoch 111 (average epoch stats below)
[2025-07-05 09:25:24,221][train][INFO] - {"epoch": 111, "train_loss": "5.131", "train_nll_loss": "0.014", "train_loss_recon": "0.101", "train_loss_info_nce": "4.123", "train_ppl": "1.01", "train_wps": "1420.7", "train_ups": "0.48", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "26841", "train_lr": "5.03457e-05", "train_gnorm": "0.937", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "498", "train_gb_free": "2.9", "train_wall": "57733"}
[2025-07-05 09:25:24,393][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 09:25:24,393][fairseq.trainer][INFO] - begin training epoch 112
[2025-07-05 09:25:24,393][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 09:30:55,090][train_inner][INFO] - {"epoch": 112, "update": 111.657, "loss": "5.131", "nll_loss": "0.014", "loss_recon": "0.101", "loss_info_nce": "4.122", "ppl": "1.01", "wps": "1420.5", "ups": "0.48", "wpb": "2956.5", "bsz": "385.4", "num_updates": "27000", "lr": "5e-05", "gnorm": "0.947", "clip": "0", "loss_scale": "65536", "train_wall": "411", "gb_free": "2.9", "wall": "58064"}
[2025-07-05 09:33:48,651][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 112 @ 27083 updates
[2025-07-05 09:33:48,667][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint112.pt
[2025-07-05 09:33:50,087][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint112.pt
[2025-07-05 09:33:50,679][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint112.pt (epoch 112 @ 27083 updates, score None) (writing took 2.013204599999881 seconds)
[2025-07-05 09:33:50,679][fairseq_cli.train][INFO] - end of epoch 112 (average epoch stats below)
[2025-07-05 09:33:50,679][train][INFO] - {"epoch": 112, "train_loss": "5.132", "train_nll_loss": "0.014", "train_loss_recon": "0.101", "train_loss_info_nce": "4.124", "train_ppl": "1.01", "train_wps": "1412.8", "train_ups": "0.48", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "27083", "train_lr": "4.98196e-05", "train_gnorm": "0.926", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "501", "train_gb_free": "2.9", "train_wall": "58239"}
[2025-07-05 09:33:50,877][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 09:33:50,877][fairseq.trainer][INFO] - begin training epoch 113
[2025-07-05 09:33:50,877][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 09:37:57,491][train_inner][INFO] - {"epoch": 113, "update": 112.483, "loss": "5.133", "nll_loss": "0.014", "loss_recon": "0.101", "loss_info_nce": "4.125", "ppl": "1.01", "wps": "1399.4", "ups": "0.47", "wpb": "2955.5", "bsz": "385.4", "num_updates": "27200", "lr": "4.95652e-05", "gnorm": "0.965", "clip": "0", "loss_scale": "65536", "train_wall": "417", "gb_free": "2.9", "wall": "58486"}
[2025-07-05 09:42:20,668][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 113 @ 27325 updates
[2025-07-05 09:42:20,668][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint113.pt
[2025-07-05 09:42:21,977][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint113.pt
[2025-07-05 09:42:22,486][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint113.pt (epoch 113 @ 27325 updates, score None) (writing took 1.824310299998615 seconds)
[2025-07-05 09:42:22,501][fairseq_cli.train][INFO] - end of epoch 113 (average epoch stats below)
[2025-07-05 09:42:22,501][train][INFO] - {"epoch": 113, "train_loss": "5.127", "train_nll_loss": "0.014", "train_loss_recon": "0.101", "train_loss_info_nce": "4.119", "train_ppl": "1.01", "train_wps": "1398", "train_ups": "0.47", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "27325", "train_lr": "4.92935e-05", "train_gnorm": "0.938", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "507", "train_gb_free": "2.9", "train_wall": "58751"}
[2025-07-05 09:42:22,699][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 09:42:22,699][fairseq.trainer][INFO] - begin training epoch 114
[2025-07-05 09:42:22,699][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 09:45:01,365][train_inner][INFO] - {"epoch": 114, "update": 113.31, "loss": "5.117", "nll_loss": "0.014", "loss_recon": "0.1", "loss_info_nce": "4.112", "ppl": "1.01", "wps": "1392.8", "ups": "0.47", "wpb": "2951.8", "bsz": "386.6", "num_updates": "27400", "lr": "4.91304e-05", "gnorm": "0.91", "clip": "0", "loss_scale": "65536", "train_wall": "419", "gb_free": "2.9", "wall": "58910"}
[2025-07-05 09:50:48,094][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 114 @ 27567 updates
[2025-07-05 09:50:48,094][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint114.pt
[2025-07-05 09:50:49,533][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint114.pt
[2025-07-05 09:50:49,989][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint114.pt (epoch 114 @ 27567 updates, score None) (writing took 1.8950264999948558 seconds)
[2025-07-05 09:50:49,989][fairseq_cli.train][INFO] - end of epoch 114 (average epoch stats below)
[2025-07-05 09:50:49,989][train][INFO] - {"epoch": 114, "train_loss": "5.126", "train_nll_loss": "0.014", "train_loss_recon": "0.101", "train_loss_info_nce": "4.119", "train_ppl": "1.01", "train_wps": "1410", "train_ups": "0.48", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "27567", "train_lr": "4.87674e-05", "train_gnorm": "1.022", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "502", "train_gb_free": "2.9", "train_wall": "59258"}
[2025-07-05 09:50:50,202][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 09:50:50,202][fairseq.trainer][INFO] - begin training epoch 115
[2025-07-05 09:50:50,202][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 09:52:02,364][train_inner][INFO] - {"epoch": 115, "update": 114.136, "loss": "5.131", "nll_loss": "0.014", "loss_recon": "0.101", "loss_info_nce": "4.121", "ppl": "1.01", "wps": "1405", "ups": "0.48", "wpb": "2957.4", "bsz": "385.1", "num_updates": "27600", "lr": "4.86957e-05", "gnorm": "1.025", "clip": "0", "loss_scale": "65536", "train_wall": "416", "gb_free": "2.9", "wall": "59331"}
[2025-07-05 09:59:06,849][train_inner][INFO] - {"epoch": 115, "update": 114.963, "loss": "5.117", "nll_loss": "0.014", "loss_recon": "0.1", "loss_info_nce": "4.113", "ppl": "1.01", "wps": "1396", "ups": "0.47", "wpb": "2963", "bsz": "387.1", "num_updates": "27800", "lr": "4.82609e-05", "gnorm": "0.882", "clip": "0", "loss_scale": "65536", "train_wall": "424", "gb_free": "2.9", "wall": "59755"}
[2025-07-05 09:59:24,532][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-05 09:59:55,770][valid][INFO] - {"epoch": 115, "valid_loss": "4.872", "valid_nll_loss": "0.013", "valid_loss_recon": "0.095", "valid_loss_info_nce": "3.917", "valid_ppl": "1.01", "valid_wps": "2641.9", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "27809", "valid_best_loss": "4.839"}
[2025-07-05 09:59:55,771][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 115 @ 27809 updates
[2025-07-05 09:59:55,772][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint115.pt
[2025-07-05 09:59:57,149][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint115.pt
[2025-07-05 09:59:57,640][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint115.pt (epoch 115 @ 27809 updates, score 4.872) (writing took 1.8684393000003183 seconds)
[2025-07-05 09:59:57,641][fairseq_cli.train][INFO] - end of epoch 115 (average epoch stats below)
[2025-07-05 09:59:57,648][train][INFO] - {"epoch": 115, "train_loss": "5.119", "train_nll_loss": "0.014", "train_loss_recon": "0.101", "train_loss_info_nce": "4.114", "train_ppl": "1.01", "train_wps": "1306.6", "train_ups": "0.44", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "27809", "train_lr": "4.82413e-05", "train_gnorm": "0.885", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "511", "train_gb_free": "2.9", "train_wall": "59806"}
[2025-07-05 09:59:57,872][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 09:59:57,876][fairseq.trainer][INFO] - begin training epoch 116
[2025-07-05 09:59:57,876][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 10:06:46,341][train_inner][INFO] - {"epoch": 116, "update": 115.789, "loss": "5.119", "nll_loss": "0.014", "loss_recon": "0.101", "loss_info_nce": "4.113", "ppl": "1.01", "wps": "1285.7", "ups": "0.44", "wpb": "2953.7", "bsz": "385.9", "num_updates": "28000", "lr": "4.78261e-05", "gnorm": "1.035", "clip": "0", "loss_scale": "65536", "train_wall": "423", "gb_free": "2.9", "wall": "60215"}
[2025-07-05 10:08:38,732][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 116 @ 28051 updates
[2025-07-05 10:08:38,732][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint116.pt
[2025-07-05 10:08:40,075][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint116.pt
[2025-07-05 10:08:40,575][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint116.pt (epoch 116 @ 28051 updates, score None) (writing took 1.8509225999951013 seconds)
[2025-07-05 10:08:40,575][fairseq_cli.train][INFO] - end of epoch 116 (average epoch stats below)
[2025-07-05 10:08:40,592][train][INFO] - {"epoch": 116, "train_loss": "5.117", "train_nll_loss": "0.014", "train_loss_recon": "0.101", "train_loss_info_nce": "4.112", "train_ppl": "1.01", "train_wps": "1368.3", "train_ups": "0.46", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "28051", "train_lr": "4.77152e-05", "train_gnorm": "0.99", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "518", "train_gb_free": "2.9", "train_wall": "60329"}
[2025-07-05 10:08:40,792][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 10:08:40,792][fairseq.trainer][INFO] - begin training epoch 117
[2025-07-05 10:08:40,792][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 10:14:10,694][train_inner][INFO] - {"epoch": 117, "update": 116.616, "loss": "5.118", "nll_loss": "0.014", "loss_recon": "0.101", "loss_info_nce": "4.113", "ppl": "1.01", "wps": "1330.7", "ups": "0.45", "wpb": "2956.5", "bsz": "385.1", "num_updates": "28200", "lr": "4.73913e-05", "gnorm": "0.856", "clip": "0", "loss_scale": "65536", "train_wall": "439", "gb_free": "2.9", "wall": "60659"}
[2025-07-05 10:17:32,491][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 117 @ 28293 updates
[2025-07-05 10:17:32,491][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint117.pt
[2025-07-05 10:17:33,849][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint117.pt
[2025-07-05 10:17:34,370][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint117.pt (epoch 117 @ 28293 updates, score None) (writing took 1.8727043000035337 seconds)
[2025-07-05 10:17:34,370][fairseq_cli.train][INFO] - end of epoch 117 (average epoch stats below)
[2025-07-05 10:17:34,375][train][INFO] - {"epoch": 117, "train_loss": "5.119", "train_nll_loss": "0.014", "train_loss_recon": "0.101", "train_loss_info_nce": "4.114", "train_ppl": "1.01", "train_wps": "1340.5", "train_ups": "0.45", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "28293", "train_lr": "4.71891e-05", "train_gnorm": "0.924", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "528", "train_gb_free": "2.9", "train_wall": "60863"}
[2025-07-05 10:17:34,580][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 10:17:34,584][fairseq.trainer][INFO] - begin training epoch 118
[2025-07-05 10:17:34,584][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 10:17:56,191][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 65536.0
[2025-07-05 10:21:32,754][train_inner][INFO] - {"epoch": 118, "update": 117.446, "loss": "5.122", "nll_loss": "0.014", "loss_recon": "0.1", "loss_info_nce": "4.117", "ppl": "1.01", "wps": "1338", "ups": "0.45", "wpb": "2957.4", "bsz": "385.6", "num_updates": "28400", "lr": "4.69565e-05", "gnorm": "1.082", "clip": "0", "loss_scale": "65536", "train_wall": "437", "gb_free": "2.9", "wall": "61101"}
[2025-07-05 10:26:19,667][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 118 @ 28534 updates
[2025-07-05 10:26:19,667][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint118.pt
[2025-07-05 10:26:26,392][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint118.pt
[2025-07-05 10:26:26,821][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint118.pt (epoch 118 @ 28534 updates, score None) (writing took 7.1574007000017446 seconds)
[2025-07-05 10:26:26,821][fairseq_cli.train][INFO] - end of epoch 118 (average epoch stats below)
[2025-07-05 10:26:26,837][train][INFO] - {"epoch": 118, "train_loss": "5.117", "train_nll_loss": "0.014", "train_loss_recon": "0.1", "train_loss_info_nce": "4.112", "train_ppl": "1.01", "train_wps": "1338.3", "train_ups": "0.45", "train_wpb": "2956.7", "train_bsz": "385.8", "train_num_updates": "28534", "train_lr": "4.66652e-05", "train_gnorm": "0.981", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "522", "train_gb_free": "2.9", "train_wall": "61395"}
[2025-07-05 10:26:27,104][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 10:26:27,104][fairseq.trainer][INFO] - begin training epoch 119
[2025-07-05 10:26:27,104][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 10:28:48,750][train_inner][INFO] - {"epoch": 119, "update": 118.273, "loss": "5.115", "nll_loss": "0.014", "loss_recon": "0.1", "loss_info_nce": "4.111", "ppl": "1.01", "wps": "1355.3", "ups": "0.46", "wpb": "2954.6", "bsz": "385.4", "num_updates": "28600", "lr": "4.65217e-05", "gnorm": "0.892", "clip": "0", "loss_scale": "65536", "train_wall": "426", "gb_free": "2.9", "wall": "61537"}
[2025-07-05 10:34:58,575][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 119 @ 28776 updates
[2025-07-05 10:34:58,575][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint119.pt
[2025-07-05 10:34:59,991][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint119.pt
[2025-07-05 10:35:00,535][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint119.pt (epoch 119 @ 28776 updates, score None) (writing took 1.9502592999997432 seconds)
[2025-07-05 10:35:00,535][fairseq_cli.train][INFO] - end of epoch 119 (average epoch stats below)
[2025-07-05 10:35:00,535][train][INFO] - {"epoch": 119, "train_loss": "5.112", "train_nll_loss": "0.014", "train_loss_recon": "0.1", "train_loss_info_nce": "4.108", "train_ppl": "1.01", "train_wps": "1392.9", "train_ups": "0.47", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "28776", "train_lr": "4.61391e-05", "train_gnorm": "0.877", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "508", "train_gb_free": "2.9", "train_wall": "61909"}
[2025-07-05 10:35:00,767][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 10:35:00,777][fairseq.trainer][INFO] - begin training epoch 120
[2025-07-05 10:35:00,777][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 10:35:52,770][train_inner][INFO] - {"epoch": 120, "update": 119.099, "loss": "5.112", "nll_loss": "0.014", "loss_recon": "0.101", "loss_info_nce": "4.107", "ppl": "1.01", "wps": "1394.1", "ups": "0.47", "wpb": "2955.5", "bsz": "385.6", "num_updates": "28800", "lr": "4.6087e-05", "gnorm": "0.851", "clip": "0", "loss_scale": "65536", "train_wall": "419", "gb_free": "2.9", "wall": "61961"}
[2025-07-05 10:42:50,140][train_inner][INFO] - {"epoch": 120, "update": 119.926, "loss": "5.108", "nll_loss": "0.014", "loss_recon": "0.1", "loss_info_nce": "4.105", "ppl": "1.01", "wps": "1419.8", "ups": "0.48", "wpb": "2963", "bsz": "387.1", "num_updates": "29000", "lr": "4.56522e-05", "gnorm": "0.922", "clip": "0", "loss_scale": "65536", "train_wall": "416", "gb_free": "2.9", "wall": "62379"}
[2025-07-05 10:43:26,237][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-05 10:43:56,687][valid][INFO] - {"epoch": 120, "valid_loss": "4.82", "valid_nll_loss": "0.013", "valid_loss_recon": "0.091", "valid_loss_info_nce": "3.909", "valid_ppl": "1.01", "valid_wps": "2716", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "29018", "valid_best_loss": "4.82"}
[2025-07-05 10:43:56,703][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 120 @ 29018 updates
[2025-07-05 10:43:56,703][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint120.pt
[2025-07-05 10:43:58,115][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint120.pt
[2025-07-05 10:43:58,975][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint120.pt (epoch 120 @ 29018 updates, score 4.82) (writing took 2.2839692999987165 seconds)
[2025-07-05 10:43:58,975][fairseq_cli.train][INFO] - end of epoch 120 (average epoch stats below)
[2025-07-05 10:43:58,990][train][INFO] - {"epoch": 120, "train_loss": "5.109", "train_nll_loss": "0.014", "train_loss_recon": "0.1", "train_loss_info_nce": "4.106", "train_ppl": "1.01", "train_wps": "1328.9", "train_ups": "0.45", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "29018", "train_lr": "4.5613e-05", "train_gnorm": "0.914", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "502", "train_gb_free": "2.9", "train_wall": "62447"}
[2025-07-05 10:43:59,249][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 10:43:59,249][fairseq.trainer][INFO] - begin training epoch 121
[2025-07-05 10:43:59,249][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 10:50:21,089][train_inner][INFO] - {"epoch": 121, "update": 120.752, "loss": "5.113", "nll_loss": "0.014", "loss_recon": "0.1", "loss_info_nce": "4.109", "ppl": "1.01", "wps": "1311.3", "ups": "0.44", "wpb": "2956.5", "bsz": "385.1", "num_updates": "29200", "lr": "4.52174e-05", "gnorm": "0.933", "clip": "0", "loss_scale": "65536", "train_wall": "415", "gb_free": "2.9", "wall": "62830"}
[2025-07-05 10:52:25,901][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 121 @ 29260 updates
[2025-07-05 10:52:25,901][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint121.pt
[2025-07-05 10:52:27,361][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint121.pt
[2025-07-05 10:52:27,886][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint121.pt (epoch 121 @ 29260 updates, score None) (writing took 1.983518400003959 seconds)
[2025-07-05 10:52:27,886][fairseq_cli.train][INFO] - end of epoch 121 (average epoch stats below)
[2025-07-05 10:52:27,912][train][INFO] - {"epoch": 121, "train_loss": "5.11", "train_nll_loss": "0.014", "train_loss_recon": "0.1", "train_loss_info_nce": "4.107", "train_ppl": "1.01", "train_wps": "1406.1", "train_ups": "0.48", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "29260", "train_lr": "4.5087e-05", "train_gnorm": "0.919", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "503", "train_gb_free": "2.9", "train_wall": "62956"}
[2025-07-05 10:52:28,135][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 10:52:28,135][fairseq.trainer][INFO] - begin training epoch 122
[2025-07-05 10:52:28,135][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 10:57:24,170][train_inner][INFO] - {"epoch": 122, "update": 121.579, "loss": "5.103", "nll_loss": "0.014", "loss_recon": "0.1", "loss_info_nce": "4.1", "ppl": "1.01", "wps": "1396.7", "ups": "0.47", "wpb": "2954.6", "bsz": "386.1", "num_updates": "29400", "lr": "4.47826e-05", "gnorm": "0.879", "clip": "0", "loss_scale": "65536", "train_wall": "418", "gb_free": "2.9", "wall": "63253"}
[2025-07-05 11:00:55,436][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 122 @ 29502 updates
[2025-07-05 11:00:55,436][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint122.pt
[2025-07-05 11:00:56,806][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint122.pt
[2025-07-05 11:00:57,272][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint122.pt (epoch 122 @ 29502 updates, score None) (writing took 1.8316952000022866 seconds)
[2025-07-05 11:00:57,272][fairseq_cli.train][INFO] - end of epoch 122 (average epoch stats below)
[2025-07-05 11:00:57,288][train][INFO] - {"epoch": 122, "train_loss": "5.102", "train_nll_loss": "0.014", "train_loss_recon": "0.1", "train_loss_info_nce": "4.1", "train_ppl": "1.01", "train_wps": "1404.8", "train_ups": "0.48", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "29502", "train_lr": "4.45609e-05", "train_gnorm": "0.891", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "504", "train_gb_free": "2.9", "train_wall": "63466"}
[2025-07-05 11:00:57,462][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 11:00:57,462][fairseq.trainer][INFO] - begin training epoch 123
[2025-07-05 11:00:57,462][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 11:04:38,157][train_inner][INFO] - {"epoch": 123, "update": 122.405, "loss": "5.101", "nll_loss": "0.014", "loss_recon": "0.1", "loss_info_nce": "4.102", "ppl": "1.01", "wps": "1360.3", "ups": "0.46", "wpb": "2951.8", "bsz": "386.3", "num_updates": "29600", "lr": "4.43478e-05", "gnorm": "0.867", "clip": "0", "loss_scale": "65536", "train_wall": "429", "gb_free": "2.9", "wall": "63687"}
[2025-07-05 11:11:07,590][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 123 @ 29744 updates
[2025-07-05 11:11:07,591][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint123.pt
[2025-07-05 11:11:09,140][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint123.pt
[2025-07-05 11:11:10,102][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint123.pt (epoch 123 @ 29744 updates, score None) (writing took 2.5128594000052544 seconds)
[2025-07-05 11:11:10,103][fairseq_cli.train][INFO] - end of epoch 123 (average epoch stats below)
[2025-07-05 11:11:10,109][train][INFO] - {"epoch": 123, "train_loss": "5.107", "train_nll_loss": "0.014", "train_loss_recon": "0.1", "train_loss_info_nce": "4.104", "train_ppl": "1.01", "train_wps": "1167.6", "train_ups": "0.39", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "29744", "train_lr": "4.40348e-05", "train_gnorm": "0.947", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "607", "train_gb_free": "2.9", "train_wall": "64079"}
[2025-07-05 11:11:10,520][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 11:11:10,526][fairseq.trainer][INFO] - begin training epoch 124
[2025-07-05 11:11:10,527][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 11:13:51,506][train_inner][INFO] - {"epoch": 124, "update": 123.231, "loss": "5.109", "nll_loss": "0.014", "loss_recon": "0.1", "loss_info_nce": "4.104", "ppl": "1.01", "wps": "1067.9", "ups": "0.36", "wpb": "2954.6", "bsz": "386.1", "num_updates": "29800", "lr": "4.3913e-05", "gnorm": "0.977", "clip": "0", "loss_scale": "65536", "train_wall": "546", "gb_free": "2.9", "wall": "64240"}
[2025-07-05 11:21:16,759][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 124 @ 29986 updates
[2025-07-05 11:21:16,759][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint124.pt
[2025-07-05 11:21:18,203][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint124.pt
[2025-07-05 11:21:18,696][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint124.pt (epoch 124 @ 29986 updates, score None) (writing took 1.9376944000032381 seconds)
[2025-07-05 11:21:18,697][fairseq_cli.train][INFO] - end of epoch 124 (average epoch stats below)
[2025-07-05 11:21:18,703][train][INFO] - {"epoch": 124, "train_loss": "5.108", "train_nll_loss": "0.014", "train_loss_recon": "0.1", "train_loss_info_nce": "4.105", "train_ppl": "1.01", "train_wps": "1175.7", "train_ups": "0.4", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "29986", "train_lr": "4.35087e-05", "train_gnorm": "0.914", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "601", "train_gb_free": "2.9", "train_wall": "64687"}
[2025-07-05 11:21:18,919][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 11:21:18,922][fairseq.trainer][INFO] - begin training epoch 125
[2025-07-05 11:21:18,923][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 11:21:53,585][train_inner][INFO] - {"epoch": 125, "update": 124.058, "loss": "5.108", "nll_loss": "0.014", "loss_recon": "0.1", "loss_info_nce": "4.104", "ppl": "1.01", "wps": "1227.3", "ups": "0.41", "wpb": "2958.3", "bsz": "384.1", "num_updates": "30000", "lr": "4.34783e-05", "gnorm": "0.938", "clip": "0", "loss_scale": "65536", "train_wall": "476", "gb_free": "2.9", "wall": "64722"}
[2025-07-05 11:21:53,586][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-05 11:22:31,905][valid][INFO] - {"epoch": 125, "valid_loss": "4.823", "valid_nll_loss": "0.013", "valid_loss_recon": "0.092", "valid_loss_info_nce": "3.903", "valid_ppl": "1.01", "valid_wps": "2153.8", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "30000", "valid_best_loss": "4.82"}
[2025-07-05 11:30:07,530][train_inner][INFO] - {"epoch": 125, "update": 124.884, "loss": "5.102", "nll_loss": "0.014", "loss_recon": "0.1", "loss_info_nce": "4.1", "ppl": "1.01", "wps": "1200.1", "ups": "0.4", "wpb": "2963.9", "bsz": "387.3", "num_updates": "30200", "lr": "4.30435e-05", "gnorm": "0.916", "clip": "0", "loss_scale": "65536", "train_wall": "455", "gb_free": "2.9", "wall": "65216"}
[2025-07-05 11:31:08,320][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-05 11:31:44,910][valid][INFO] - {"epoch": 125, "valid_loss": "4.82", "valid_nll_loss": "0.013", "valid_loss_recon": "0.091", "valid_loss_info_nce": "3.908", "valid_ppl": "1.01", "valid_wps": "2249.9", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "30228", "valid_best_loss": "4.82"}
[2025-07-05 11:31:44,911][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 125 @ 30228 updates
[2025-07-05 11:31:44,912][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint125.pt
[2025-07-05 11:31:46,246][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint125.pt
[2025-07-05 11:31:47,409][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint125.pt (epoch 125 @ 30228 updates, score 4.82) (writing took 2.4980022000017925 seconds)
[2025-07-05 11:31:47,410][fairseq_cli.train][INFO] - end of epoch 125 (average epoch stats below)
[2025-07-05 11:31:47,485][train][INFO] - {"epoch": 125, "train_loss": "5.103", "train_nll_loss": "0.014", "train_loss_recon": "0.1", "train_loss_info_nce": "4.1", "train_ppl": "1.01", "train_wps": "1138.1", "train_ups": "0.38", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "30228", "train_lr": "4.29826e-05", "train_gnorm": "0.92", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "548", "train_gb_free": "2.9", "train_wall": "65316"}
[2025-07-05 11:31:47,724][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 11:31:47,727][fairseq.trainer][INFO] - begin training epoch 126
[2025-07-05 11:31:47,727][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 11:38:15,369][train_inner][INFO] - {"epoch": 126, "update": 125.711, "loss": "5.1", "nll_loss": "0.014", "loss_recon": "0.1", "loss_info_nce": "4.099", "ppl": "1.01", "wps": "1211.3", "ups": "0.41", "wpb": "2954.6", "bsz": "385.7", "num_updates": "30400", "lr": "4.26087e-05", "gnorm": "0.842", "clip": "0", "loss_scale": "131072", "train_wall": "445", "gb_free": "2.9", "wall": "65704"}
[2025-07-05 11:40:49,760][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 126 @ 30470 updates
[2025-07-05 11:40:49,761][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint126.pt
[2025-07-05 11:40:51,105][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250704 2en 1de w norm\output_model\checkpoints\checkpoint126.pt
[2025-07-05 11:40:51,644][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint126.pt (epoch 126 @ 30470 updates, score None) (writing took 1.8832508000050439 seconds)
[2025-07-05 11:40:51,644][fairseq_cli.train][INFO] - end of epoch 126 (average epoch stats below)
[2025-07-05 11:40:51,653][train][INFO] - {"epoch": 126, "train_loss": "5.098", "train_nll_loss": "0.014", "train_loss_recon": "0.1", "train_loss_info_nce": "4.097", "train_ppl": "1.01", "train_wps": "1314.9", "train_ups": "0.44", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "30470", "train_lr": "4.24565e-05", "train_gnorm": "0.852", "train_clip": "0", "train_loss_scale": "131072", "train_train_wall": "539", "train_gb_free": "2.9", "train_wall": "65860"}
[2025-07-05 11:40:51,880][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-05 11:40:51,884][fairseq.trainer][INFO] - begin training epoch 127
[2025-07-05 11:40:51,884][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-05 11:45:42,353][train_inner][INFO] - {"epoch": 127, "update": 126.537, "loss": "5.093", "nll_loss": "0.014", "loss_recon": "0.1", "loss_info_nce": "4.094", "ppl": "1.01", "wps": "1322", "ups": "0.45", "wpb": "2954.6", "bsz": "386.1", "num_updates": "30600", "lr": "4.21739e-05", "gnorm": "0.81", "clip": "0", "loss_scale": "131072", "train_wall": "442", "gb_free": "2.9", "wall": "66151"}
[2025-07-05 11:49:07,335][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 65536.0
