{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "zqEAfZZYIaiX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/incantator/Documents/mbari-mae\n"
     ]
    }
   ],
   "source": [
    "# Standard library\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "os.chdir(\"../..\")\n",
    "print(os.getcwd())\n",
    "\n",
    "# Numerical computing\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Hugging Face Transformers\n",
    "from transformers import (\n",
    "    HubertModel,\n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    Wav2Vec2Model,\n",
    ")\n",
    "\n",
    "# Datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "# PyTorch Lightning\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "# Metrics\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torchmetrics.classification import (\n",
    "    MulticlassF1Score,\n",
    "    MulticlassPrecision,\n",
    "    MulticlassRecall,\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "C5aQ8ZsGkD7s"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2418e5b2c6ca412e9722875a5caa2c2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fee526b6468416890187b4d2c4a5293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8107bfcc8fe340fca03c85721454b02b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 1017 examples, 31 classes\n",
      "  Clymene_Dolphin: 38\n",
      "  Bottlenose_Dolphin: 15\n",
      "  Spinner_Dolphin: 69\n",
      "  Beluga,_White_Whale: 30\n",
      "  Bearded_Seal: 22\n",
      "  Minke_Whale: 10\n",
      "  Humpback_Whale: 38\n",
      "  Southern_Right_Whale: 15\n",
      "  White-sided_Dolphin: 33\n",
      "  Narwhal: 30\n",
      "  White-beaked_Dolphin: 34\n",
      "  Northern_Right_Whale: 32\n",
      "  Frasers_Dolphin: 52\n",
      "  Grampus,_Rissos_Dolphin: 40\n",
      "  Harp_Seal: 28\n",
      "  Atlantic_Spotted_Dolphin: 35\n",
      "  Fin,_Finback_Whale: 30\n",
      "  Ross_Seal: 30\n",
      "  Rough-Toothed_Dolphin: 30\n",
      "  Killer_Whale: 21\n",
      "  Pantropical_Spotted_Dolphin: 40\n",
      "  Short-Finned_Pacific_Pilot_Whale: 40\n",
      "  Bowhead_Whale: 36\n",
      "  False_Killer_Whale: 35\n",
      "  Melon_Headed_Whale: 38\n",
      "  Long-Finned_Pilot_Whale: 42\n",
      "  Striped_Dolphin: 49\n",
      "  Leopard_Seal: 6\n",
      "  Walrus: 23\n",
      "  Sperm_Whale: 45\n",
      "  Common_Dolphin: 31\n",
      "Validation dataset: 339 examples, 31 classes\n",
      "  Clymene_Dolphin: 12\n",
      "  Bottlenose_Dolphin: 5\n",
      "  Spinner_Dolphin: 23\n",
      "  Beluga,_White_Whale: 10\n",
      "  Bearded_Seal: 7\n",
      "  Minke_Whale: 4\n",
      "  Humpback_Whale: 13\n",
      "  Southern_Right_Whale: 5\n",
      "  White-sided_Dolphin: 11\n",
      "  Narwhal: 10\n",
      "  White-beaked_Dolphin: 11\n",
      "  Northern_Right_Whale: 11\n",
      "  Frasers_Dolphin: 18\n",
      "  Grampus,_Rissos_Dolphin: 13\n",
      "  Harp_Seal: 9\n",
      "  Atlantic_Spotted_Dolphin: 12\n",
      "  Fin,_Finback_Whale: 10\n",
      "  Ross_Seal: 10\n",
      "  Rough-Toothed_Dolphin: 10\n",
      "  Killer_Whale: 7\n",
      "  Pantropical_Spotted_Dolphin: 13\n",
      "  Short-Finned_Pacific_Pilot_Whale: 13\n",
      "  Bowhead_Whale: 12\n",
      "  False_Killer_Whale: 12\n",
      "  Melon_Headed_Whale: 13\n",
      "  Long-Finned_Pilot_Whale: 14\n",
      "  Striped_Dolphin: 16\n",
      "  Leopard_Seal: 2\n",
      "  Walrus: 8\n",
      "  Sperm_Whale: 15\n",
      "  Common_Dolphin: 10\n",
      "Test dataset: 339 examples, 31 classes\n",
      "  Clymene_Dolphin: 13\n",
      "  Bottlenose_Dolphin: 4\n",
      "  Spinner_Dolphin: 22\n",
      "  Beluga,_White_Whale: 10\n",
      "  Bearded_Seal: 8\n",
      "  Minke_Whale: 3\n",
      "  Humpback_Whale: 13\n",
      "  Southern_Right_Whale: 5\n",
      "  White-sided_Dolphin: 11\n",
      "  Narwhal: 10\n",
      "  White-beaked_Dolphin: 12\n",
      "  Northern_Right_Whale: 11\n",
      "  Frasers_Dolphin: 17\n",
      "  Grampus,_Rissos_Dolphin: 14\n",
      "  Harp_Seal: 10\n",
      "  Atlantic_Spotted_Dolphin: 11\n",
      "  Fin,_Finback_Whale: 10\n",
      "  Ross_Seal: 10\n",
      "  Rough-Toothed_Dolphin: 10\n",
      "  Killer_Whale: 7\n",
      "  Pantropical_Spotted_Dolphin: 13\n",
      "  Short-Finned_Pacific_Pilot_Whale: 14\n",
      "  Bowhead_Whale: 12\n",
      "  False_Killer_Whale: 12\n",
      "  Melon_Headed_Whale: 12\n",
      "  Long-Finned_Pilot_Whale: 14\n",
      "  Striped_Dolphin: 16\n",
      "  Leopard_Seal: 2\n",
      "  Walrus: 7\n",
      "  Sperm_Whale: 15\n",
      "  Common_Dolphin: 11\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "data_dir = \"data/watkins\"\n",
    "annotations_file_train = os.path.join(data_dir, \"annotations.train.csv\")\n",
    "annotations_file_valid = os.path.join(data_dir, \"annotations.valid.csv\")\n",
    "annotations_file_test = os.path.join(data_dir, \"annotations.test.csv\")\n",
    "\n",
    "ds = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\"train\": annotations_file_train,\n",
    "                \"validation\": annotations_file_valid,\n",
    "                \"test\": annotations_file_test},\n",
    ")\n",
    "\n",
    "for split_name in [\"train\", \"validation\", \"test\"]:\n",
    "    split_dataset = ds[split_name]\n",
    "    labels = split_dataset[\"label\"]\n",
    "    total = len(labels)\n",
    "    counts = Counter(labels)\n",
    "\n",
    "    print(f\"{split_name.capitalize()} dataset: {total} examples, {len(counts)} classes\")\n",
    "    if \"label\" in split_dataset.features and hasattr(split_dataset.features[\"label\"], \"names\"):\n",
    "        class_names = split_dataset.features[\"label\"].names\n",
    "        for idx, name in enumerate(class_names):\n",
    "            print(f\"  {idx} ({name}): {counts.get(name, 0)}\")\n",
    "    else:\n",
    "        for label, count in counts.items():\n",
    "            print(f\"  {label}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class weights calculation\n",
    "train_labels = ds[\"train\"][\"label\"]\n",
    "unique_labels = sorted(set(train_labels))\n",
    "label_to_int = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "y_train = [label_to_int[lbl] for lbl in train_labels]\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.arange(len(unique_labels)),\n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "num_classes = len(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "6JYR66KeIUsg"
   },
   "outputs": [],
   "source": [
    "# Model definition\n",
    "class WMMDClassifier(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int,\n",
    "        lr: float = 1e-4,\n",
    "        backbone: str = \"facebook/wav2vec2-base\",\n",
    "        ckpt_path: str = \"\",\n",
    "        finetune: bool = False,\n",
    "        class_weights=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        if backbone == \"facebook/wav2vec2-base\":\n",
    "            self.backbone     = Wav2Vec2Model.from_pretrained(backbone)\n",
    "            self.embedding_dim = self.backbone.config.hidden_size\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported backbone '{backbone}'\")\n",
    "\n",
    "        try:\n",
    "            self.backbone.gradient_checkpointing_enable()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = finetune\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(self.embedding_dim, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, num_classes),\n",
    "        )\n",
    "\n",
    "        if class_weights is not None:\n",
    "            cw = torch.tensor(class_weights, dtype=torch.float)\n",
    "            self.criterion = nn.CrossEntropyLoss(weight=cw)\n",
    "        else:\n",
    "            self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        metrics_kwargs = dict(num_classes=num_classes, average='macro')\n",
    "        self.train_precision = MulticlassPrecision(**metrics_kwargs)\n",
    "        self.train_recall = MulticlassRecall(**metrics_kwargs)\n",
    "        self.train_f1 = MulticlassF1Score(**metrics_kwargs)\n",
    "        self.val_precision = MulticlassPrecision(**metrics_kwargs)\n",
    "        self.val_recall = MulticlassRecall(**metrics_kwargs)\n",
    "        self.val_f1 = MulticlassF1Score(**metrics_kwargs)\n",
    "        self.test_precision = MulticlassPrecision(**metrics_kwargs)\n",
    "        self.test_recall = MulticlassRecall(**metrics_kwargs)\n",
    "        self.test_f1 = MulticlassF1Score(**metrics_kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bname = self.hparams.backbone.lower()\n",
    "        if bname == \"facebook/wav2vec2-base\":\n",
    "            out = self.backbone(x)\n",
    "            hidden = out.last_hidden_state\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported backbone in forward(): '{self.hparams.backbone}'\")\n",
    "\n",
    "        emb = hidden.mean(dim=1)\n",
    "        return self.classifier(emb)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        self.log_batch_metrics(loss, preds, y, prefix='train')\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        self.log_batch_metrics(loss, preds, y, prefix='val')\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        self.log_batch_metrics(loss, preds, y, prefix='test')\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "    def log_batch_metrics(self, loss, preds, targets, prefix):\n",
    "        self.log(f'{prefix}_loss', loss, prog_bar=True, on_epoch=True)\n",
    "        acc = (preds == targets).float().mean()\n",
    "        self.log(f'{prefix}_acc', acc, prog_bar=True, on_epoch=True)\n",
    "        precision = getattr(self, f'{prefix}_precision')(preds, targets)\n",
    "        recall = getattr(self, f'{prefix}_recall')(preds, targets)\n",
    "        f1 = getattr(self, f'{prefix}_f1')(preds, targets)\n",
    "        self.log(f'{prefix}_precision', precision, on_epoch=True)\n",
    "        self.log(f'{prefix}_recall', recall, on_epoch=True)\n",
    "        self.log(f'{prefix}_f1', f1, on_epoch=True)\n",
    "\n",
    "    # 2. Save the model state at the end of training\n",
    "    def on_train_end(self):\n",
    "        save_dir = getattr(self, 'save_dir', None)\n",
    "        if save_dir:\n",
    "            self.save_model(save_dir)\n",
    "\n",
    "    def save_model(self):\n",
    "        # New folder structure: /model/<model_name>_<balance_flag>/<timestamp>/\n",
    "        base_dir = 'model'\n",
    "        bn = self.hparams.backbone.replace('/', '_')\n",
    "        # Determine balance flag based on class_weights\n",
    "        cw = getattr(self.hparams, 'class_weights', None)\n",
    "        balance_flag = 'imbalance' if cw is not None else 'balance'\n",
    "        timestamp = getattr(self, 'finish_time', datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "        folder = os.path.join(base_dir, f\"{bn}_{balance_flag}\", timestamp)\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "        # 1) checkpoint\n",
    "        ckpt_path = os.path.join(folder, f\"{timestamp}.pt\")\n",
    "        payload = {\n",
    "            'state_dict': self.state_dict(),\n",
    "            'hparams': dict(self.hparams)\n",
    "        }\n",
    "        for attr in ('test_results', 'finish_time', 'epochs_trained'):\n",
    "            if hasattr(self, attr):\n",
    "                payload[attr] = getattr(self, attr)\n",
    "        torch.save(payload, ckpt_path)\n",
    "\n",
    "        # 2) human-readable stats\n",
    "        stats_path = os.path.join(folder, f\"{timestamp}.txt\")\n",
    "        raw_hparams = dict(self.hparams)\n",
    "        serializable_hparams = {}\n",
    "        for k, v in raw_hparams.items():\n",
    "            if isinstance(v, np.ndarray):\n",
    "                serializable_hparams[k] = v.tolist()\n",
    "            elif isinstance(v, torch.Tensor):\n",
    "                serializable_hparams[k] = v.cpu().item() if v.ndim == 0 else v.cpu().tolist()\n",
    "            else:\n",
    "                serializable_hparams[k] = v\n",
    "\n",
    "        serializable_results = {}\n",
    "        if hasattr(self, 'test_results'):\n",
    "            for k, v in self.test_results.items():\n",
    "                serializable_results[k] = v.cpu().item() if isinstance(v, torch.Tensor) else v\n",
    "\n",
    "        with open(stats_path, 'w') as f:\n",
    "            f.write(f\"Model architecture:\\n{self}\\n\\n\")\n",
    "            f.write(\"Hyperparameters:\\n\")\n",
    "            f.write(json.dumps(serializable_hparams, indent=4))\n",
    "            f.write(\"\\n\\n\")\n",
    "            if serializable_results:\n",
    "                f.write(\"Test results:\\n\")\n",
    "                f.write(json.dumps(serializable_results, indent=4))\n",
    "                f.write(\"\\n\\n\")\n",
    "            if hasattr(self, 'epochs_trained'):\n",
    "                f.write(f\"Epochs trained: {self.epochs_trained}\\n\")\n",
    "\n",
    "        # store for downstream use\n",
    "        self._last_save_dir = folder\n",
    "        self._last_timestamp = timestamp\n",
    "        print(f\"Artifacts saved to {folder}/\")\n",
    "\n",
    "    # 3. Load back the model for inference or continuation\n",
    "    @classmethod\n",
    "    def load_model(cls, load_dir: str, map_location=None):\n",
    "        \"\"\"\n",
    "        Load a model checkpoint and hyperparameters from a directory.\n",
    "\n",
    "        Returns:\n",
    "            model (MammalClassifier): Loaded model\n",
    "        \"\"\"\n",
    "        # Load hyperparameters\n",
    "        hparams_path = os.path.join(load_dir, 'hparams.json')\n",
    "        with open(hparams_path, 'r') as f:\n",
    "            hparams = json.load(f)\n",
    "\n",
    "        # Instantiate model\n",
    "        model = cls(**hparams)\n",
    "        # Load checkpoint\n",
    "        ckpt_path = os.path.join(load_dir, f'{cls.__name__}.ckpt')\n",
    "        state = torch.load(ckpt_path, map_location=map_location)\n",
    "        model.load_state_dict(state['state_dict'])\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ycDcNmcwQ_ON"
   },
   "outputs": [],
   "source": [
    "def WMMD_Collate(batch):\n",
    "    waveforms, labels = zip(*batch)\n",
    "    max_len = max(w.shape[0] for w in waveforms)\n",
    "    min_len = 400\n",
    "    padded_len = max(max_len, min_len)\n",
    "\n",
    "    padded_waveforms = []\n",
    "    for waveform in waveforms:\n",
    "        padding_needed = padded_len - waveform.shape[0]\n",
    "        padded_waveform = torch.nn.functional.pad(waveform, (0, padding_needed))\n",
    "        padded_waveforms.append(padded_waveform)\n",
    "\n",
    "    padded = torch.stack(padded_waveforms, dim=0)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return padded, labels\n",
    "\n",
    "class WMMDSoundDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, backbone: str, target_sr: int = 2000):\n",
    "        \"\"\"\n",
    "        dataset: list of dicts with keys 'path' & 'label'\n",
    "        backbone: 'facebook/wav2vec2-base' or 'mae-ast'\n",
    "        target_sr: sampling rate (e.g. 2000)\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.backbone = backbone.lower()\n",
    "        self.target_sr = target_sr\n",
    "        self.resampler_cache = {}\n",
    "\n",
    "        if self.backbone == \"facebook/wav2vec2-base\":\n",
    "            self.processor = Wav2Vec2FeatureExtractor.from_pretrained(\n",
    "                \"facebook/wav2vec2-base\", return_attention_mask=False, sampling_rate=target_sr\n",
    "            )\n",
    "        elif self.backbone == \"mae-ast\":\n",
    "            self.processor = None\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported backbone '{backbone}'\")\n",
    "\n",
    "        labels = sorted({item['label'] for item in dataset})\n",
    "        self.label_to_int = {lbl: i for i, lbl in enumerate(labels)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        audio_path = item[\"path\"]\n",
    "        waveform, orig_sr = torchaudio.load(audio_path)\n",
    "\n",
    "        if orig_sr != self.target_sr:\n",
    "            if orig_sr not in self.resampler_cache:\n",
    "                self.resampler_cache[orig_sr] = torchaudio.transforms.Resample(orig_sr, self.target_sr)\n",
    "            waveform = self.resampler_cache[orig_sr](waveform)\n",
    "\n",
    "        waveform = waveform / (waveform.abs().max() + 1e-6)\n",
    "        wav_1d = waveform.squeeze(0)  \n",
    "        \n",
    "        if self.backbone == \"facebook/wav2vec2-base\":\n",
    "            arr = wav_1d.numpy()\n",
    "            feats = self.processor(arr, sampling_rate=self.target_sr, return_tensors=\"pt\")\n",
    "            inp = feats.input_values.squeeze(0)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported backbone '{self.backbone}'\")\n",
    "\n",
    "        lbl = self.label_to_int[item['label']]\n",
    "        return inp, lbl\n",
    "\n",
    "class WMMDDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_dict,\n",
    "        backbone: str,\n",
    "        batch_size: int = 2,\n",
    "        num_workers: int = 1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dataset_dict = dataset_dict\n",
    "        self.backbone = backbone   # ‚Üê store it here\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # pass backbone into each split\n",
    "        self.train_ds = WMMDSoundDataset(self.dataset_dict[\"train\"], backbone=self.backbone)\n",
    "        self.val_ds   = WMMDSoundDataset(self.dataset_dict[\"validation\"], backbone=self.backbone)\n",
    "        self.test_ds  = WMMDSoundDataset(self.dataset_dict[\"test\"], backbone=self.backbone)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            collate_fn=WMMD_Collate\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            collate_fn=WMMD_Collate\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        # Use the test split for testing\n",
    "        return DataLoader(\n",
    "            self.test_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            collate_fn=WMMD_Collate\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "kQ2XWE-pRKAM"
   },
   "outputs": [],
   "source": [
    "#callback for logging metrics\n",
    "class MetricsLogger(pl.Callback):\n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accs = []\n",
    "        self.val_accs = []\n",
    "        self.train_precisions = []\n",
    "        self.val_precisions = []\n",
    "        self.train_recalls = []\n",
    "        self.val_recalls = []\n",
    "        self.train_f1s = []\n",
    "        self.val_f1s = []\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        m = trainer.callback_metrics\n",
    "        self.train_losses.append(m['train_loss'].item())\n",
    "        self.train_accs.append(m['train_acc'].item())\n",
    "        self.train_precisions.append(m['train_precision'].item())\n",
    "        self.train_recalls.append(m['train_recall'].item())\n",
    "        self.train_f1s.append(m['train_f1'].item())\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        m = trainer.callback_metrics\n",
    "        self.val_losses.append(m['val_loss'].item())\n",
    "        self.val_accs.append(m['val_acc'].item())\n",
    "        self.val_precisions.append(m['val_precision'].item())\n",
    "        self.val_recalls.append(m['val_recall'].item())\n",
    "        self.val_f1s.append(m['val_f1'].item())\n",
    "\n",
    "# callback for early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    patience=10,\n",
    "    min_delta=0.01,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configurations\n",
    "model_configs = [\n",
    "    {\"num_classes\": num_classes, \"lr\": 1e-3, \"backbone\": \"facebook/wav2vec2-base\",  \"finetune\": False, \"class_weights\": class_weights, \"max_epochs\": 3},\n",
    "    # {\"num_classes\": num_classes, \"lr\": 1e-3, \"backbone\": \"mae-ast\",  \"finetune\": False, \"class_weights\": class_weights, \"max_epochs\": 3, \"ckpt_path\": \"4Enc_1Dec-61epoch-0.103loss.pt\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "9k7VBnf5TWfW"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2359ed9523164077ad5b48a54b7b5343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/incantator/Documents/mbari-mae/venv/lib/python3.9/site-packages/transformers/configuration_utils.py:309: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d55ec6f83cc74ba0b7e0a841a1fcb907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/380M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 10\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cfg \u001b[38;5;129;01min\u001b[39;00m model_configs:\n\u001b[1;32m      3\u001b[0m     dm \u001b[38;5;241m=\u001b[39m WMMDDataModule(\n\u001b[1;32m      4\u001b[0m         dataset_dict\u001b[38;5;241m=\u001b[39mds,\n\u001b[1;32m      5\u001b[0m         backbone\u001b[38;5;241m=\u001b[39mcfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackbone\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      6\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m      7\u001b[0m         num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      8\u001b[0m     )\n\u001b[0;32m---> 10\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mWMMDClassifier\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_classes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbackbone\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbackbone\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinetune\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfinetune\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclass_weights\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     metrics_cb \u001b[38;5;241m=\u001b[39m MetricsLogger()\n\u001b[1;32m     16\u001b[0m     callbacks \u001b[38;5;241m=\u001b[39m [metrics_cb, early_stopping]\n",
      "Cell \u001b[0;32mIn[14], line 16\u001b[0m, in \u001b[0;36mWMMDClassifier.__init__\u001b[0;34m(self, num_classes, lr, backbone, ckpt_path, finetune, class_weights)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_hyperparameters()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backbone \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/wav2vec2-base\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone     \u001b[38;5;241m=\u001b[39m \u001b[43mWav2Vec2Model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhidden_size\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/mbari-mae/venv/lib/python3.9/site-packages/transformers/modeling_utils.py:311\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    313\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m~/Documents/mbari-mae/venv/lib/python3.9/site-packages/transformers/modeling_utils.py:4833\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4823\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4824\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   4826\u001b[0m     (\n\u001b[1;32m   4827\u001b[0m         model,\n\u001b[1;32m   4828\u001b[0m         missing_keys,\n\u001b[1;32m   4829\u001b[0m         unexpected_keys,\n\u001b[1;32m   4830\u001b[0m         mismatched_keys,\n\u001b[1;32m   4831\u001b[0m         offload_index,\n\u001b[1;32m   4832\u001b[0m         error_msgs,\n\u001b[0;32m-> 4833\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4834\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4835\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4836\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4837\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4838\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4839\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4842\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4846\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4849\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4851\u001b[0m \u001b[38;5;66;03m# record tp degree the model sharded to\u001b[39;00m\n\u001b[1;32m   4852\u001b[0m model\u001b[38;5;241m.\u001b[39m_tp_size \u001b[38;5;241m=\u001b[39m tp_size\n",
      "File \u001b[0;32m~/Documents/mbari-mae/venv/lib/python3.9/site-packages/transformers/modeling_utils.py:5099\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[0m\n\u001b[1;32m   5096\u001b[0m     original_checkpoint_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(state_dict\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m   5097\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5098\u001b[0m     original_checkpoint_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m-> 5099\u001b[0m         \u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m   5100\u001b[0m     )\n\u001b[1;32m   5102\u001b[0m \u001b[38;5;66;03m# Check if we are in a special state, i.e. loading from a state dict coming from a different architecture\u001b[39;00m\n\u001b[1;32m   5103\u001b[0m prefix \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mbase_model_prefix\n",
      "File \u001b[0;32m~/Documents/mbari-mae/venv/lib/python3.9/site-packages/transformers/modeling_utils.py:556\u001b[0m, in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file, is_quantized, map_location, weights_only)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;66;03m# Fallback to torch.load (if weights_only was explicitly False, do not check safety as this is known to be unsafe)\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[0;32m--> 556\u001b[0m     \u001b[43mcheck_torch_load_is_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m map_location \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/mbari-mae/venv/lib/python3.9/site-packages/transformers/utils/import_utils.py:1517\u001b[0m, in \u001b[0;36mcheck_torch_load_is_safe\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcheck_torch_load_is_safe\u001b[39m():\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_greater_or_equal(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.6\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1517\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1518\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDue to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1519\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1520\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhen loading files with safetensors.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1521\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1522\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3ad76eccdcc4d889866e2769b3e7f8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/380M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training Loops\n",
    "for cfg in model_configs:\n",
    "    dm = WMMDDataModule(\n",
    "        dataset_dict=ds,\n",
    "        backbone=cfg[\"backbone\"],\n",
    "        batch_size=2,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    model = WMMDClassifier(\n",
    "        num_classes=cfg['num_classes'], lr=cfg['lr'],\n",
    "        backbone=cfg['backbone'], finetune=cfg['finetune'],\n",
    "        class_weights=cfg['class_weights']\n",
    "    )\n",
    "    metrics_cb = MetricsLogger()\n",
    "    callbacks = [metrics_cb, early_stopping]\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=cfg['max_epochs'],\n",
    "        accelerator='gpu', devices=1,\n",
    "        precision='16-mixed', accumulate_grad_batches=2,\n",
    "        check_val_every_n_epoch=1,\n",
    "        num_sanity_val_steps=0,\n",
    "        enable_progress_bar=True, log_every_n_steps=1,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    # Train and test\n",
    "    trainer.fit(model, dm)\n",
    "    test_res = trainer.test(model, dm)[0]\n",
    "\n",
    "    # Attach metadata and save\n",
    "    model.test_results = test_res\n",
    "    model.finish_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model.epochs_trained = trainer.current_epoch + 1\n",
    "    model.save_model()\n",
    "\n",
    "    # Plot train/val curves\n",
    "    metrics_map = {\n",
    "        'accuracy':   ('train_accs',      'val_accs'),\n",
    "        'precision':  ('train_precisions','val_precisions'),\n",
    "        'recall':     ('train_recalls',   'val_recalls'),\n",
    "        'f1_score':   ('train_f1s',       'val_f1s'),\n",
    "    }\n",
    "    \n",
    "    for metric_name, (train_attr, val_attr) in metrics_map.items():\n",
    "        train_vals = getattr(metrics_cb, train_attr)\n",
    "        val_vals = getattr(metrics_cb, val_attr)\n",
    "        epochs = list(range(1, len(train_vals) + 1))\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(epochs, train_vals, label=f'train_{metric_name}')\n",
    "        plt.plot(epochs, val_vals,   label=f'val_{metric_name}')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(metric_name.replace('_', ' ').title())\n",
    "        plt.title(f\"{metric_name.replace('_', ' ').title()} over Epochs {model._last_timestamp}\")\n",
    "        plt.grid(True)\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "        plot_file = os.path.join(model._last_save_dir, f\"{model._last_timestamp}_{metric_name}.png\")\n",
    "        plt.savefig(plot_file)\n",
    "        plt.close()\n",
    "\n",
    "    print(f\"Completed {cfg['backbone']} ({'FT' if cfg['finetune'] else 'Frozen'}), artifacts in {model._last_save_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_58411/374231145.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(\"4Enc_1Dec-61epoch-0.103loss.pt\")\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '4Enc_1Dec-61epoch-0.103loss.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m4Enc_1Dec-61epoch-0.103loss.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/mbari-mae/venv/lib/python3.9/site-packages/torch/serialization.py:1319\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1317\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1319\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1321\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1323\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/Documents/mbari-mae/venv/lib/python3.9/site-packages/torch/serialization.py:659\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 659\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    661\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/Documents/mbari-mae/venv/lib/python3.9/site-packages/torch/serialization.py:640\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 640\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '4Enc_1Dec-61epoch-0.103loss.pt'"
     ]
    }
   ],
   "source": [
    "torch.load(\"4Enc_1Dec-61epoch-0.103loss.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
