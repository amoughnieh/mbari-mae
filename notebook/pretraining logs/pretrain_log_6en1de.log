[2025-07-02 21:02:40,203][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'tblog', 'wandb_project': None, 'azureml_logging': False, 'seed': 1337, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': 'C:\\Users\\Ali\\OneDrive - Georgia Institute of Technology\\25-5 Summer\\CS 7643 - Deep Learning\\_Project\\MAE-AST-Public\\mae_ast', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 6, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 1048576, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 5, 'validate_interval_updates': 10000, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1048576, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [8], 'lr': [0.0001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 25000, 'keep_interval_updates': 1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'mae_ast', 'ast_kernel_size_chan': 16, 'ast_kernel_size_time': 16, 'ast_kernel_stride_chan': 16, 'ast_kernel_stride_time': 16, 'encoder_layers': 6, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_norm_first': False, 'feature_grad_mult': 0.1, 'use_post_enc_proj': False, 'decoder_embed_dim': 768, 'decoder_layers': 1, 'decoder_layerdrop': 0.0, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'random_mask_prob': 0.75, 'mask_length': 10, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'checkpoint_activations': False, 'max_token_length': 48000, 'enc_sine_pos': True, 'enc_conv_pos': False, 'dec_sine_pos': True, 'dec_conv_pos': False}, 'task': {'_name': 'mae_ast_pretraining', 'data': 'D:\\MBARI 2KHz\\training\\input_dir', 'sample_rate': 2000, 'normalize': True, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 40000, 'min_sample_size': 5000, 'random_crop': True, 'pad_audio': False, 'feature_type': 'fbank', 'feature_rate': 100, 'feature_dim': 128, 'deltas': False, 'mask_spans': False, 'mask_type': random_mask}, 'criterion': {'_name': 'mae_ast', 'reconstruction_weight': 10.0, 'classification_weight': 1.0}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0001]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 4000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000.0, 'lr': [0.0001]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2025-07-02 21:02:40,214][mae_ast.tasks.mae_ast_pretraining][INFO] - current directory is D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model
[2025-07-02 21:02:40,214][mae_ast.tasks.mae_ast_pretraining][INFO] - MAEPretrainingTask Config {'_name': 'mae_ast_pretraining', 'data': 'D:\\MBARI 2KHz\\training\\input_dir', 'sample_rate': 2000, 'normalize': True, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 40000, 'min_sample_size': 5000, 'random_crop': True, 'pad_audio': False, 'feature_type': 'fbank', 'feature_rate': 100, 'feature_dim': 128, 'deltas': False, 'mask_spans': False, 'mask_type': random_mask}
[2025-07-02 21:02:40,214][mae_ast.models.mae_ast][INFO] - MAEModel Config: {'_name': 'mae_ast', 'ast_kernel_size_chan': 16, 'ast_kernel_size_time': 16, 'ast_kernel_stride_chan': 16, 'ast_kernel_stride_time': 16, 'encoder_layers': 6, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_norm_first': False, 'feature_grad_mult': 0.1, 'use_post_enc_proj': False, 'decoder_embed_dim': 768, 'decoder_layers': 1, 'decoder_layerdrop': 0.0, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'random_mask_prob': 0.75, 'mask_length': 10, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'checkpoint_activations': False, 'max_token_length': 48000, 'enc_sine_pos': True, 'enc_conv_pos': False, 'dec_sine_pos': True, 'dec_conv_pos': False}
[2025-07-02 21:02:40,774][fairseq_cli.train][INFO] - MAE_AST(
  (feature_extractor): Identity()
  (post_extract_proj): Linear(in_features=256, out_features=768, bias=True)
  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (batch_norm): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  (unfold): Unfold(kernel_size=(16, 16), dilation=1, padding=0, stride=(16, 16))
  (dropout_input): Dropout(p=0.1, inplace=False)
  (enc_sine_pos_embed): SinusoidalPositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (dec_sine_pos_embed): SinusoidalPositionalEncoding()
  (decoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (final_proj_reconstruction): Linear(in_features=768, out_features=256, bias=True)
  (final_proj_classification): Linear(in_features=768, out_features=256, bias=True)
)
[2025-07-02 21:02:40,774][fairseq_cli.train][INFO] - task: MAE_AST_Pretraining_Task
[2025-07-02 21:02:40,774][fairseq_cli.train][INFO] - model: MAE_AST
[2025-07-02 21:02:40,774][fairseq_cli.train][INFO] - criterion: MAE_AST_Criterion
[2025-07-02 21:02:40,774][fairseq_cli.train][INFO] - num. shared model params: 50,211,072 (num. trained: 50,211,072)
[2025-07-02 21:02:40,774][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2025-07-02 21:02:40,947][mae_ast.data.mae_ast_dataset][INFO] - max_keep=None, min_keep=5000, loaded 10374, skipped 0 short and 0 long, longest-loaded=20000, shortest-loaded=10000
[2025-07-02 21:02:40,957][mae_ast.data.mae_ast_dataset][INFO] - pad_audio=False, random_crop=True, normalize=True, max_sample_size=40000
[2025-07-02 21:02:41,173][fairseq.trainer][INFO] - detected shared parameter: batch_norm.weight <- batch_norm.bias
[2025-07-02 21:02:41,173][fairseq.utils][INFO] - ***********************CUDA enviroments for all 1 workers***********************
[2025-07-02 21:02:41,173][fairseq.utils][INFO] - rank   0: capabilities =  7.5  ; total memory = 6.000 GB ; name = NVIDIA GeForce RTX 2060                 
[2025-07-02 21:02:41,173][fairseq.utils][INFO] - ***********************CUDA enviroments for all 1 workers***********************
[2025-07-02 21:02:41,173][fairseq_cli.train][INFO] - training on 1 devices (GPUs/TPUs)
[2025-07-02 21:02:41,173][fairseq_cli.train][INFO] - max tokens per device = 1048576 and max sentences per device = None
[2025-07-02 21:02:41,173][fairseq.trainer][INFO] - Preparing to load checkpoint checkpoints\checkpoint_last.pt
[2025-07-02 21:02:41,173][fairseq.trainer][INFO] - No existing checkpoint found checkpoints\checkpoint_last.pt
[2025-07-02 21:02:41,173][fairseq.trainer][INFO] - loading train data for epoch 1
[2025-07-02 21:02:41,216][mae_ast.data.mae_ast_dataset][INFO] - max_keep=None, min_keep=5000, loaded 93367, skipped 0 short and 0 long, longest-loaded=20000, shortest-loaded=10000
[2025-07-02 21:02:41,216][mae_ast.data.mae_ast_dataset][INFO] - pad_audio=False, random_crop=True, normalize=True, max_sample_size=40000
[2025-07-02 21:02:43,063][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 21:02:43,063][fairseq.trainer][INFO] - begin training epoch 1
[2025-07-02 21:02:43,063][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 21:16:08,667][train_inner][INFO] - {"epoch": 1, "update": 0.826, "loss": "18.749", "nll_loss": "0.051", "loss_recon": "0.688", "loss_info_nce": "11.868", "ppl": "1.04", "wps": "757.7", "ups": "0.26", "wpb": "2965.8", "bsz": "386.3", "num_updates": "200", "lr": "5e-06", "gnorm": "14.263", "clip": "55", "loss_scale": "128", "train_wall": "783", "gb_free": "1.5", "wall": "801"}
[2025-07-02 21:18:50,342][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1 @ 242 updates
[2025-07-02 21:18:50,357][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint1.pt
[2025-07-02 21:18:52,468][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint1.pt
[2025-07-02 21:18:55,249][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint1.pt (epoch 1 @ 242 updates, score None) (writing took 4.895312200000035 seconds)
[2025-07-02 21:18:55,249][fairseq_cli.train][INFO] - end of epoch 1 (average epoch stats below)
[2025-07-02 21:18:55,306][train][INFO] - {"epoch": 1, "train_loss": "17.84", "train_nll_loss": "0.048", "train_loss_recon": "0.638", "train_loss_info_nce": "11.452", "train_ppl": "1.03", "train_wps": "748.5", "train_ups": "0.25", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "242", "train_lr": "6.05e-06", "train_gnorm": "12.467", "train_clip": "45.5", "train_loss_scale": "128", "train_train_wall": "944", "train_gb_free": "1.5", "train_wall": "974"}
[2025-07-02 21:18:55,534][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 21:18:55,534][fairseq.trainer][INFO] - begin training epoch 2
[2025-07-02 21:18:55,534][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 21:29:19,297][train_inner][INFO] - {"epoch": 2, "update": 1.653, "loss": "12.125", "nll_loss": "0.033", "loss_recon": "0.302", "loss_info_nce": "9.106", "ppl": "1.02", "wps": "747.2", "ups": "0.25", "wpb": "2953.7", "bsz": "386.4", "num_updates": "400", "lr": "1e-05", "gnorm": "2.853", "clip": "0", "loss_scale": "128", "train_wall": "782", "gb_free": "1.5", "wall": "1598"}
[2025-07-02 21:34:40,374][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 2 @ 484 updates
[2025-07-02 21:34:40,374][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint2.pt
[2025-07-02 21:34:43,883][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint2.pt
[2025-07-02 21:34:44,783][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint2.pt (epoch 2 @ 484 updates, score None) (writing took 4.400708200000054 seconds)
[2025-07-02 21:34:44,783][fairseq_cli.train][INFO] - end of epoch 2 (average epoch stats below)
[2025-07-02 21:34:44,809][train][INFO] - {"epoch": 2, "train_loss": "11.56", "train_nll_loss": "0.031", "train_loss_recon": "0.263", "train_loss_info_nce": "8.931", "train_ppl": "1.02", "train_wps": "753.6", "train_ups": "0.25", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "484", "train_lr": "1.21e-05", "train_gnorm": "2.64", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "941", "train_gb_free": "1.7", "train_wall": "1924"}
[2025-07-02 21:34:45,025][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 21:34:45,025][fairseq.trainer][INFO] - begin training epoch 3
[2025-07-02 21:34:45,025][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 21:43:01,273][train_inner][INFO] - {"epoch": 3, "update": 2.479, "loss": "11.053", "nll_loss": "0.03", "loss_recon": "0.232", "loss_info_nce": "8.736", "ppl": "1.02", "wps": "719.1", "ups": "0.24", "wpb": "2955.5", "bsz": "385.6", "num_updates": "600", "lr": "1.5e-05", "gnorm": "2.964", "clip": "0", "loss_scale": "128", "train_wall": "814", "gb_free": "1.5", "wall": "2420"}
[2025-07-02 21:54:56,331][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 3 @ 726 updates
[2025-07-02 21:54:56,331][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint3.pt
[2025-07-02 21:54:58,820][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint3.pt
[2025-07-02 21:54:59,693][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint3.pt (epoch 3 @ 726 updates, score None) (writing took 3.3531629999997676 seconds)
[2025-07-02 21:54:59,693][fairseq_cli.train][INFO] - end of epoch 3 (average epoch stats below)
[2025-07-02 21:54:59,693][train][INFO] - {"epoch": 3, "train_loss": "10.929", "train_nll_loss": "0.029", "train_loss_recon": "0.226", "train_loss_info_nce": "8.665", "train_ppl": "1.02", "train_wps": "589", "train_ups": "0.2", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "726", "train_lr": "1.815e-05", "train_gnorm": "3.654", "train_clip": "4.5", "train_loss_scale": "128", "train_train_wall": "1208", "train_gb_free": "1.5", "train_wall": "3139"}
[2025-07-02 21:54:59,919][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 21:54:59,919][fairseq.trainer][INFO] - begin training epoch 4
[2025-07-02 21:54:59,919][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 22:00:45,772][train_inner][INFO] - {"epoch": 4, "update": 3.306, "loss": "10.847", "nll_loss": "0.029", "loss_recon": "0.223", "loss_info_nce": "8.619", "ppl": "1.02", "wps": "555.5", "ups": "0.19", "wpb": "2956.5", "bsz": "385.1", "num_updates": "800", "lr": "2e-05", "gnorm": "3.836", "clip": "8", "loss_scale": "128", "train_wall": "1057", "gb_free": "1.5", "wall": "3485"}
[2025-07-02 22:13:32,337][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 4 @ 968 updates
[2025-07-02 22:13:32,337][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint4.pt
[2025-07-02 22:13:34,710][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint4.pt
[2025-07-02 22:13:35,534][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint4.pt (epoch 4 @ 968 updates, score None) (writing took 3.2111845999997968 seconds)
[2025-07-02 22:13:35,534][fairseq_cli.train][INFO] - end of epoch 4 (average epoch stats below)
[2025-07-02 22:13:35,549][train][INFO] - {"epoch": 4, "train_loss": "10.746", "train_nll_loss": "0.029", "train_loss_recon": "0.218", "train_loss_info_nce": "8.567", "train_ppl": "1.02", "train_wps": "641.3", "train_ups": "0.22", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "968", "train_lr": "2.42e-05", "train_gnorm": "4.38", "train_clip": "10.7", "train_loss_scale": "128", "train_train_wall": "1109", "train_gb_free": "1.5", "train_wall": "4254"}
[2025-07-02 22:13:35,757][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 22:13:35,757][fairseq.trainer][INFO] - begin training epoch 5
[2025-07-02 22:13:35,757][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 22:16:45,766][train_inner][INFO] - {"epoch": 5, "update": 4.132, "loss": "10.711", "nll_loss": "0.029", "loss_recon": "0.216", "loss_info_nce": "8.548", "ppl": "1.02", "wps": "614.8", "ups": "0.21", "wpb": "2950.9", "bsz": "386", "num_updates": "1000", "lr": "2.5e-05", "gnorm": "4.983", "clip": "13", "loss_scale": "128", "train_wall": "953", "gb_free": "1.5", "wall": "4445"}
[2025-07-02 22:32:39,394][train_inner][INFO] - {"epoch": 5, "update": 4.959, "loss": "10.604", "nll_loss": "0.029", "loss_recon": "0.209", "loss_info_nce": "8.513", "ppl": "1.02", "wps": "622.2", "ups": "0.21", "wpb": "2966.7", "bsz": "386.8", "num_updates": "1200", "lr": "3e-05", "gnorm": "9.292", "clip": "22", "loss_scale": "128", "train_wall": "952", "gb_free": "1.5", "wall": "5398"}
[2025-07-02 22:33:22,387][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-02 22:34:26,594][valid][INFO] - {"epoch": 5, "valid_loss": "10.339", "valid_nll_loss": "0.028", "valid_loss_recon": "0.194", "valid_loss_info_nce": "8.398", "valid_ppl": "1.02", "valid_wps": "1710.1", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "1210"}
[2025-07-02 22:34:26,594][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 5 @ 1210 updates
[2025-07-02 22:34:26,594][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint5.pt
[2025-07-02 22:34:28,938][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint5.pt
[2025-07-02 22:34:33,821][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint5.pt (epoch 5 @ 1210 updates, score 10.339) (writing took 7.2204979999996795 seconds)
[2025-07-02 22:34:33,821][fairseq_cli.train][INFO] - end of epoch 5 (average epoch stats below)
[2025-07-02 22:34:33,825][train][INFO] - {"epoch": 5, "train_loss": "10.607", "train_nll_loss": "0.029", "train_loss_recon": "0.209", "train_loss_info_nce": "8.512", "train_ppl": "1.02", "train_wps": "568.7", "train_ups": "0.19", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "1210", "train_lr": "3.025e-05", "train_gnorm": "8.497", "train_clip": "20.2", "train_loss_scale": "128", "train_train_wall": "1183", "train_gb_free": "1.5", "train_wall": "5513"}
[2025-07-02 22:34:35,005][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 22:34:35,021][fairseq.trainer][INFO] - begin training epoch 6
[2025-07-02 22:34:35,024][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 22:59:20,284][train_inner][INFO] - {"epoch": 6, "update": 5.785, "loss": "10.472", "nll_loss": "0.028", "loss_recon": "0.201", "loss_info_nce": "8.459", "ppl": "1.02", "wps": "369.2", "ups": "0.12", "wpb": "2955.5", "bsz": "385.4", "num_updates": "1400", "lr": "3.5e-05", "gnorm": "4.735", "clip": "6.5", "loss_scale": "128", "train_wall": "1522", "gb_free": "1.5", "wall": "6999"}
[2025-07-02 23:07:31,018][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 6 @ 1452 updates
[2025-07-02 23:07:31,018][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint6.pt
[2025-07-02 23:07:33,421][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint6.pt
[2025-07-02 23:07:35,639][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint6.pt (epoch 6 @ 1452 updates, score None) (writing took 4.6203230000000985 seconds)
[2025-07-02 23:07:35,641][fairseq_cli.train][INFO] - end of epoch 6 (average epoch stats below)
[2025-07-02 23:07:35,655][train][INFO] - {"epoch": 6, "train_loss": "10.459", "train_nll_loss": "0.028", "train_loss_recon": "0.2", "train_loss_info_nce": "8.455", "train_ppl": "1.02", "train_wps": "361.1", "train_ups": "0.12", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "1452", "train_lr": "3.63e-05", "train_gnorm": "5.655", "train_clip": "12", "train_loss_scale": "128", "train_train_wall": "1969", "train_gb_free": "1.5", "train_wall": "7494"}
[2025-07-02 23:07:36,010][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 23:07:36,026][fairseq.trainer][INFO] - begin training epoch 7
[2025-07-02 23:07:36,026][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-02 23:31:16,277][train_inner][INFO] - {"epoch": 7, "update": 6.612, "loss": "10.376", "nll_loss": "0.028", "loss_recon": "0.195", "loss_info_nce": "8.424", "ppl": "1.02", "wps": "308.6", "ups": "0.1", "wpb": "2956.5", "bsz": "385.5", "num_updates": "1600", "lr": "4e-05", "gnorm": "5.612", "clip": "12", "loss_scale": "128", "train_wall": "1904", "gb_free": "1.5", "wall": "8915"}
[2025-07-02 23:46:17,898][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 7 @ 1694 updates
[2025-07-02 23:46:17,898][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint7.pt
[2025-07-02 23:46:20,270][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint7.pt
[2025-07-02 23:46:21,904][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint7.pt (epoch 7 @ 1694 updates, score None) (writing took 4.004445399999895 seconds)
[2025-07-02 23:46:21,904][fairseq_cli.train][INFO] - end of epoch 7 (average epoch stats below)
[2025-07-02 23:46:21,915][train][INFO] - {"epoch": 7, "train_loss": "10.344", "train_nll_loss": "0.028", "train_loss_recon": "0.194", "train_loss_info_nce": "8.409", "train_ppl": "1.02", "train_wps": "307.6", "train_ups": "0.1", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "1694", "train_lr": "4.235e-05", "train_gnorm": "5.164", "train_clip": "9.9", "train_loss_scale": "128", "train_train_wall": "2315", "train_gb_free": "1.5", "train_wall": "9821"}
[2025-07-02 23:46:22,288][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-02 23:46:22,288][fairseq.trainer][INFO] - begin training epoch 8
[2025-07-02 23:46:22,288][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-03 00:03:26,089][train_inner][INFO] - {"epoch": 8, "update": 7.438, "loss": "10.3", "nll_loss": "0.028", "loss_recon": "0.191", "loss_info_nce": "8.395", "ppl": "1.02", "wps": "306.2", "ups": "0.1", "wpb": "2954.6", "bsz": "385.9", "num_updates": "1800", "lr": "4.5e-05", "gnorm": "7.369", "clip": "23.5", "loss_scale": "128", "train_wall": "1920", "gb_free": "1.5", "wall": "10845"}
[2025-07-03 00:24:47,591][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 8 @ 1936 updates
[2025-07-03 00:24:47,591][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint8.pt
[2025-07-03 00:24:50,045][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint8.pt
[2025-07-03 00:24:51,989][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint8.pt (epoch 8 @ 1936 updates, score None) (writing took 4.396089600000778 seconds)
[2025-07-03 00:24:51,989][fairseq_cli.train][INFO] - end of epoch 8 (average epoch stats below)
[2025-07-03 00:24:52,004][train][INFO] - {"epoch": 8, "train_loss": "10.232", "train_nll_loss": "0.028", "train_loss_recon": "0.186", "train_loss_info_nce": "8.367", "train_ppl": "1.02", "train_wps": "309.7", "train_ups": "0.1", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "1936", "train_lr": "4.84e-05", "train_gnorm": "7.428", "train_clip": "24.4", "train_loss_scale": "128", "train_train_wall": "2299", "train_gb_free": "1.5", "train_wall": "12131"}
[2025-07-03 00:24:52,360][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-03 00:24:52,376][fairseq.trainer][INFO] - begin training epoch 9
[2025-07-03 00:24:52,376][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-03 00:35:19,268][train_inner][INFO] - {"epoch": 9, "update": 8.264, "loss": "10.177", "nll_loss": "0.027", "loss_recon": "0.184", "loss_info_nce": "8.339", "ppl": "1.02", "wps": "308.7", "ups": "0.1", "wpb": "2952.8", "bsz": "386.2", "num_updates": "2000", "lr": "5e-05", "gnorm": "6.575", "clip": "20", "loss_scale": "128", "train_wall": "1903", "gb_free": "1.5", "wall": "12758"}
[2025-07-03 01:03:42,674][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 9 @ 2178 updates
[2025-07-03 01:03:42,674][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint9.pt
[2025-07-03 01:03:45,142][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint9.pt
[2025-07-03 01:03:46,720][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint9.pt (epoch 9 @ 2178 updates, score None) (writing took 4.047403500000655 seconds)
[2025-07-03 01:03:46,720][fairseq_cli.train][INFO] - end of epoch 9 (average epoch stats below)
[2025-07-03 01:03:46,736][train][INFO] - {"epoch": 9, "train_loss": "10.113", "train_nll_loss": "0.027", "train_loss_recon": "0.181", "train_loss_info_nce": "8.308", "train_ppl": "1.02", "train_wps": "306.5", "train_ups": "0.1", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "2178", "train_lr": "5.445e-05", "train_gnorm": "7.187", "train_clip": "26.4", "train_loss_scale": "256", "train_train_wall": "2323", "train_gb_free": "1.5", "train_wall": "14466"}
[2025-07-03 01:03:47,108][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-03 01:03:47,118][fairseq.trainer][INFO] - begin training epoch 10
[2025-07-03 01:03:47,118][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-03 01:07:28,558][train_inner][INFO] - {"epoch": 10, "update": 9.091, "loss": "10.098", "nll_loss": "0.027", "loss_recon": "0.18", "loss_info_nce": "8.301", "ppl": "1.02", "wps": "306.4", "ups": "0.1", "wpb": "2955.5", "bsz": "385.4", "num_updates": "2200", "lr": "5.5e-05", "gnorm": "7.442", "clip": "28.5", "loss_scale": "256", "train_wall": "1918", "gb_free": "1.5", "wall": "14687"}
[2025-07-03 01:39:36,185][train_inner][INFO] - {"epoch": 10, "update": 9.917, "loss": "10.037", "nll_loss": "0.027", "loss_recon": "0.177", "loss_info_nce": "8.267", "ppl": "1.02", "wps": "307.6", "ups": "0.1", "wpb": "2964.8", "bsz": "386.6", "num_updates": "2400", "lr": "6e-05", "gnorm": "7.387", "clip": "25", "loss_scale": "256", "train_wall": "1924", "gb_free": "1.5", "wall": "16615"}
[2025-07-03 01:42:39,089][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-03 01:43:40,276][valid][INFO] - {"epoch": 10, "valid_loss": "9.794", "valid_nll_loss": "0.027", "valid_loss_recon": "0.166", "valid_loss_info_nce": "8.138", "valid_ppl": "1.02", "valid_wps": "1345.9", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "2420", "valid_best_loss": "9.794"}
[2025-07-03 01:43:40,276][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 10 @ 2420 updates
[2025-07-03 01:43:40,276][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint10.pt
[2025-07-03 01:43:42,730][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint10.pt
[2025-07-03 01:43:46,357][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint10.pt (epoch 10 @ 2420 updates, score 9.794) (writing took 6.078762700002699 seconds)
[2025-07-03 01:43:46,357][fairseq_cli.train][INFO] - end of epoch 10 (average epoch stats below)
[2025-07-03 01:43:46,372][train][INFO] - {"epoch": 10, "train_loss": "10.038", "train_nll_loss": "0.027", "train_loss_recon": "0.177", "train_loss_info_nce": "8.266", "train_ppl": "1.02", "train_wps": "298.2", "train_ups": "0.1", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "2420", "train_lr": "6.05e-05", "train_gnorm": "7.193", "train_clip": "22.7", "train_loss_scale": "256", "train_train_wall": "2325", "train_gb_free": "1.6", "train_wall": "16865"}
[2025-07-03 01:43:46,772][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-03 01:43:46,772][fairseq.trainer][INFO] - begin training epoch 11
[2025-07-03 01:43:46,772][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-03 02:12:38,228][train_inner][INFO] - {"epoch": 11, "update": 10.744, "loss": "9.972", "nll_loss": "0.027", "loss_recon": "0.175", "loss_info_nce": "8.226", "ppl": "1.02", "wps": "298.4", "ups": "0.1", "wpb": "2957.4", "bsz": "384.8", "num_updates": "2600", "lr": "6.5e-05", "gnorm": "8.777", "clip": "30", "loss_scale": "256", "train_wall": "1908", "gb_free": "1.5", "wall": "18597"}
[2025-07-03 02:22:33,435][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 11 @ 2662 updates
[2025-07-03 02:22:33,435][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint11.pt
[2025-07-03 02:22:35,893][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint11.pt
[2025-07-03 02:22:37,522][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint11.pt (epoch 11 @ 2662 updates, score None) (writing took 4.0846892000008665 seconds)
[2025-07-03 02:22:37,522][fairseq_cli.train][INFO] - end of epoch 11 (average epoch stats below)
[2025-07-03 02:22:37,540][train][INFO] - {"epoch": 11, "train_loss": "9.948", "train_nll_loss": "0.027", "train_loss_recon": "0.174", "train_loss_info_nce": "8.212", "train_ppl": "1.02", "train_wps": "306.9", "train_ups": "0.1", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "2662", "train_lr": "6.655e-05", "train_gnorm": "8.884", "train_clip": "31", "train_loss_scale": "256", "train_train_wall": "2319", "train_gb_free": "1.5", "train_wall": "19196"}
[2025-07-03 02:22:37,923][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-03 02:22:37,923][fairseq.trainer][INFO] - begin training epoch 12
[2025-07-03 02:22:37,923][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-03 02:45:00,311][train_inner][INFO] - {"epoch": 12, "update": 11.57, "loss": "9.845", "nll_loss": "0.027", "loss_recon": "0.169", "loss_info_nce": "8.157", "ppl": "1.02", "wps": "304.4", "ups": "0.1", "wpb": "2955.5", "bsz": "386.1", "num_updates": "2800", "lr": "7e-05", "gnorm": "7.209", "clip": "19", "loss_scale": "256", "train_wall": "1931", "gb_free": "1.5", "wall": "20539"}
[2025-07-03 03:01:37,559][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 12 @ 2904 updates
[2025-07-03 03:01:37,559][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint12.pt
[2025-07-03 03:01:39,960][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint12.pt
[2025-07-03 03:01:42,027][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint12.pt (epoch 12 @ 2904 updates, score None) (writing took 4.469174699999712 seconds)
[2025-07-03 03:01:42,027][fairseq_cli.train][INFO] - end of epoch 12 (average epoch stats below)
[2025-07-03 03:01:42,060][train][INFO] - {"epoch": 12, "train_loss": "9.802", "train_nll_loss": "0.026", "train_loss_recon": "0.167", "train_loss_info_nce": "8.13", "train_ppl": "1.02", "train_wps": "305.2", "train_ups": "0.1", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "2904", "train_lr": "7.26e-05", "train_gnorm": "8.603", "train_clip": "32.2", "train_loss_scale": "256", "train_train_wall": "2332", "train_gb_free": "1.5", "train_wall": "21541"}
[2025-07-03 03:01:42,429][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-03 03:01:42,429][fairseq.trainer][INFO] - begin training epoch 13
[2025-07-03 03:01:42,429][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-03 03:17:14,062][train_inner][INFO] - {"epoch": 13, "update": 12.397, "loss": "9.705", "nll_loss": "0.026", "loss_recon": "0.163", "loss_info_nce": "8.071", "ppl": "1.02", "wps": "305.3", "ups": "0.1", "wpb": "2951.8", "bsz": "386.2", "num_updates": "3000", "lr": "7.5e-05", "gnorm": "8.441", "clip": "35", "loss_scale": "256", "train_wall": "1922", "gb_free": "1.5", "wall": "22473"}
[2025-07-03 03:40:26,592][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 13 @ 3146 updates
[2025-07-03 03:40:26,592][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint13.pt
[2025-07-03 03:40:29,038][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint13.pt
[2025-07-03 03:40:30,721][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint13.pt (epoch 13 @ 3146 updates, score None) (writing took 4.131752199999028 seconds)
[2025-07-03 03:40:30,721][fairseq_cli.train][INFO] - end of epoch 13 (average epoch stats below)
[2025-07-03 03:40:30,738][train][INFO] - {"epoch": 13, "train_loss": "9.569", "train_nll_loss": "0.026", "train_loss_recon": "0.158", "train_loss_info_nce": "7.986", "train_ppl": "1.02", "train_wps": "307.3", "train_ups": "0.1", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "3146", "train_lr": "7.865e-05", "train_gnorm": "6.248", "train_clip": "16.1", "train_loss_scale": "256", "train_train_wall": "2318", "train_gb_free": "1.5", "train_wall": "23870"}
[2025-07-03 03:40:31,107][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-03 03:40:31,122][fairseq.trainer][INFO] - begin training epoch 14
[2025-07-03 03:40:31,122][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-03 03:49:21,535][train_inner][INFO] - {"epoch": 14, "update": 13.223, "loss": "9.498", "nll_loss": "0.026", "loss_recon": "0.156", "loss_info_nce": "7.94", "ppl": "1.02", "wps": "307.1", "ups": "0.1", "wpb": "2959.3", "bsz": "384.5", "num_updates": "3200", "lr": "8e-05", "gnorm": "6.1", "clip": "13.5", "loss_scale": "256", "train_wall": "1918", "gb_free": "1.5", "wall": "24400"}
[2025-07-03 04:19:31,286][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 14 @ 3388 updates
[2025-07-03 04:19:31,286][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint14.pt
[2025-07-03 04:19:33,682][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint14.pt
[2025-07-03 04:19:35,748][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint14.pt (epoch 14 @ 3388 updates, score None) (writing took 4.458585000000312 seconds)
[2025-07-03 04:19:35,748][fairseq_cli.train][INFO] - end of epoch 14 (average epoch stats below)
[2025-07-03 04:19:35,773][train][INFO] - {"epoch": 14, "train_loss": "9.368", "train_nll_loss": "0.025", "train_loss_recon": "0.151", "train_loss_info_nce": "7.856", "train_ppl": "1.02", "train_wps": "305.1", "train_ups": "0.1", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "3388", "train_lr": "8.47e-05", "train_gnorm": "5.465", "train_clip": "9.9", "train_loss_scale": "256", "train_train_wall": "2334", "train_gb_free": "1.5", "train_wall": "26215"}
[2025-07-03 04:19:36,146][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-03 04:19:36,156][fairseq.trainer][INFO] - begin training epoch 15
[2025-07-03 04:19:36,156][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-03 04:21:36,970][train_inner][INFO] - {"epoch": 15, "update": 14.05, "loss": "9.347", "nll_loss": "0.025", "loss_recon": "0.151", "loss_info_nce": "7.84", "ppl": "1.02", "wps": "305.1", "ups": "0.1", "wpb": "2952.8", "bsz": "386.2", "num_updates": "3400", "lr": "8.5e-05", "gnorm": "5.473", "clip": "10.5", "loss_scale": "256", "train_wall": "1924", "gb_free": "1.5", "wall": "26336"}
[2025-07-03 04:54:04,768][train_inner][INFO] - {"epoch": 15, "update": 14.876, "loss": "9.201", "nll_loss": "0.025", "loss_recon": "0.146", "loss_info_nce": "7.736", "ppl": "1.02", "wps": "304.2", "ups": "0.1", "wpb": "2963", "bsz": "387.1", "num_updates": "3600", "lr": "9e-05", "gnorm": "5.971", "clip": "14", "loss_scale": "256", "train_wall": "1944", "gb_free": "1.5", "wall": "28284"}
[2025-07-03 04:58:42,054][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-03 04:59:43,194][valid][INFO] - {"epoch": 15, "valid_loss": "8.741", "valid_nll_loss": "0.024", "valid_loss_recon": "0.131", "valid_loss_info_nce": "7.435", "valid_ppl": "1.02", "valid_wps": "1344.6", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "3630", "valid_best_loss": "8.741"}
[2025-07-03 04:59:43,194][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 15 @ 3630 updates
[2025-07-03 04:59:43,194][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint15.pt
[2025-07-03 04:59:45,573][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint15.pt
[2025-07-03 04:59:50,246][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint15.pt (epoch 15 @ 3630 updates, score 8.741) (writing took 7.05424370000037 seconds)
[2025-07-03 04:59:50,246][fairseq_cli.train][INFO] - end of epoch 15 (average epoch stats below)
[2025-07-03 04:59:50,303][train][INFO] - {"epoch": 15, "train_loss": "9.196", "train_nll_loss": "0.025", "train_loss_recon": "0.146", "train_loss_info_nce": "7.732", "train_ppl": "1.02", "train_wps": "296.4", "train_ups": "0.1", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "3630", "train_lr": "9.075e-05", "train_gnorm": "5.837", "train_clip": "12.4", "train_loss_scale": "256", "train_train_wall": "2338", "train_gb_free": "1.5", "train_wall": "28629"}
[2025-07-03 04:59:50,684][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-03 04:59:50,684][fairseq.trainer][INFO] - begin training epoch 16
[2025-07-03 04:59:50,684][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-03 05:25:59,726][train_inner][INFO] - {"epoch": 16, "update": 15.702, "loss": "9.063", "nll_loss": "0.024", "loss_recon": "0.143", "loss_info_nce": "7.632", "ppl": "1.02", "wps": "309", "ups": "0.1", "wpb": "2958.3", "bsz": "385", "num_updates": "3800", "lr": "9.5e-05", "gnorm": "5.749", "clip": "8.5", "loss_scale": "256", "train_wall": "1837", "gb_free": "1.5", "wall": "30199"}
[2025-07-03 05:36:26,552][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 16 @ 3872 updates
[2025-07-03 05:36:26,554][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint16.pt
[2025-07-03 05:36:28,985][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint16.pt
[2025-07-03 05:36:30,320][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint16.pt (epoch 16 @ 3872 updates, score None) (writing took 3.7765857000013057 seconds)
[2025-07-03 05:36:30,320][fairseq_cli.train][INFO] - end of epoch 16 (average epoch stats below)
[2025-07-03 05:36:30,337][train][INFO] - {"epoch": 16, "train_loss": "9.03", "train_nll_loss": "0.024", "train_loss_recon": "0.142", "train_loss_info_nce": "7.605", "train_ppl": "1.02", "train_wps": "325.2", "train_ups": "0.11", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "3872", "train_lr": "9.68e-05", "train_gnorm": "5.574", "train_clip": "9.5", "train_loss_scale": "256", "train_train_wall": "2185", "train_gb_free": "1.5", "train_wall": "30829"}
[2025-07-03 05:36:30,720][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-03 05:36:30,720][fairseq.trainer][INFO] - begin training epoch 17
[2025-07-03 05:36:30,720][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-03 05:55:24,176][train_inner][INFO] - {"epoch": 17, "update": 16.529, "loss": "8.943", "nll_loss": "0.024", "loss_recon": "0.141", "loss_info_nce": "7.536", "ppl": "1.02", "wps": "334.6", "ups": "0.11", "wpb": "2951.8", "bsz": "386.3", "num_updates": "4000", "lr": "0.0001", "gnorm": "5.43", "clip": "9", "loss_scale": "256", "train_wall": "1751", "gb_free": "1.5", "wall": "31963"}
[2025-07-03 06:11:38,559][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 17 @ 4114 updates
[2025-07-03 06:11:38,559][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint17.pt
[2025-07-03 06:11:40,962][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint17.pt
[2025-07-03 06:11:42,630][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint17.pt (epoch 17 @ 4114 updates, score None) (writing took 4.072537400003057 seconds)
[2025-07-03 06:11:42,630][fairseq_cli.train][INFO] - end of epoch 17 (average epoch stats below)
[2025-07-03 06:11:42,646][train][INFO] - {"epoch": 17, "train_loss": "8.891", "train_nll_loss": "0.024", "train_loss_recon": "0.14", "train_loss_info_nce": "7.496", "train_ppl": "1.02", "train_wps": "338.8", "train_ups": "0.11", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "4114", "train_lr": "9.97522e-05", "train_gnorm": "5.093", "train_clip": "5.8", "train_loss_scale": "512", "train_train_wall": "2098", "train_gb_free": "1.5", "train_wall": "32941"}
[2025-07-03 06:11:43,036][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-03 06:11:43,036][fairseq.trainer][INFO] - begin training epoch 18
[2025-07-03 06:11:43,036][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-03 06:24:27,067][train_inner][INFO] - {"epoch": 18, "update": 17.355, "loss": "8.829", "nll_loss": "0.024", "loss_recon": "0.139", "loss_info_nce": "7.443", "ppl": "1.02", "wps": "339.2", "ups": "0.11", "wpb": "2955.5", "bsz": "385.4", "num_updates": "4200", "lr": "9.95652e-05", "gnorm": "4.564", "clip": "2.5", "loss_scale": "512", "train_wall": "1732", "gb_free": "1.5", "wall": "33706"}
[2025-07-03 06:46:57,976][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 18 @ 4356 updates
[2025-07-03 06:46:57,976][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint18.pt
[2025-07-03 06:47:00,435][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint18.pt
[2025-07-03 06:47:02,076][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint18.pt (epoch 18 @ 4356 updates, score None) (writing took 4.096269100002246 seconds)
[2025-07-03 06:47:02,078][fairseq_cli.train][INFO] - end of epoch 18 (average epoch stats below)
[2025-07-03 06:47:02,150][train][INFO] - {"epoch": 18, "train_loss": "8.74", "train_nll_loss": "0.024", "train_loss_recon": "0.138", "train_loss_info_nce": "7.36", "train_ppl": "1.02", "train_wps": "337.6", "train_ups": "0.11", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "4356", "train_lr": "9.92261e-05", "train_gnorm": "5.029", "train_clip": "7", "train_loss_scale": "512", "train_train_wall": "2108", "train_gb_free": "1.5", "train_wall": "35061"}
[2025-07-03 06:47:02,535][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-03 06:47:02,535][fairseq.trainer][INFO] - begin training epoch 19
[2025-07-03 06:47:02,535][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-03 06:53:35,484][train_inner][INFO] - {"epoch": 19, "update": 18.182, "loss": "8.693", "nll_loss": "0.023", "loss_recon": "0.138", "loss_info_nce": "7.315", "ppl": "1.02", "wps": "338.1", "ups": "0.11", "wpb": "2955.5", "bsz": "385.9", "num_updates": "4400", "lr": "9.91304e-05", "gnorm": "5.621", "clip": "13", "loss_scale": "512", "train_wall": "1738", "gb_free": "1.5", "wall": "35454"}
[2025-07-03 07:22:20,686][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 19 @ 4598 updates
[2025-07-03 07:22:20,686][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint19.pt
[2025-07-03 07:22:23,125][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint19.pt
[2025-07-03 07:22:24,759][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint19.pt (epoch 19 @ 4598 updates, score None) (writing took 4.061861300004239 seconds)
[2025-07-03 07:22:24,759][fairseq_cli.train][INFO] - end of epoch 19 (average epoch stats below)
[2025-07-03 07:22:24,769][train][INFO] - {"epoch": 19, "train_loss": "8.547", "train_nll_loss": "0.023", "train_loss_recon": "0.137", "train_loss_info_nce": "7.18", "train_ppl": "1.02", "train_wps": "337.1", "train_ups": "0.11", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "4598", "train_lr": "9.87e-05", "train_gnorm": "4.782", "train_clip": "5.8", "train_loss_scale": "512", "train_train_wall": "2111", "train_gb_free": "1.5", "train_wall": "37184"}
[2025-07-03 07:22:25,504][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-03 07:22:25,504][fairseq.trainer][INFO] - begin training epoch 20
[2025-07-03 07:22:25,504][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-03 07:22:46,049][train_inner][INFO] - {"epoch": 20, "update": 19.008, "loss": "8.526", "nll_loss": "0.023", "loss_recon": "0.136", "loss_info_nce": "7.161", "ppl": "1.02", "wps": "337.8", "ups": "0.11", "wpb": "2956.5", "bsz": "385.1", "num_updates": "4600", "lr": "9.86957e-05", "gnorm": "4.309", "clip": "2", "loss_scale": "512", "train_wall": "1739", "gb_free": "1.5", "wall": "37205"}
[2025-07-03 07:52:11,388][train_inner][INFO] - {"epoch": 20, "update": 19.835, "loss": "8.287", "nll_loss": "0.022", "loss_recon": "0.134", "loss_info_nce": "6.944", "ppl": "1.02", "wps": "335.7", "ups": "0.11", "wpb": "2963", "bsz": "387.1", "num_updates": "4800", "lr": "9.82609e-05", "gnorm": "4.404", "clip": "6.5", "loss_scale": "512", "train_wall": "1761", "gb_free": "1.5", "wall": "38970"}
[2025-07-03 07:57:52,493][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-03 07:58:53,406][valid][INFO] - {"epoch": 20, "valid_loss": "7.534", "valid_nll_loss": "0.02", "valid_loss_recon": "0.122", "valid_loss_info_nce": "6.316", "valid_ppl": "1.01", "valid_wps": "1345.8", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "4840", "valid_best_loss": "7.534"}
[2025-07-03 07:58:53,406][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 20 @ 4840 updates
[2025-07-03 07:58:53,406][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint20.pt
[2025-07-03 07:58:55,869][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint20.pt
[2025-07-03 07:58:59,541][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint20.pt (epoch 20 @ 4840 updates, score 7.534) (writing took 6.142231599995284 seconds)
[2025-07-03 07:58:59,541][fairseq_cli.train][INFO] - end of epoch 20 (average epoch stats below)
[2025-07-03 07:58:59,583][train][INFO] - {"epoch": 20, "train_loss": "8.263", "train_nll_loss": "0.022", "train_loss_recon": "0.134", "train_loss_info_nce": "6.922", "train_ppl": "1.02", "train_wps": "326", "train_ups": "0.11", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "4840", "train_lr": "9.81739e-05", "train_gnorm": "4.285", "train_clip": "5.4", "train_loss_scale": "512", "train_train_wall": "2119", "train_gb_free": "1.6", "train_wall": "39378"}
[2025-07-03 07:58:59,983][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-03 07:58:59,983][fairseq.trainer][INFO] - begin training epoch 21
[2025-07-03 07:58:59,983][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-03 08:30:54,694][train_inner][INFO] - {"epoch": 21, "update": 20.661, "loss": "8.036", "nll_loss": "0.022", "loss_recon": "0.132", "loss_info_nce": "6.719", "ppl": "1.02", "wps": "254.6", "ups": "0.09", "wpb": "2957.4", "bsz": "385.1", "num_updates": "5000", "lr": "9.78261e-05", "gnorm": "4.623", "clip": "6.5", "loss_scale": "512", "train_wall": "2250", "gb_free": "1.5", "wall": "41294"}
[2025-07-03 08:47:07,495][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 21 @ 5082 updates
[2025-07-03 08:47:07,495][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint21.pt
[2025-07-03 08:47:09,791][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint21.pt
[2025-07-03 08:47:10,689][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint21.pt (epoch 21 @ 5082 updates, score None) (writing took 3.1923034000064945 seconds)
[2025-07-03 08:47:10,689][fairseq_cli.train][INFO] - end of epoch 21 (average epoch stats below)
[2025-07-03 08:47:10,700][train][INFO] - {"epoch": 21, "train_loss": "7.953", "train_nll_loss": "0.021", "train_loss_recon": "0.131", "train_loss_info_nce": "6.644", "train_ppl": "1.01", "train_wps": "247.5", "train_ups": "0.08", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "5082", "train_lr": "9.76478e-05", "train_gnorm": "4.533", "train_clip": "5.4", "train_loss_scale": "512", "train_train_wall": "2881", "train_gb_free": "1.5", "train_wall": "42270"}
[2025-07-03 08:47:10,983][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-03 08:47:10,988][fairseq.trainer][INFO] - begin training epoch 22
[2025-07-03 08:47:10,988][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-03 20:39:10,530][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'tblog', 'wandb_project': None, 'azureml_logging': False, 'seed': 1337, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 6, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 1048576, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 5, 'validate_interval_updates': 10000, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1048576, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [8], 'lr': [0.0001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 25000, 'keep_interval_updates': 1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'mae_ast', 'dropout_input': 0.1, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'feature_grad_mult': 0.1, 'encoder_layerdrop': 0.05, 'decoder_layerdrop': 0.0, 'encoder_layers': 6, 'decoder_layers': 1, 'random_mask_prob': 0.75, 'enc_conv_pos': False, 'enc_sine_pos': True, 'dec_conv_pos': False, 'dec_sine_pos': True, 'ast_kernel_size_chan': 16, 'ast_kernel_size_time': 16, 'ast_kernel_stride_chan': 16, 'ast_kernel_stride_time': 16}, 'task': {'_name': 'mae_ast_pretraining', 'data': 'D:\\MBARI 2KHz\\training\\input_dir', 'feature_type': 'fbank', 'mask_type': 'random_mask', 'sample_rate': 2000, 'max_sample_size': 40000, 'min_sample_size': 5000, 'feature_rate': 100, 'feature_dim': 128, 'pad_audio': False, 'random_crop': True, 'normalize': True, 'deltas': False}, 'criterion': {'_name': 'mae_ast', 'classification_weight': 1, 'reconstruction_weight': 10}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0001]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 4000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000.0, 'lr': [0.0001]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2025-07-03 20:40:36,589][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'tblog', 'wandb_project': None, 'azureml_logging': False, 'seed': 1337, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': 'C:\\Users\\Ali\\OneDrive - Georgia Institute of Technology\\25-5 Summer\\CS 7643 - Deep Learning\\_Project\\MAE-AST-Public\\mae_ast', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 6, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 1048576, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 5, 'validate_interval_updates': 10000, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1048576, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [8], 'lr': [0.0001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 25000, 'keep_interval_updates': 1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'mae_ast', 'ast_kernel_size_chan': 16, 'ast_kernel_size_time': 16, 'ast_kernel_stride_chan': 16, 'ast_kernel_stride_time': 16, 'encoder_layers': 6, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_norm_first': False, 'feature_grad_mult': 0.1, 'use_post_enc_proj': False, 'decoder_embed_dim': 768, 'decoder_layers': 1, 'decoder_layerdrop': 0.0, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'random_mask_prob': 0.75, 'mask_length': 10, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'checkpoint_activations': False, 'max_token_length': 48000, 'enc_sine_pos': True, 'enc_conv_pos': False, 'dec_sine_pos': True, 'dec_conv_pos': False}, 'task': {'_name': 'mae_ast_pretraining', 'data': 'D:\\MBARI 2KHz\\training\\input_dir', 'sample_rate': 2000, 'normalize': True, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 40000, 'min_sample_size': 5000, 'random_crop': True, 'pad_audio': False, 'feature_type': 'fbank', 'feature_rate': 100, 'feature_dim': 128, 'deltas': False, 'mask_spans': False, 'mask_type': random_mask}, 'criterion': {'_name': 'mae_ast', 'reconstruction_weight': 10.0, 'classification_weight': 1.0}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0001]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 4000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000.0, 'lr': [0.0001]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2025-07-03 20:40:36,605][mae_ast.tasks.mae_ast_pretraining][INFO] - current directory is D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model
[2025-07-03 20:40:36,605][mae_ast.tasks.mae_ast_pretraining][INFO] - MAEPretrainingTask Config {'_name': 'mae_ast_pretraining', 'data': 'D:\\MBARI 2KHz\\training\\input_dir', 'sample_rate': 2000, 'normalize': True, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 40000, 'min_sample_size': 5000, 'random_crop': True, 'pad_audio': False, 'feature_type': 'fbank', 'feature_rate': 100, 'feature_dim': 128, 'deltas': False, 'mask_spans': False, 'mask_type': random_mask}
[2025-07-03 20:40:36,608][mae_ast.models.mae_ast][INFO] - MAEModel Config: {'_name': 'mae_ast', 'ast_kernel_size_chan': 16, 'ast_kernel_size_time': 16, 'ast_kernel_stride_chan': 16, 'ast_kernel_stride_time': 16, 'encoder_layers': 6, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_norm_first': False, 'feature_grad_mult': 0.1, 'use_post_enc_proj': False, 'decoder_embed_dim': 768, 'decoder_layers': 1, 'decoder_layerdrop': 0.0, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'random_mask_prob': 0.75, 'mask_length': 10, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'checkpoint_activations': False, 'max_token_length': 48000, 'enc_sine_pos': True, 'enc_conv_pos': False, 'dec_sine_pos': True, 'dec_conv_pos': False}
[2025-07-03 20:40:37,194][fairseq_cli.train][INFO] - MAE_AST(
  (feature_extractor): Identity()
  (post_extract_proj): Linear(in_features=256, out_features=768, bias=True)
  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (batch_norm): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  (unfold): Unfold(kernel_size=(16, 16), dilation=1, padding=0, stride=(16, 16))
  (dropout_input): Dropout(p=0.1, inplace=False)
  (enc_sine_pos_embed): SinusoidalPositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (dec_sine_pos_embed): SinusoidalPositionalEncoding()
  (decoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (final_proj_reconstruction): Linear(in_features=768, out_features=256, bias=True)
  (final_proj_classification): Linear(in_features=768, out_features=256, bias=True)
)
[2025-07-03 20:40:37,205][fairseq_cli.train][INFO] - task: MAE_AST_Pretraining_Task
[2025-07-03 20:40:37,205][fairseq_cli.train][INFO] - model: MAE_AST
[2025-07-03 20:40:37,205][fairseq_cli.train][INFO] - criterion: MAE_AST_Criterion
[2025-07-03 20:40:37,205][fairseq_cli.train][INFO] - num. shared model params: 50,211,072 (num. trained: 50,211,072)
[2025-07-03 20:40:37,205][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2025-07-03 20:40:37,376][mae_ast.data.mae_ast_dataset][INFO] - max_keep=None, min_keep=5000, loaded 10374, skipped 0 short and 0 long, longest-loaded=20000, shortest-loaded=10000
[2025-07-03 20:40:37,376][mae_ast.data.mae_ast_dataset][INFO] - pad_audio=False, random_crop=True, normalize=True, max_sample_size=40000
[2025-07-03 20:40:37,640][fairseq.trainer][INFO] - detected shared parameter: batch_norm.weight <- batch_norm.bias
[2025-07-03 20:40:37,640][fairseq.utils][INFO] - ***********************CUDA enviroments for all 1 workers***********************
[2025-07-03 20:40:37,640][fairseq.utils][INFO] - rank   0: capabilities =  7.5  ; total memory = 6.000 GB ; name = NVIDIA GeForce RTX 2060                 
[2025-07-03 20:40:37,640][fairseq.utils][INFO] - ***********************CUDA enviroments for all 1 workers***********************
[2025-07-03 20:40:37,640][fairseq_cli.train][INFO] - training on 1 devices (GPUs/TPUs)
[2025-07-03 20:40:37,640][fairseq_cli.train][INFO] - max tokens per device = 1048576 and max sentences per device = None
[2025-07-03 20:40:37,640][fairseq.trainer][INFO] - Preparing to load checkpoint checkpoints\checkpoint_last.pt
[2025-07-03 20:40:41,081][fairseq.trainer][INFO] - Loaded checkpoint checkpoints\checkpoint_last.pt (epoch 22 @ 5082 updates)
[2025-07-03 20:40:41,122][fairseq.trainer][INFO] - loading train data for epoch 22
[2025-07-03 20:40:41,179][mae_ast.data.mae_ast_dataset][INFO] - max_keep=None, min_keep=5000, loaded 93367, skipped 0 short and 0 long, longest-loaded=20000, shortest-loaded=10000
[2025-07-03 20:40:41,179][mae_ast.data.mae_ast_dataset][INFO] - pad_audio=False, random_crop=True, normalize=True, max_sample_size=40000
[2025-07-03 20:40:41,773][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-03 20:40:41,773][fairseq.trainer][INFO] - begin training epoch 22
[2025-07-03 20:40:41,773][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-03 20:47:09,251][train_inner][INFO] - {"epoch": 22, "update": 21.488, "loss": "7.728", "nll_loss": "0.021", "loss_recon": "0.128", "loss_info_nce": "6.444", "ppl": "1.01", "wps": "955.8", "ups": "0.32", "wpb": "2960.2", "bsz": "388.2", "num_updates": "5200", "lr": "9.73913e-05", "gnorm": "4.015", "clip": "1.7", "loss_scale": "512", "train_wall": "365", "gb_free": "1.5", "wall": "385"}
[2025-07-03 20:53:46,730][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 22 @ 5324 updates
[2025-07-03 20:53:46,731][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint22.pt
[2025-07-03 20:53:49,056][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint22.pt
[2025-07-03 20:53:50,655][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint22.pt (epoch 22 @ 5324 updates, score None) (writing took 3.932673099999988 seconds)
[2025-07-03 20:53:50,655][fairseq_cli.train][INFO] - end of epoch 22 (average epoch stats below)
[2025-07-03 20:53:50,712][train][INFO] - {"epoch": 22, "train_loss": "7.687", "train_nll_loss": "0.021", "train_loss_recon": "0.128", "train_loss_info_nce": "6.404", "train_ppl": "1.01", "train_wps": "925.4", "train_ups": "0.31", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "5324", "train_lr": "9.71217e-05", "train_gnorm": "3.963", "train_clip": "1.2", "train_loss_scale": "512", "train_train_wall": "758", "train_gb_free": "1.5", "train_wall": "793"}
[2025-07-03 20:53:51,036][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-03 20:53:51,042][fairseq.trainer][INFO] - begin training epoch 23
[2025-07-03 20:53:51,043][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-03 20:57:57,568][train_inner][INFO] - {"epoch": 23, "update": 22.314, "loss": "7.618", "nll_loss": "0.021", "loss_recon": "0.127", "loss_info_nce": "6.344", "ppl": "1.01", "wps": "912.6", "ups": "0.31", "wpb": "2958.3", "bsz": "384.1", "num_updates": "5400", "lr": "9.69565e-05", "gnorm": "3.981", "clip": "0.5", "loss_scale": "512", "train_wall": "636", "gb_free": "1.5", "wall": "1040"}
[2025-07-03 21:06:53,507][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 23 @ 5566 updates
[2025-07-03 21:06:53,507][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint23.pt
[2025-07-03 21:06:55,830][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint23.pt
[2025-07-03 21:06:57,264][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint23.pt (epoch 23 @ 5566 updates, score None) (writing took 3.7447323999999753 seconds)
[2025-07-03 21:06:57,264][fairseq_cli.train][INFO] - end of epoch 23 (average epoch stats below)
[2025-07-03 21:06:57,273][train][INFO] - {"epoch": 23, "train_loss": "7.524", "train_nll_loss": "0.02", "train_loss_recon": "0.126", "train_loss_info_nce": "6.266", "train_ppl": "1.01", "train_wps": "909.7", "train_ups": "0.31", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "5566", "train_lr": "9.65957e-05", "train_gnorm": "3.676", "train_clip": "0", "train_loss_scale": "512", "train_train_wall": "776", "train_gb_free": "1.7", "train_wall": "1580"}
[2025-07-03 21:06:57,574][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-03 21:06:57,574][fairseq.trainer][INFO] - begin training epoch 24
[2025-07-03 21:06:57,574][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-03 21:08:47,788][train_inner][INFO] - {"epoch": 24, "update": 23.14, "loss": "7.49", "nll_loss": "0.02", "loss_recon": "0.126", "loss_info_nce": "6.234", "ppl": "1.01", "wps": "908.2", "ups": "0.31", "wpb": "2952.8", "bsz": "387", "num_updates": "5600", "lr": "9.65217e-05", "gnorm": "3.769", "clip": "0", "loss_scale": "512", "train_wall": "640", "gb_free": "1.5", "wall": "1690"}
[2025-07-03 21:18:12,361][train_inner][INFO] - {"epoch": 24, "update": 23.967, "loss": "7.344", "nll_loss": "0.02", "loss_recon": "0.124", "loss_info_nce": "6.104", "ppl": "1.01", "wps": "1050.6", "ups": "0.35", "wpb": "2965.8", "bsz": "386.3", "num_updates": "5800", "lr": "9.6087e-05", "gnorm": "3.471", "clip": "0", "loss_scale": "512", "train_wall": "563", "gb_free": "1.5", "wall": "2255"}
[2025-07-03 21:18:33,802][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 24 @ 5808 updates
[2025-07-03 21:18:33,812][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint24.pt
[2025-07-03 21:18:36,186][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint24.pt
[2025-07-03 21:18:37,757][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint24.pt (epoch 24 @ 5808 updates, score None) (writing took 3.947276999999758 seconds)
[2025-07-03 21:18:37,757][fairseq_cli.train][INFO] - end of epoch 24 (average epoch stats below)
[2025-07-03 21:18:37,772][train][INFO] - {"epoch": 24, "train_loss": "7.354", "train_nll_loss": "0.02", "train_loss_recon": "0.124", "train_loss_info_nce": "6.112", "train_ppl": "1.01", "train_wps": "1021.5", "train_ups": "0.35", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "5808", "train_lr": "9.60696e-05", "train_gnorm": "3.683", "train_clip": "0", "train_loss_scale": "512", "train_train_wall": "691", "train_gb_free": "1.5", "train_wall": "2280"}
[2025-07-03 21:18:38,083][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-03 21:18:38,083][fairseq.trainer][INFO] - begin training epoch 25
[2025-07-03 21:18:38,083][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-03 21:28:28,234][train_inner][INFO] - {"epoch": 25, "update": 24.793, "loss": "7.19", "nll_loss": "0.019", "loss_recon": "0.123", "loss_info_nce": "5.965", "ppl": "1.01", "wps": "959.8", "ups": "0.32", "wpb": "2955.5", "bsz": "385.4", "num_updates": "6000", "lr": "9.56522e-05", "gnorm": "3.52", "clip": "1.5", "loss_scale": "512", "train_wall": "605", "gb_free": "1.5", "wall": "2871"}
[2025-07-03 21:30:55,860][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-03 21:31:42,060][valid][INFO] - {"epoch": 25, "valid_loss": "6.367", "valid_nll_loss": "0.017", "valid_loss_recon": "0.113", "valid_loss_info_nce": "5.239", "valid_ppl": "1.01", "valid_wps": "2726.8", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "6050", "valid_best_loss": "6.367"}
[2025-07-03 21:31:42,060][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 25 @ 6050 updates
[2025-07-03 21:31:42,060][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint25.pt
[2025-07-03 21:31:44,235][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint25.pt
[2025-07-03 21:31:45,822][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint25.pt (epoch 25 @ 6050 updates, score 6.367) (writing took 3.757842499999697 seconds)
[2025-07-03 21:31:45,822][fairseq_cli.train][INFO] - end of epoch 25 (average epoch stats below)
[2025-07-03 21:31:45,853][train][INFO] - {"epoch": 25, "train_loss": "7.172", "train_nll_loss": "0.019", "train_loss_recon": "0.122", "train_loss_info_nce": "5.95", "train_ppl": "1.01", "train_wps": "908", "train_ups": "0.31", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "6050", "train_lr": "9.55435e-05", "train_gnorm": "3.508", "train_clip": "1.2", "train_loss_scale": "512", "train_train_wall": "731", "train_gb_free": "1.5", "train_wall": "3068"}
[2025-07-03 21:31:46,038][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-03 21:31:46,038][fairseq.trainer][INFO] - begin training epoch 26
[2025-07-03 21:31:46,038][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-03 21:39:08,941][train_inner][INFO] - {"epoch": 26, "update": 25.62, "loss": "7.065", "nll_loss": "0.019", "loss_recon": "0.121", "loss_info_nce": "5.854", "ppl": "1.01", "wps": "922.9", "ups": "0.31", "wpb": "2956.5", "bsz": "385.4", "num_updates": "6200", "lr": "9.52174e-05", "gnorm": "3.133", "clip": "0.5", "loss_scale": "512", "train_wall": "586", "gb_free": "1.5", "wall": "3511"}
[2025-07-03 21:43:18,565][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 26 @ 6292 updates
[2025-07-03 21:43:18,566][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint26.pt
[2025-07-03 21:43:20,764][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint26.pt
[2025-07-03 21:43:21,615][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint26.pt (epoch 26 @ 6292 updates, score None) (writing took 3.063110500000221 seconds)
[2025-07-03 21:43:21,615][fairseq_cli.train][INFO] - end of epoch 26 (average epoch stats below)
[2025-07-03 21:43:21,630][train][INFO] - {"epoch": 26, "train_loss": "7.013", "train_nll_loss": "0.019", "train_loss_recon": "0.121", "train_loss_info_nce": "5.807", "train_ppl": "1.01", "train_wps": "1028.4", "train_ups": "0.35", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "6292", "train_lr": "9.50174e-05", "train_gnorm": "3.105", "train_clip": "0.4", "train_loss_scale": "512", "train_train_wall": "688", "train_gb_free": "1.6", "train_wall": "3764"}
[2025-07-03 21:43:21,827][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-03 21:43:21,843][fairseq.trainer][INFO] - begin training epoch 27
[2025-07-03 21:43:21,843][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-03 21:48:35,266][train_inner][INFO] - {"epoch": 27, "update": 26.446, "loss": "6.927", "nll_loss": "0.019", "loss_recon": "0.12", "loss_info_nce": "5.731", "ppl": "1.01", "wps": "1043.1", "ups": "0.35", "wpb": "2953.7", "bsz": "386.4", "num_updates": "6400", "lr": "9.47826e-05", "gnorm": "3.057", "clip": "0", "loss_scale": "512", "train_wall": "560", "gb_free": "1.5", "wall": "4078"}
[2025-07-03 21:54:53,449][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 27 @ 6534 updates
[2025-07-03 21:54:53,449][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint27.pt
[2025-07-03 21:54:55,625][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint27.pt
[2025-07-03 21:54:56,408][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint27.pt (epoch 27 @ 6534 updates, score None) (writing took 2.9531629000002795 seconds)
[2025-07-03 21:54:56,408][fairseq_cli.train][INFO] - end of epoch 27 (average epoch stats below)
[2025-07-03 21:54:56,408][train][INFO] - {"epoch": 27, "train_loss": "6.872", "train_nll_loss": "0.019", "train_loss_recon": "0.119", "train_loss_info_nce": "5.679", "train_ppl": "1.01", "train_wps": "1029.9", "train_ups": "0.35", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "6534", "train_lr": "9.44913e-05", "train_gnorm": "2.942", "train_clip": "0", "train_loss_scale": "512", "train_train_wall": "688", "train_gb_free": "1.5", "train_wall": "4459"}
[2025-07-03 21:54:56,580][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-03 21:54:56,580][fairseq.trainer][INFO] - begin training epoch 28
[2025-07-03 21:54:56,580][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-03 21:58:00,649][train_inner][INFO] - {"epoch": 28, "update": 27.273, "loss": "6.826", "nll_loss": "0.018", "loss_recon": "0.119", "loss_info_nce": "5.637", "ppl": "1.01", "wps": "1046.2", "ups": "0.35", "wpb": "2957.4", "bsz": "384.5", "num_updates": "6600", "lr": "9.43478e-05", "gnorm": "2.907", "clip": "0", "loss_scale": "512", "train_wall": "559", "gb_free": "1.5", "wall": "4643"}
[2025-07-03 22:06:19,755][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 28 @ 6776 updates
[2025-07-03 22:06:19,765][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint28.pt
[2025-07-03 22:06:21,906][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint28.pt
[2025-07-03 22:06:22,712][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint28.pt (epoch 28 @ 6776 updates, score None) (writing took 2.9509877000000415 seconds)
[2025-07-03 22:06:22,712][fairseq_cli.train][INFO] - end of epoch 28 (average epoch stats below)
[2025-07-03 22:06:22,727][train][INFO] - {"epoch": 28, "train_loss": "6.731", "train_nll_loss": "0.018", "train_loss_recon": "0.118", "train_loss_info_nce": "5.549", "train_ppl": "1.01", "train_wps": "1042.6", "train_ups": "0.35", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "6776", "train_lr": "9.39652e-05", "train_gnorm": "2.854", "train_clip": "0", "train_loss_scale": "512", "train_train_wall": "680", "train_gb_free": "1.5", "train_wall": "5145"}
[2025-07-03 22:06:22,899][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-03 22:06:22,899][fairseq.trainer][INFO] - begin training epoch 29
[2025-07-03 22:06:22,899][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-03 22:07:34,766][train_inner][INFO] - {"epoch": 29, "update": 28.099, "loss": "6.704", "nll_loss": "0.018", "loss_recon": "0.118", "loss_info_nce": "5.524", "ppl": "1.01", "wps": "1028.9", "ups": "0.35", "wpb": "2953.7", "bsz": "386.2", "num_updates": "6800", "lr": "9.3913e-05", "gnorm": "2.844", "clip": "0", "loss_scale": "512", "train_wall": "568", "gb_free": "1.5", "wall": "5217"}
[2025-07-03 22:17:43,992][train_inner][INFO] - {"epoch": 29, "update": 28.926, "loss": "6.588", "nll_loss": "0.018", "loss_recon": "0.117", "loss_info_nce": "5.419", "ppl": "1.01", "wps": "973", "ups": "0.33", "wpb": "2963.9", "bsz": "386.8", "num_updates": "7000", "lr": "9.34783e-05", "gnorm": "2.516", "clip": "0", "loss_scale": "512", "train_wall": "606", "gb_free": "1.5", "wall": "5826"}
[2025-07-03 22:18:37,007][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 29 @ 7018 updates
[2025-07-03 22:18:37,007][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint29.pt
[2025-07-03 22:18:45,487][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint29.pt
[2025-07-03 22:18:46,339][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint29.pt (epoch 29 @ 7018 updates, score None) (writing took 9.328567300000032 seconds)
[2025-07-03 22:18:46,339][fairseq_cli.train][INFO] - end of epoch 29 (average epoch stats below)
[2025-07-03 22:18:46,396][train][INFO] - {"epoch": 29, "train_loss": "6.59", "train_nll_loss": "0.018", "train_loss_recon": "0.117", "train_loss_info_nce": "5.42", "train_ppl": "1.01", "train_wps": "962.2", "train_ups": "0.33", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "7018", "train_lr": "9.34391e-05", "train_gnorm": "2.584", "train_clip": "0", "train_loss_scale": "512", "train_train_wall": "729", "train_gb_free": "1.5", "train_wall": "5889"}
[2025-07-03 22:18:46,605][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-03 22:18:46,621][fairseq.trainer][INFO] - begin training epoch 30
[2025-07-03 22:18:46,621][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-03 22:27:41,714][train_inner][INFO] - {"epoch": 30, "update": 29.752, "loss": "6.474", "nll_loss": "0.017", "loss_recon": "0.116", "loss_info_nce": "5.313", "ppl": "1.01", "wps": "988.9", "ups": "0.33", "wpb": "2955.5", "bsz": "385.8", "num_updates": "7200", "lr": "9.30435e-05", "gnorm": "2.574", "clip": "0", "loss_scale": "1024", "train_wall": "584", "gb_free": "1.5", "wall": "6424"}
[2025-07-03 22:30:26,160][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-03 22:30:56,394][valid][INFO] - {"epoch": 30, "valid_loss": "5.832", "valid_nll_loss": "0.016", "valid_loss_recon": "0.105", "valid_loss_info_nce": "4.78", "valid_ppl": "1.01", "valid_wps": "2729.2", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "7260", "valid_best_loss": "5.832"}
[2025-07-03 22:30:56,395][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 30 @ 7260 updates
[2025-07-03 22:30:56,395][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint30.pt
[2025-07-03 22:30:58,631][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint30.pt
[2025-07-03 22:31:00,178][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint30.pt (epoch 30 @ 7260 updates, score 5.832) (writing took 3.7832560000006197 seconds)
[2025-07-03 22:31:00,178][fairseq_cli.train][INFO] - end of epoch 30 (average epoch stats below)
[2025-07-03 22:31:00,178][train][INFO] - {"epoch": 30, "train_loss": "6.451", "train_nll_loss": "0.017", "train_loss_recon": "0.116", "train_loss_info_nce": "5.292", "train_ppl": "1.01", "train_wps": "975.2", "train_ups": "0.33", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "7260", "train_lr": "9.2913e-05", "train_gnorm": "2.604", "train_clip": "0", "train_loss_scale": "1024", "train_train_wall": "695", "train_gb_free": "1.5", "train_wall": "6623"}
[2025-07-03 22:31:00,345][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-03 22:31:00,355][fairseq.trainer][INFO] - begin training epoch 31
[2025-07-03 22:31:00,355][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-03 22:38:08,889][train_inner][INFO] - {"epoch": 31, "update": 30.579, "loss": "6.367", "nll_loss": "0.017", "loss_recon": "0.115", "loss_info_nce": "5.215", "ppl": "1.01", "wps": "943.1", "ups": "0.32", "wpb": "2957.4", "bsz": "384.6", "num_updates": "7400", "lr": "9.26087e-05", "gnorm": "2.664", "clip": "0", "loss_scale": "1024", "train_wall": "588", "gb_free": "1.5", "wall": "7051"}
[2025-07-03 22:43:19,935][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 31 @ 7502 updates
[2025-07-03 22:43:19,937][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint31.pt
[2025-07-03 22:43:22,220][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint31.pt
[2025-07-03 22:43:23,472][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint31.pt (epoch 31 @ 7502 updates, score None) (writing took 3.540410900000097 seconds)
[2025-07-03 22:43:23,472][fairseq_cli.train][INFO] - end of epoch 31 (average epoch stats below)
[2025-07-03 22:43:23,472][train][INFO] - {"epoch": 31, "train_loss": "6.333", "train_nll_loss": "0.017", "train_loss_recon": "0.115", "train_loss_info_nce": "5.183", "train_ppl": "1.01", "train_wps": "962.7", "train_ups": "0.33", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "7502", "train_lr": "9.2387e-05", "train_gnorm": "2.661", "train_clip": "0", "train_loss_scale": "1024", "train_train_wall": "734", "train_gb_free": "1.5", "train_wall": "7366"}
[2025-07-03 22:43:23,865][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-03 22:43:23,865][fairseq.trainer][INFO] - begin training epoch 32
[2025-07-03 22:43:23,880][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-03 22:48:39,039][train_inner][INFO] - {"epoch": 32, "update": 31.405, "loss": "6.277", "nll_loss": "0.017", "loss_recon": "0.115", "loss_info_nce": "5.131", "ppl": "1.01", "wps": "936.6", "ups": "0.32", "wpb": "2950.9", "bsz": "386.4", "num_updates": "7600", "lr": "9.21739e-05", "gnorm": "2.608", "clip": "0", "loss_scale": "1024", "train_wall": "620", "gb_free": "1.5", "wall": "7681"}
[2025-07-03 22:55:17,495][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 32 @ 7744 updates
[2025-07-03 22:55:17,495][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint32.pt
[2025-07-03 22:55:19,688][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint32.pt
[2025-07-03 22:55:20,473][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint32.pt (epoch 32 @ 7744 updates, score None) (writing took 2.9861393999999564 seconds)
[2025-07-03 22:55:20,473][fairseq_cli.train][INFO] - end of epoch 32 (average epoch stats below)
[2025-07-03 22:55:20,488][train][INFO] - {"epoch": 32, "train_loss": "6.221", "train_nll_loss": "0.017", "train_loss_recon": "0.114", "train_loss_info_nce": "5.081", "train_ppl": "1.01", "train_wps": "998", "train_ups": "0.34", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "7744", "train_lr": "9.18609e-05", "train_gnorm": "2.515", "train_clip": "0", "train_loss_scale": "1024", "train_train_wall": "708", "train_gb_free": "1.5", "train_wall": "8083"}
[2025-07-03 22:55:20,685][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-03 22:55:20,685][fairseq.trainer][INFO] - begin training epoch 33
[2025-07-03 22:55:20,685][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-03 22:57:57,169][train_inner][INFO] - {"epoch": 33, "update": 32.231, "loss": "6.187", "nll_loss": "0.017", "loss_recon": "0.113", "loss_info_nce": "5.054", "ppl": "1.01", "wps": "1060.1", "ups": "0.36", "wpb": "2958.3", "bsz": "385.3", "num_updates": "7800", "lr": "9.17391e-05", "gnorm": "2.515", "clip": "0", "loss_scale": "1024", "train_wall": "551", "gb_free": "1.5", "wall": "8240"}
[2025-07-03 23:06:38,904][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 33 @ 7986 updates
[2025-07-03 23:06:38,904][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint33.pt
[2025-07-03 23:06:41,203][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint33.pt
[2025-07-03 23:06:42,012][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint33.pt (epoch 33 @ 7986 updates, score None) (writing took 3.111540799998693 seconds)
[2025-07-03 23:06:42,012][fairseq_cli.train][INFO] - end of epoch 33 (average epoch stats below)
[2025-07-03 23:06:42,028][train][INFO] - {"epoch": 33, "train_loss": "6.133", "train_nll_loss": "0.017", "train_loss_recon": "0.113", "train_loss_info_nce": "5.004", "train_ppl": "1.01", "train_wps": "1049.9", "train_ups": "0.36", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "7986", "train_lr": "9.13348e-05", "train_gnorm": "2.293", "train_clip": "0", "train_loss_scale": "1024", "train_train_wall": "675", "train_gb_free": "1.5", "train_wall": "8764"}
[2025-07-03 23:06:42,212][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-03 23:06:42,212][fairseq.trainer][INFO] - begin training epoch 34
[2025-07-03 23:06:42,212][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-03 23:07:22,862][train_inner][INFO] - {"epoch": 34, "update": 33.058, "loss": "6.123", "nll_loss": "0.017", "loss_recon": "0.113", "loss_info_nce": "4.994", "ppl": "1.01", "wps": "1043.9", "ups": "0.35", "wpb": "2952.8", "bsz": "386.2", "num_updates": "8000", "lr": "9.13043e-05", "gnorm": "2.265", "clip": "0", "loss_scale": "1024", "train_wall": "559", "gb_free": "1.5", "wall": "8805"}
[2025-07-03 23:16:29,626][train_inner][INFO] - {"epoch": 34, "update": 33.884, "loss": "6.066", "nll_loss": "0.016", "loss_recon": "0.112", "loss_info_nce": "4.945", "ppl": "1.01", "wps": "1085.2", "ups": "0.37", "wpb": "2966.7", "bsz": "386", "num_updates": "8200", "lr": "9.08696e-05", "gnorm": "2.139", "clip": "0", "loss_scale": "1024", "train_wall": "545", "gb_free": "1.5", "wall": "9352"}
[2025-07-03 23:17:45,038][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 34 @ 8228 updates
[2025-07-03 23:17:45,038][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint34.pt
[2025-07-03 23:17:47,237][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint34.pt
[2025-07-03 23:17:48,037][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint34.pt (epoch 34 @ 8228 updates, score None) (writing took 3.0029681000014534 seconds)
[2025-07-03 23:17:48,037][fairseq_cli.train][INFO] - end of epoch 34 (average epoch stats below)
[2025-07-03 23:17:48,053][train][INFO] - {"epoch": 34, "train_loss": "6.064", "train_nll_loss": "0.016", "train_loss_recon": "0.112", "train_loss_info_nce": "4.944", "train_ppl": "1.01", "train_wps": "1074.4", "train_ups": "0.36", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "8228", "train_lr": "9.08087e-05", "train_gnorm": "2.162", "train_clip": "0", "train_loss_scale": "1024", "train_train_wall": "659", "train_gb_free": "1.5", "train_wall": "9430"}
[2025-07-03 23:17:48,223][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-03 23:17:48,223][fairseq.trainer][INFO] - begin training epoch 35
[2025-07-03 23:17:48,223][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-03 23:25:43,898][train_inner][INFO] - {"epoch": 35, "update": 34.711, "loss": "6.015", "nll_loss": "0.016", "loss_recon": "0.111", "loss_info_nce": "4.902", "ppl": "1.01", "wps": "1065.5", "ups": "0.36", "wpb": "2952.8", "bsz": "386.7", "num_updates": "8400", "lr": "9.04348e-05", "gnorm": "2.208", "clip": "0", "loss_scale": "1024", "train_wall": "548", "gb_free": "1.5", "wall": "9906"}
[2025-07-03 23:28:54,731][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-03 23:29:24,343][valid][INFO] - {"epoch": 35, "valid_loss": "5.612", "valid_nll_loss": "0.015", "valid_loss_recon": "0.105", "valid_loss_info_nce": "4.564", "valid_ppl": "1.01", "valid_wps": "2804.1", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "8470", "valid_best_loss": "5.612"}
[2025-07-03 23:29:24,343][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 35 @ 8470 updates
[2025-07-03 23:29:24,343][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint35.pt
[2025-07-03 23:29:26,441][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint35.pt
[2025-07-03 23:29:28,128][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint35.pt (epoch 35 @ 8470 updates, score 5.612) (writing took 3.784897400000773 seconds)
[2025-07-03 23:29:28,128][fairseq_cli.train][INFO] - end of epoch 35 (average epoch stats below)
[2025-07-03 23:29:28,154][train][INFO] - {"epoch": 35, "train_loss": "6.006", "train_nll_loss": "0.016", "train_loss_recon": "0.111", "train_loss_info_nce": "4.892", "train_ppl": "1.01", "train_wps": "1022.1", "train_ups": "0.35", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "8470", "train_lr": "9.02826e-05", "train_gnorm": "2.132", "train_clip": "0", "train_loss_scale": "1024", "train_train_wall": "663", "train_gb_free": "1.5", "train_wall": "10130"}
[2025-07-03 23:29:28,399][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-03 23:29:28,399][fairseq.trainer][INFO] - begin training epoch 36
[2025-07-03 23:29:28,399][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-03 23:35:33,651][train_inner][INFO] - {"epoch": 36, "update": 35.537, "loss": "5.971", "nll_loss": "0.016", "loss_recon": "0.111", "loss_info_nce": "4.859", "ppl": "1.01", "wps": "1002", "ups": "0.34", "wpb": "2954.6", "bsz": "385.4", "num_updates": "8600", "lr": "9e-05", "gnorm": "1.986", "clip": "0", "loss_scale": "1024", "train_wall": "553", "gb_free": "1.5", "wall": "10496"}
[2025-07-03 23:40:52,383][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 36 @ 8712 updates
[2025-07-03 23:40:52,383][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint36.pt
[2025-07-03 23:40:54,639][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint36.pt
[2025-07-03 23:40:55,376][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint36.pt (epoch 36 @ 8712 updates, score None) (writing took 2.9909590000006574 seconds)
[2025-07-03 23:40:55,376][fairseq_cli.train][INFO] - end of epoch 36 (average epoch stats below)
[2025-07-03 23:40:55,376][train][INFO] - {"epoch": 36, "train_loss": "5.95", "train_nll_loss": "0.016", "train_loss_recon": "0.111", "train_loss_info_nce": "4.843", "train_ppl": "1.01", "train_wps": "1041.2", "train_ups": "0.35", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "8712", "train_lr": "8.97565e-05", "train_gnorm": "1.998", "train_clip": "0", "train_loss_scale": "1024", "train_train_wall": "680", "train_gb_free": "1.5", "train_wall": "10818"}
[2025-07-03 23:40:55,558][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-03 23:40:55,558][fairseq.trainer][INFO] - begin training epoch 37
[2025-07-03 23:40:55,558][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-03 23:45:09,762][train_inner][INFO] - {"epoch": 37, "update": 36.364, "loss": "5.932", "nll_loss": "0.016", "loss_recon": "0.11", "loss_info_nce": "4.827", "ppl": "1.01", "wps": "1026.4", "ups": "0.35", "wpb": "2956.5", "bsz": "384.8", "num_updates": "8800", "lr": "8.95652e-05", "gnorm": "2.073", "clip": "0", "loss_scale": "1024", "train_wall": "570", "gb_free": "1.5", "wall": "11072"}
[2025-07-03 23:52:08,030][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 37 @ 8954 updates
[2025-07-03 23:52:08,030][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint37.pt
[2025-07-03 23:52:10,238][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint37.pt
[2025-07-03 23:52:10,970][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint37.pt (epoch 37 @ 8954 updates, score None) (writing took 2.935402699999031 seconds)
[2025-07-03 23:52:10,970][fairseq_cli.train][INFO] - end of epoch 37 (average epoch stats below)
[2025-07-03 23:52:10,985][train][INFO] - {"epoch": 37, "train_loss": "5.905", "train_nll_loss": "0.016", "train_loss_recon": "0.11", "train_loss_info_nce": "4.803", "train_ppl": "1.01", "train_wps": "1059.1", "train_ups": "0.36", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "8954", "train_lr": "8.92304e-05", "train_gnorm": "1.957", "train_clip": "0", "train_loss_scale": "1024", "train_train_wall": "669", "train_gb_free": "1.5", "train_wall": "11493"}
[2025-07-03 23:52:11,242][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-03 23:52:11,242][fairseq.trainer][INFO] - begin training epoch 38
[2025-07-03 23:52:11,242][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-03 23:54:25,541][train_inner][INFO] - {"epoch": 38, "update": 37.19, "loss": "5.889", "nll_loss": "0.016", "loss_recon": "0.11", "loss_info_nce": "4.792", "ppl": "1.01", "wps": "1063.6", "ups": "0.36", "wpb": "2955.5", "bsz": "386.2", "num_updates": "9000", "lr": "8.91304e-05", "gnorm": "1.911", "clip": "0", "loss_scale": "1024", "train_wall": "549", "gb_free": "1.5", "wall": "11628"}
[2025-07-04 00:03:31,733][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 38 @ 9196 updates
[2025-07-04 00:03:31,733][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint38.pt
[2025-07-04 00:03:34,009][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint38.pt
[2025-07-04 00:03:34,902][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint38.pt (epoch 38 @ 9196 updates, score None) (writing took 3.168931500000326 seconds)
[2025-07-04 00:03:34,902][fairseq_cli.train][INFO] - end of epoch 38 (average epoch stats below)
[2025-07-04 00:03:34,902][train][INFO] - {"epoch": 38, "train_loss": "5.86", "train_nll_loss": "0.016", "train_loss_recon": "0.11", "train_loss_info_nce": "4.764", "train_ppl": "1.01", "train_wps": "1046.2", "train_ups": "0.35", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "9196", "train_lr": "8.87043e-05", "train_gnorm": "1.842", "train_clip": "0", "train_loss_scale": "2048", "train_train_wall": "677", "train_gb_free": "1.5", "train_wall": "12177"}
[2025-07-04 00:03:35,099][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 00:03:35,099][fairseq.trainer][INFO] - begin training epoch 39
[2025-07-04 00:03:35,099][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 00:03:48,692][train_inner][INFO] - {"epoch": 39, "update": 38.017, "loss": "5.856", "nll_loss": "0.016", "loss_recon": "0.11", "loss_info_nce": "4.76", "ppl": "1.01", "wps": "1049.6", "ups": "0.36", "wpb": "2955.5", "bsz": "385.4", "num_updates": "9200", "lr": "8.86957e-05", "gnorm": "1.766", "clip": "0", "loss_scale": "2048", "train_wall": "557", "gb_free": "1.5", "wall": "12191"}
[2025-07-04 00:12:45,738][train_inner][INFO] - {"epoch": 39, "update": 38.843, "loss": "5.82", "nll_loss": "0.016", "loss_recon": "0.109", "loss_info_nce": "4.73", "ppl": "1.01", "wps": "1103.8", "ups": "0.37", "wpb": "2963.9", "bsz": "386.8", "num_updates": "9400", "lr": "8.82609e-05", "gnorm": "1.866", "clip": "0", "loss_scale": "2048", "train_wall": "536", "gb_free": "1.5", "wall": "12728"}
[2025-07-04 00:14:25,013][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 39 @ 9438 updates
[2025-07-04 00:14:25,029][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint39.pt
[2025-07-04 00:14:27,258][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint39.pt
[2025-07-04 00:14:28,073][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint39.pt (epoch 39 @ 9438 updates, score None) (writing took 3.0493428999998287 seconds)
[2025-07-04 00:14:28,073][fairseq_cli.train][INFO] - end of epoch 39 (average epoch stats below)
[2025-07-04 00:14:28,089][train][INFO] - {"epoch": 39, "train_loss": "5.818", "train_nll_loss": "0.016", "train_loss_recon": "0.109", "train_loss_info_nce": "4.727", "train_ppl": "1.01", "train_wps": "1095.5", "train_ups": "0.37", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "9438", "train_lr": "8.81783e-05", "train_gnorm": "1.896", "train_clip": "0", "train_loss_scale": "2048", "train_train_wall": "647", "train_gb_free": "1.5", "train_wall": "12830"}
[2025-07-04 00:14:28,313][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 00:14:28,313][fairseq.trainer][INFO] - begin training epoch 40
[2025-07-04 00:14:28,313][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 00:21:42,855][train_inner][INFO] - {"epoch": 40, "update": 39.669, "loss": "5.785", "nll_loss": "0.016", "loss_recon": "0.109", "loss_info_nce": "4.699", "ppl": "1.01", "wps": "1100.2", "ups": "0.37", "wpb": "2954.6", "bsz": "386.1", "num_updates": "9600", "lr": "8.78261e-05", "gnorm": "1.957", "clip": "0", "loss_scale": "2048", "train_wall": "531", "gb_free": "1.5", "wall": "13265"}
[2025-07-04 00:25:12,455][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-04 00:25:42,158][valid][INFO] - {"epoch": 40, "valid_loss": "5.395", "valid_nll_loss": "0.015", "valid_loss_recon": "0.1", "valid_loss_info_nce": "4.395", "valid_ppl": "1.01", "valid_wps": "2782.3", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "9680", "valid_best_loss": "5.395"}
[2025-07-04 00:25:42,158][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 40 @ 9680 updates
[2025-07-04 00:25:42,158][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint40.pt
[2025-07-04 00:25:44,422][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint40.pt
[2025-07-04 00:25:45,861][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint40.pt (epoch 40 @ 9680 updates, score 5.395) (writing took 3.710350400000607 seconds)
[2025-07-04 00:25:45,861][fairseq_cli.train][INFO] - end of epoch 40 (average epoch stats below)
[2025-07-04 00:25:45,903][train][INFO] - {"epoch": 40, "train_loss": "5.776", "train_nll_loss": "0.016", "train_loss_recon": "0.109", "train_loss_info_nce": "4.689", "train_ppl": "1.01", "train_wps": "1055.7", "train_ups": "0.36", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "9680", "train_lr": "8.76522e-05", "train_gnorm": "1.99", "train_clip": "0", "train_loss_scale": "2048", "train_train_wall": "641", "train_gb_free": "1.5", "train_wall": "13508"}
[2025-07-04 00:25:46,106][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 00:25:46,106][fairseq.trainer][INFO] - begin training epoch 41
[2025-07-04 00:25:46,106][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 00:31:06,956][train_inner][INFO] - {"epoch": 41, "update": 40.496, "loss": "5.758", "nll_loss": "0.016", "loss_recon": "0.109", "loss_info_nce": "4.67", "ppl": "1.01", "wps": "1047.2", "ups": "0.35", "wpb": "2953.7", "bsz": "386.2", "num_updates": "9800", "lr": "8.73913e-05", "gnorm": "1.909", "clip": "0", "loss_scale": "2048", "train_wall": "527", "gb_free": "1.5", "wall": "13829"}
[2025-07-04 00:36:25,939][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 41 @ 9922 updates
[2025-07-04 00:36:25,939][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint41.pt
[2025-07-04 00:36:28,153][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint41.pt
[2025-07-04 00:36:28,926][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint41.pt (epoch 41 @ 9922 updates, score None) (writing took 2.9983155999998417 seconds)
[2025-07-04 00:36:28,941][fairseq_cli.train][INFO] - end of epoch 41 (average epoch stats below)
[2025-07-04 00:36:28,942][train][INFO] - {"epoch": 41, "train_loss": "5.741", "train_nll_loss": "0.015", "train_loss_recon": "0.108", "train_loss_info_nce": "4.657", "train_ppl": "1.01", "train_wps": "1112.8", "train_ups": "0.38", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "9922", "train_lr": "8.71261e-05", "train_gnorm": "1.867", "train_clip": "0", "train_loss_scale": "2048", "train_train_wall": "637", "train_gb_free": "1.5", "train_wall": "14151"}
[2025-07-04 00:36:29,157][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 00:36:29,167][fairseq.trainer][INFO] - begin training epoch 42
[2025-07-04 00:36:29,167][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 00:39:58,012][train_inner][INFO] - {"epoch": 42, "update": 41.322, "loss": "5.721", "nll_loss": "0.015", "loss_recon": "0.108", "loss_info_nce": "4.641", "ppl": "1.01", "wps": "1114.1", "ups": "0.38", "wpb": "2958.3", "bsz": "384.3", "num_updates": "10000", "lr": "8.69565e-05", "gnorm": "1.882", "clip": "0", "loss_scale": "2048", "train_wall": "525", "gb_free": "1.5", "wall": "14360"}
[2025-07-04 00:39:58,012][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-04 00:40:28,087][valid][INFO] - {"epoch": 42, "valid_loss": "5.322", "valid_nll_loss": "0.014", "valid_loss_recon": "0.098", "valid_loss_info_nce": "4.34", "valid_ppl": "1.01", "valid_wps": "2742.8", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "10000", "valid_best_loss": "5.322"}
[2025-07-04 00:47:39,902][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 42 @ 10164 updates
[2025-07-04 00:47:39,902][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint42.pt
[2025-07-04 00:47:42,139][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint42.pt
[2025-07-04 00:47:42,954][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint42.pt (epoch 42 @ 10164 updates, score None) (writing took 3.0599238999984664 seconds)
[2025-07-04 00:47:42,954][fairseq_cli.train][INFO] - end of epoch 42 (average epoch stats below)
[2025-07-04 00:47:42,964][train][INFO] - {"epoch": 42, "train_loss": "5.696", "train_nll_loss": "0.015", "train_loss_recon": "0.108", "train_loss_info_nce": "4.617", "train_ppl": "1.01", "train_wps": "1061.6", "train_ups": "0.36", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "10164", "train_lr": "8.66e-05", "train_gnorm": "1.783", "train_clip": "0", "train_loss_scale": "2048", "train_train_wall": "638", "train_gb_free": "1.5", "train_wall": "14825"}
[2025-07-04 00:47:43,137][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 00:47:43,152][fairseq.trainer][INFO] - begin training epoch 43
[2025-07-04 00:47:43,152][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 00:49:20,047][train_inner][INFO] - {"epoch": 43, "update": 42.149, "loss": "5.688", "nll_loss": "0.015", "loss_recon": "0.108", "loss_info_nce": "4.608", "ppl": "1.01", "wps": "1051.7", "ups": "0.36", "wpb": "2955.5", "bsz": "385.6", "num_updates": "10200", "lr": "8.65217e-05", "gnorm": "1.747", "clip": "0", "loss_scale": "2048", "train_wall": "526", "gb_free": "1.5", "wall": "14922"}
[2025-07-04 00:58:09,615][train_inner][INFO] - {"epoch": 43, "update": 42.975, "loss": "5.656", "nll_loss": "0.015", "loss_recon": "0.108", "loss_info_nce": "4.581", "ppl": "1.01", "wps": "1119.4", "ups": "0.38", "wpb": "2963.9", "bsz": "386.8", "num_updates": "10400", "lr": "8.6087e-05", "gnorm": "1.723", "clip": "0", "loss_scale": "2048", "train_wall": "528", "gb_free": "1.5", "wall": "15452"}
[2025-07-04 00:58:23,806][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 43 @ 10406 updates
[2025-07-04 00:58:23,806][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint43.pt
[2025-07-04 00:58:25,996][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint43.pt
[2025-07-04 00:58:26,686][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint43.pt (epoch 43 @ 10406 updates, score None) (writing took 2.8752732000011747 seconds)
[2025-07-04 00:58:26,686][fairseq_cli.train][INFO] - end of epoch 43 (average epoch stats below)
[2025-07-04 00:58:26,702][train][INFO] - {"epoch": 43, "train_loss": "5.659", "train_nll_loss": "0.015", "train_loss_recon": "0.108", "train_loss_info_nce": "4.583", "train_ppl": "1.01", "train_wps": "1111.6", "train_ups": "0.38", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "10406", "train_lr": "8.60739e-05", "train_gnorm": "1.71", "train_clip": "0", "train_loss_scale": "2048", "train_train_wall": "637", "train_gb_free": "1.5", "train_wall": "15469"}
[2025-07-04 00:58:26,925][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 00:58:26,925][fairseq.trainer][INFO] - begin training epoch 44
[2025-07-04 00:58:26,925][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 01:07:02,564][train_inner][INFO] - {"epoch": 44, "update": 43.802, "loss": "5.628", "nll_loss": "0.015", "loss_recon": "0.107", "loss_info_nce": "4.556", "ppl": "1.01", "wps": "1109.5", "ups": "0.38", "wpb": "2956.5", "bsz": "385.4", "num_updates": "10600", "lr": "8.56522e-05", "gnorm": "1.742", "clip": "0", "loss_scale": "2048", "train_wall": "527", "gb_free": "1.5", "wall": "15985"}
[2025-07-04 01:09:07,666][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 44 @ 10648 updates
[2025-07-04 01:09:07,666][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint44.pt
[2025-07-04 01:09:09,920][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint44.pt
[2025-07-04 01:09:10,734][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint44.pt (epoch 44 @ 10648 updates, score None) (writing took 3.0558789999995497 seconds)
[2025-07-04 01:09:10,734][fairseq_cli.train][INFO] - end of epoch 44 (average epoch stats below)
[2025-07-04 01:09:10,734][train][INFO] - {"epoch": 44, "train_loss": "5.622", "train_nll_loss": "0.015", "train_loss_recon": "0.107", "train_loss_info_nce": "4.552", "train_ppl": "1.01", "train_wps": "1111", "train_ups": "0.38", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "10648", "train_lr": "8.55478e-05", "train_gnorm": "1.712", "train_clip": "0", "train_loss_scale": "2048", "train_train_wall": "638", "train_gb_free": "1.5", "train_wall": "16113"}
[2025-07-04 01:09:10,915][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 01:09:10,915][fairseq.trainer][INFO] - begin training epoch 45
[2025-07-04 01:09:10,915][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 01:15:55,417][train_inner][INFO] - {"epoch": 45, "update": 44.628, "loss": "5.599", "nll_loss": "0.015", "loss_recon": "0.107", "loss_info_nce": "4.532", "ppl": "1.01", "wps": "1107.9", "ups": "0.38", "wpb": "2951.8", "bsz": "386.2", "num_updates": "10800", "lr": "8.52174e-05", "gnorm": "1.648", "clip": "0", "loss_scale": "2048", "train_wall": "527", "gb_free": "1.5", "wall": "16518"}
[2025-07-04 01:19:51,612][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-04 01:20:20,056][valid][INFO] - {"epoch": 45, "valid_loss": "5.236", "valid_nll_loss": "0.014", "valid_loss_recon": "0.099", "valid_loss_info_nce": "4.244", "valid_ppl": "1.01", "valid_wps": "2890.6", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "10890", "valid_best_loss": "5.236"}
[2025-07-04 01:20:20,056][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 45 @ 10890 updates
[2025-07-04 01:20:20,056][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint45.pt
[2025-07-04 01:20:28,389][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint45.pt
[2025-07-04 01:20:30,802][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint45.pt (epoch 45 @ 10890 updates, score 5.236) (writing took 10.739394000000175 seconds)
[2025-07-04 01:20:30,802][fairseq_cli.train][INFO] - end of epoch 45 (average epoch stats below)
[2025-07-04 01:20:30,827][train][INFO] - {"epoch": 45, "train_loss": "5.593", "train_nll_loss": "0.015", "train_loss_recon": "0.107", "train_loss_info_nce": "4.526", "train_ppl": "1.01", "train_wps": "1052.2", "train_ups": "0.36", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "10890", "train_lr": "8.50217e-05", "train_gnorm": "1.573", "train_clip": "0", "train_loss_scale": "2048", "train_train_wall": "637", "train_gb_free": "1.5", "train_wall": "16793"}
[2025-07-04 01:20:31,040][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 01:20:31,040][fairseq.trainer][INFO] - begin training epoch 46
[2025-07-04 01:20:31,040][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 01:25:25,737][train_inner][INFO] - {"epoch": 46, "update": 45.455, "loss": "5.575", "nll_loss": "0.015", "loss_recon": "0.106", "loss_info_nce": "4.511", "ppl": "1.01", "wps": "1035.5", "ups": "0.35", "wpb": "2952.8", "bsz": "386.6", "num_updates": "11000", "lr": "8.47826e-05", "gnorm": "1.597", "clip": "0", "loss_scale": "2048", "train_wall": "528", "gb_free": "1.5", "wall": "17088"}
[2025-07-04 01:31:11,705][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 46 @ 11132 updates
[2025-07-04 01:31:11,705][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint46.pt
[2025-07-04 01:31:13,934][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint46.pt
[2025-07-04 01:31:14,756][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint46.pt (epoch 46 @ 11132 updates, score None) (writing took 3.0408597999994527 seconds)
[2025-07-04 01:31:14,756][fairseq_cli.train][INFO] - end of epoch 46 (average epoch stats below)
[2025-07-04 01:31:14,766][train][INFO] - {"epoch": 46, "train_loss": "5.562", "train_nll_loss": "0.015", "train_loss_recon": "0.106", "train_loss_info_nce": "4.498", "train_ppl": "1.01", "train_wps": "1111.2", "train_ups": "0.38", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "11132", "train_lr": "8.44957e-05", "train_gnorm": "1.561", "train_clip": "0", "train_loss_scale": "2048", "train_train_wall": "638", "train_gb_free": "1.5", "train_wall": "17437"}
[2025-07-04 01:31:14,938][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 01:31:14,938][fairseq.trainer][INFO] - begin training epoch 47
[2025-07-04 01:31:14,938][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 01:34:16,482][train_inner][INFO] - {"epoch": 47, "update": 46.281, "loss": "5.555", "nll_loss": "0.015", "loss_recon": "0.106", "loss_info_nce": "4.49", "ppl": "1.01", "wps": "1115.1", "ups": "0.38", "wpb": "2959.3", "bsz": "384.1", "num_updates": "11200", "lr": "8.43478e-05", "gnorm": "1.497", "clip": "0", "loss_scale": "2048", "train_wall": "525", "gb_free": "1.5", "wall": "17619"}
[2025-07-04 01:41:56,414][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 47 @ 11374 updates
[2025-07-04 01:41:56,414][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint47.pt
[2025-07-04 01:41:58,691][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint47.pt
[2025-07-04 01:41:59,439][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint47.pt (epoch 47 @ 11374 updates, score None) (writing took 3.026312699999835 seconds)
[2025-07-04 01:41:59,439][fairseq_cli.train][INFO] - end of epoch 47 (average epoch stats below)
[2025-07-04 01:41:59,465][train][INFO] - {"epoch": 47, "train_loss": "5.536", "train_nll_loss": "0.015", "train_loss_recon": "0.106", "train_loss_info_nce": "4.476", "train_ppl": "1.01", "train_wps": "1109.9", "train_ups": "0.38", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "11374", "train_lr": "8.39696e-05", "train_gnorm": "1.684", "train_clip": "0", "train_loss_scale": "4096", "train_train_wall": "638", "train_gb_free": "1.5", "train_wall": "18082"}
[2025-07-04 01:41:59,651][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 01:41:59,651][fairseq.trainer][INFO] - begin training epoch 48
[2025-07-04 01:41:59,651][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 01:43:11,333][train_inner][INFO] - {"epoch": 48, "update": 47.107, "loss": "5.531", "nll_loss": "0.015", "loss_recon": "0.106", "loss_info_nce": "4.472", "ppl": "1.01", "wps": "1105", "ups": "0.37", "wpb": "2954.6", "bsz": "386.1", "num_updates": "11400", "lr": "8.3913e-05", "gnorm": "1.81", "clip": "0", "loss_scale": "4096", "train_wall": "528", "gb_free": "1.5", "wall": "18154"}
[2025-07-04 01:52:01,683][train_inner][INFO] - {"epoch": 48, "update": 47.934, "loss": "5.507", "nll_loss": "0.015", "loss_recon": "0.105", "loss_info_nce": "4.454", "ppl": "1.01", "wps": "1118.1", "ups": "0.38", "wpb": "2964.8", "bsz": "387.1", "num_updates": "11600", "lr": "8.34783e-05", "gnorm": "1.491", "clip": "0", "loss_scale": "4096", "train_wall": "529", "gb_free": "1.5", "wall": "18684"}
[2025-07-04 01:52:41,754][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 48 @ 11616 updates
[2025-07-04 01:52:41,754][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint48.pt
[2025-07-04 01:52:44,009][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint48.pt
[2025-07-04 01:52:44,748][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint48.pt (epoch 48 @ 11616 updates, score None) (writing took 2.997789900000498 seconds)
[2025-07-04 01:52:44,748][fairseq_cli.train][INFO] - end of epoch 48 (average epoch stats below)
[2025-07-04 01:52:44,758][train][INFO] - {"epoch": 48, "train_loss": "5.511", "train_nll_loss": "0.015", "train_loss_recon": "0.106", "train_loss_info_nce": "4.455", "train_ppl": "1.01", "train_wps": "1108.9", "train_ups": "0.38", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "11616", "train_lr": "8.34435e-05", "train_gnorm": "1.661", "train_clip": "0", "train_loss_scale": "4096", "train_train_wall": "639", "train_gb_free": "1.5", "train_wall": "18727"}
[2025-07-04 01:52:45,005][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 01:52:45,005][fairseq.trainer][INFO] - begin training epoch 49
[2025-07-04 01:52:45,005][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 02:00:54,238][train_inner][INFO] - {"epoch": 49, "update": 48.76, "loss": "5.496", "nll_loss": "0.015", "loss_recon": "0.106", "loss_info_nce": "4.44", "ppl": "1.01", "wps": "1109.9", "ups": "0.38", "wpb": "2955.5", "bsz": "385.4", "num_updates": "11800", "lr": "8.30435e-05", "gnorm": "1.582", "clip": "0", "loss_scale": "4096", "train_wall": "526", "gb_free": "1.5", "wall": "19217"}
[2025-07-04 02:03:25,586][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 49 @ 11858 updates
[2025-07-04 02:03:25,586][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint49.pt
[2025-07-04 02:03:27,779][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint49.pt
[2025-07-04 02:03:28,501][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint49.pt (epoch 49 @ 11858 updates, score None) (writing took 2.919077099999413 seconds)
[2025-07-04 02:03:28,501][fairseq_cli.train][INFO] - end of epoch 49 (average epoch stats below)
[2025-07-04 02:03:28,511][train][INFO] - {"epoch": 49, "train_loss": "5.492", "train_nll_loss": "0.015", "train_loss_recon": "0.105", "train_loss_info_nce": "4.437", "train_ppl": "1.01", "train_wps": "1111.5", "train_ups": "0.38", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "11858", "train_lr": "8.29174e-05", "train_gnorm": "1.481", "train_clip": "0", "train_loss_scale": "4096", "train_train_wall": "637", "train_gb_free": "1.5", "train_wall": "19371"}
[2025-07-04 02:03:28,741][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 02:03:28,741][fairseq.trainer][INFO] - begin training epoch 50
[2025-07-04 02:03:28,741][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 02:09:47,051][train_inner][INFO] - {"epoch": 50, "update": 49.587, "loss": "5.476", "nll_loss": "0.015", "loss_recon": "0.105", "loss_info_nce": "4.422", "ppl": "1.01", "wps": "1108", "ups": "0.38", "wpb": "2951.8", "bsz": "386.3", "num_updates": "12000", "lr": "8.26087e-05", "gnorm": "1.496", "clip": "0", "loss_scale": "4096", "train_wall": "527", "gb_free": "1.5", "wall": "19749"}
[2025-07-04 02:14:09,126][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-04 02:14:39,423][valid][INFO] - {"epoch": 50, "valid_loss": "5.125", "valid_nll_loss": "0.014", "valid_loss_recon": "0.097", "valid_loss_info_nce": "4.152", "valid_ppl": "1.01", "valid_wps": "2716.1", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "12100", "valid_best_loss": "5.125"}
[2025-07-04 02:14:39,423][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 50 @ 12100 updates
[2025-07-04 02:14:39,423][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint50.pt
[2025-07-04 02:14:41,666][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint50.pt
[2025-07-04 02:14:43,094][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint50.pt (epoch 50 @ 12100 updates, score 5.125) (writing took 3.6761541000014404 seconds)
[2025-07-04 02:14:43,094][fairseq_cli.train][INFO] - end of epoch 50 (average epoch stats below)
[2025-07-04 02:14:43,104][train][INFO] - {"epoch": 50, "train_loss": "5.466", "train_nll_loss": "0.015", "train_loss_recon": "0.105", "train_loss_info_nce": "4.415", "train_ppl": "1.01", "train_wps": "1060.7", "train_ups": "0.36", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "12100", "train_lr": "8.23913e-05", "train_gnorm": "1.483", "train_clip": "0", "train_loss_scale": "4096", "train_train_wall": "637", "train_gb_free": "1.5", "train_wall": "20045"}
[2025-07-04 02:14:43,333][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 02:14:43,333][fairseq.trainer][INFO] - begin training epoch 51
[2025-07-04 02:14:43,333][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 02:19:09,379][train_inner][INFO] - {"epoch": 51, "update": 50.413, "loss": "5.457", "nll_loss": "0.015", "loss_recon": "0.105", "loss_info_nce": "4.408", "ppl": "1.01", "wps": "1053.2", "ups": "0.36", "wpb": "2961.1", "bsz": "384.2", "num_updates": "12200", "lr": "8.21739e-05", "gnorm": "1.521", "clip": "0", "loss_scale": "4096", "train_wall": "525", "gb_free": "1.5", "wall": "20312"}
[2025-07-04 02:25:24,190][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 51 @ 12342 updates
[2025-07-04 02:25:24,190][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint51.pt
[2025-07-04 02:25:26,335][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint51.pt
[2025-07-04 02:25:27,101][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint51.pt (epoch 51 @ 12342 updates, score None) (writing took 2.907087399999 seconds)
[2025-07-04 02:25:27,101][fairseq_cli.train][INFO] - end of epoch 51 (average epoch stats below)
[2025-07-04 02:25:27,111][train][INFO] - {"epoch": 51, "train_loss": "5.444", "train_nll_loss": "0.015", "train_loss_recon": "0.105", "train_loss_info_nce": "4.395", "train_ppl": "1.01", "train_wps": "1111.1", "train_ups": "0.38", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "12342", "train_lr": "8.18652e-05", "train_gnorm": "1.498", "train_clip": "0", "train_loss_scale": "4096", "train_train_wall": "638", "train_gb_free": "1.5", "train_wall": "20689"}
[2025-07-04 02:25:27,298][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 02:25:27,298][fairseq.trainer][INFO] - begin training epoch 52
[2025-07-04 02:25:27,298][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 02:28:02,482][train_inner][INFO] - {"epoch": 52, "update": 51.24, "loss": "5.432", "nll_loss": "0.015", "loss_recon": "0.104", "loss_info_nce": "4.387", "ppl": "1.01", "wps": "1108.1", "ups": "0.38", "wpb": "2953.7", "bsz": "385.6", "num_updates": "12400", "lr": "8.17391e-05", "gnorm": "1.478", "clip": "0", "loss_scale": "4096", "train_wall": "527", "gb_free": "1.5", "wall": "20845"}
[2025-07-04 02:36:09,392][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 52 @ 12584 updates
[2025-07-04 02:36:09,392][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint52.pt
[2025-07-04 02:36:11,688][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint52.pt
[2025-07-04 02:36:12,436][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint52.pt (epoch 52 @ 12584 updates, score None) (writing took 3.0440362000008463 seconds)
[2025-07-04 02:36:12,436][fairseq_cli.train][INFO] - end of epoch 52 (average epoch stats below)
[2025-07-04 02:36:12,436][train][INFO] - {"epoch": 52, "train_loss": "5.422", "train_nll_loss": "0.015", "train_loss_recon": "0.105", "train_loss_info_nce": "4.376", "train_ppl": "1.01", "train_wps": "1108.8", "train_ups": "0.38", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "12584", "train_lr": "8.13391e-05", "train_gnorm": "1.571", "train_clip": "0", "train_loss_scale": "4096", "train_train_wall": "638", "train_gb_free": "1.5", "train_wall": "21335"}
[2025-07-04 02:36:12,635][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 02:36:12,635][fairseq.trainer][INFO] - begin training epoch 53
[2025-07-04 02:36:12,635][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 02:36:56,866][train_inner][INFO] - {"epoch": 53, "update": 52.066, "loss": "5.421", "nll_loss": "0.015", "loss_recon": "0.105", "loss_info_nce": "4.374", "ppl": "1.01", "wps": "1105.8", "ups": "0.37", "wpb": "2954.6", "bsz": "385.9", "num_updates": "12600", "lr": "8.13043e-05", "gnorm": "1.535", "clip": "0", "loss_scale": "4096", "train_wall": "528", "gb_free": "1.5", "wall": "21379"}
[2025-07-04 02:45:46,935][train_inner][INFO] - {"epoch": 53, "update": 52.893, "loss": "5.395", "nll_loss": "0.015", "loss_recon": "0.104", "loss_info_nce": "4.354", "ppl": "1.01", "wps": "1118.3", "ups": "0.38", "wpb": "2963.9", "bsz": "386.8", "num_updates": "12800", "lr": "8.08696e-05", "gnorm": "1.366", "clip": "0", "loss_scale": "4096", "train_wall": "529", "gb_free": "1.5", "wall": "21909"}
[2025-07-04 02:46:54,490][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 53 @ 12826 updates
[2025-07-04 02:46:54,490][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint53.pt
[2025-07-04 02:46:56,731][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint53.pt
[2025-07-04 02:46:57,522][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint53.pt (epoch 53 @ 12826 updates, score None) (writing took 3.035780499998509 seconds)
[2025-07-04 02:46:57,522][fairseq_cli.train][INFO] - end of epoch 53 (average epoch stats below)
[2025-07-04 02:46:57,537][train][INFO] - {"epoch": 53, "train_loss": "5.396", "train_nll_loss": "0.015", "train_loss_recon": "0.104", "train_loss_info_nce": "4.354", "train_ppl": "1.01", "train_wps": "1109.2", "train_ups": "0.38", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "12826", "train_lr": "8.0813e-05", "train_gnorm": "1.392", "train_clip": "0", "train_loss_scale": "4096", "train_train_wall": "639", "train_gb_free": "1.5", "train_wall": "21980"}
[2025-07-04 02:46:57,703][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 02:46:57,703][fairseq.trainer][INFO] - begin training epoch 54
[2025-07-04 02:46:57,703][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 02:54:40,411][train_inner][INFO] - {"epoch": 54, "update": 53.719, "loss": "5.382", "nll_loss": "0.015", "loss_recon": "0.104", "loss_info_nce": "4.342", "ppl": "1.01", "wps": "1108.4", "ups": "0.37", "wpb": "2956.5", "bsz": "385.1", "num_updates": "13000", "lr": "8.04348e-05", "gnorm": "1.346", "clip": "0", "loss_scale": "4096", "train_wall": "527", "gb_free": "1.5", "wall": "22443"}
[2025-07-04 02:57:39,851][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 54 @ 13068 updates
[2025-07-04 02:57:39,851][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint54.pt
[2025-07-04 02:57:42,123][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint54.pt
[2025-07-04 02:57:43,014][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint54.pt (epoch 54 @ 13068 updates, score None) (writing took 3.1629285999988497 seconds)
[2025-07-04 02:57:43,014][fairseq_cli.train][INFO] - end of epoch 54 (average epoch stats below)
[2025-07-04 02:57:43,030][train][INFO] - {"epoch": 54, "train_loss": "5.381", "train_nll_loss": "0.015", "train_loss_recon": "0.104", "train_loss_info_nce": "4.34", "train_ppl": "1.01", "train_wps": "1108.6", "train_ups": "0.37", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "13068", "train_lr": "8.0287e-05", "train_gnorm": "1.415", "train_clip": "0", "train_loss_scale": "4096", "train_train_wall": "639", "train_gb_free": "1.5", "train_wall": "22625"}
[2025-07-04 02:57:43,301][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 02:57:43,301][fairseq.trainer][INFO] - begin training epoch 55
[2025-07-04 02:57:43,301][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 03:03:34,305][train_inner][INFO] - {"epoch": 55, "update": 54.545, "loss": "5.372", "nll_loss": "0.014", "loss_recon": "0.104", "loss_info_nce": "4.33", "ppl": "1.01", "wps": "1107.2", "ups": "0.37", "wpb": "2955.5", "bsz": "385.6", "num_updates": "13200", "lr": "8e-05", "gnorm": "1.605", "clip": "0", "loss_scale": "4096", "train_wall": "528", "gb_free": "1.5", "wall": "22977"}
[2025-07-04 03:08:24,257][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-04 03:08:54,109][valid][INFO] - {"epoch": 55, "valid_loss": "5.047", "valid_nll_loss": "0.014", "valid_loss_recon": "0.097", "valid_loss_info_nce": "4.077", "valid_ppl": "1.01", "valid_wps": "2759.5", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "13310", "valid_best_loss": "5.047"}
[2025-07-04 03:08:54,109][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 55 @ 13310 updates
[2025-07-04 03:08:54,109][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint55.pt
[2025-07-04 03:08:56,358][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint55.pt
[2025-07-04 03:08:57,871][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint55.pt (epoch 55 @ 13310 updates, score 5.047) (writing took 3.7626247000007425 seconds)
[2025-07-04 03:08:57,871][fairseq_cli.train][INFO] - end of epoch 55 (average epoch stats below)
[2025-07-04 03:08:57,886][train][INFO] - {"epoch": 55, "train_loss": "5.361", "train_nll_loss": "0.014", "train_loss_recon": "0.104", "train_loss_info_nce": "4.323", "train_ppl": "1.01", "train_wps": "1060.3", "train_ups": "0.36", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "13310", "train_lr": "7.97609e-05", "train_gnorm": "1.529", "train_clip": "0", "train_loss_scale": "8192", "train_train_wall": "638", "train_gb_free": "1.6", "train_wall": "23300"}
[2025-07-04 03:08:58,069][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 03:08:58,069][fairseq.trainer][INFO] - begin training epoch 56
[2025-07-04 03:08:58,069][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 03:12:57,638][train_inner][INFO] - {"epoch": 56, "update": 55.372, "loss": "5.349", "nll_loss": "0.014", "loss_recon": "0.103", "loss_info_nce": "4.316", "ppl": "1.01", "wps": "1048.6", "ups": "0.36", "wpb": "2953.7", "bsz": "385.9", "num_updates": "13400", "lr": "7.95652e-05", "gnorm": "1.415", "clip": "0", "loss_scale": "8192", "train_wall": "526", "gb_free": "1.5", "wall": "23540"}
[2025-07-04 03:19:39,632][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 56 @ 13552 updates
[2025-07-04 03:19:39,632][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint56.pt
[2025-07-04 03:19:41,890][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint56.pt
[2025-07-04 03:19:42,639][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint56.pt (epoch 56 @ 13552 updates, score None) (writing took 2.9956739000008383 seconds)
[2025-07-04 03:19:42,639][fairseq_cli.train][INFO] - end of epoch 56 (average epoch stats below)
[2025-07-04 03:19:42,639][train][INFO] - {"epoch": 56, "train_loss": "5.342", "train_nll_loss": "0.014", "train_loss_recon": "0.103", "train_loss_info_nce": "4.308", "train_ppl": "1.01", "train_wps": "1109.8", "train_ups": "0.38", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "13552", "train_lr": "7.92348e-05", "train_gnorm": "1.357", "train_clip": "0", "train_loss_scale": "8192", "train_train_wall": "638", "train_gb_free": "1.5", "train_wall": "23945"}
[2025-07-04 03:19:42,862][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 03:19:42,862][fairseq.trainer][INFO] - begin training epoch 57
[2025-07-04 03:19:42,862][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 03:21:52,383][train_inner][INFO] - {"epoch": 57, "update": 56.198, "loss": "5.337", "nll_loss": "0.014", "loss_recon": "0.103", "loss_info_nce": "4.303", "ppl": "1.01", "wps": "1104", "ups": "0.37", "wpb": "2951.8", "bsz": "386.9", "num_updates": "13600", "lr": "7.91304e-05", "gnorm": "1.337", "clip": "0", "loss_scale": "8192", "train_wall": "529", "gb_free": "1.5", "wall": "24075"}
[2025-07-04 03:30:23,458][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 57 @ 13794 updates
[2025-07-04 03:30:23,458][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint57.pt
[2025-07-04 03:30:25,691][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint57.pt
[2025-07-04 03:30:26,438][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint57.pt (epoch 57 @ 13794 updates, score None) (writing took 2.9793175000013434 seconds)
[2025-07-04 03:30:26,438][fairseq_cli.train][INFO] - end of epoch 57 (average epoch stats below)
[2025-07-04 03:30:26,464][train][INFO] - {"epoch": 57, "train_loss": "5.328", "train_nll_loss": "0.014", "train_loss_recon": "0.103", "train_loss_info_nce": "4.295", "train_ppl": "1.01", "train_wps": "1111.4", "train_ups": "0.38", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "13794", "train_lr": "7.87087e-05", "train_gnorm": "1.501", "train_clip": "0", "train_loss_scale": "8192", "train_train_wall": "638", "train_gb_free": "1.5", "train_wall": "24589"}
[2025-07-04 03:30:26,651][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 03:30:26,661][fairseq.trainer][INFO] - begin training epoch 58
[2025-07-04 03:30:26,661][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 03:30:44,421][train_inner][INFO] - {"epoch": 58, "update": 57.025, "loss": "5.327", "nll_loss": "0.014", "loss_recon": "0.103", "loss_info_nce": "4.295", "ppl": "1.01", "wps": "1112.1", "ups": "0.38", "wpb": "2958.3", "bsz": "384.6", "num_updates": "13800", "lr": "7.86957e-05", "gnorm": "1.562", "clip": "0", "loss_scale": "8192", "train_wall": "526", "gb_free": "1.5", "wall": "24607"}
[2025-07-04 03:39:33,158][train_inner][INFO] - {"epoch": 58, "update": 57.851, "loss": "5.311", "nll_loss": "0.014", "loss_recon": "0.103", "loss_info_nce": "4.281", "ppl": "1.01", "wps": "1121.1", "ups": "0.38", "wpb": "2963.9", "bsz": "386.8", "num_updates": "14000", "lr": "7.82609e-05", "gnorm": "1.213", "clip": "0", "loss_scale": "8192", "train_wall": "527", "gb_free": "1.5", "wall": "25136"}
[2025-07-04 03:41:07,081][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 58 @ 14036 updates
[2025-07-04 03:41:07,081][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint58.pt
[2025-07-04 03:41:09,328][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint58.pt
[2025-07-04 03:41:10,151][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint58.pt (epoch 58 @ 14036 updates, score None) (writing took 3.069797899999685 seconds)
[2025-07-04 03:41:10,151][fairseq_cli.train][INFO] - end of epoch 58 (average epoch stats below)
[2025-07-04 03:41:10,177][train][INFO] - {"epoch": 58, "train_loss": "5.311", "train_nll_loss": "0.014", "train_loss_recon": "0.103", "train_loss_info_nce": "4.281", "train_ppl": "1.01", "train_wps": "1111.6", "train_ups": "0.38", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "14036", "train_lr": "7.81826e-05", "train_gnorm": "1.287", "train_clip": "0", "train_loss_scale": "8192", "train_train_wall": "637", "train_gb_free": "1.5", "train_wall": "25233"}
[2025-07-04 03:41:10,348][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 03:41:10,348][fairseq.trainer][INFO] - begin training epoch 59
[2025-07-04 03:41:10,348][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 03:48:28,191][train_inner][INFO] - {"epoch": 59, "update": 58.678, "loss": "5.3", "nll_loss": "0.014", "loss_recon": "0.103", "loss_info_nce": "4.271", "ppl": "1.01", "wps": "1104.1", "ups": "0.37", "wpb": "2953.7", "bsz": "386", "num_updates": "14200", "lr": "7.78261e-05", "gnorm": "1.506", "clip": "0", "loss_scale": "8192", "train_wall": "528", "gb_free": "1.5", "wall": "25671"}
[2025-07-04 03:51:52,333][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 59 @ 14278 updates
[2025-07-04 03:51:52,333][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint59.pt
[2025-07-04 03:51:54,549][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint59.pt
[2025-07-04 03:51:55,298][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint59.pt (epoch 59 @ 14278 updates, score None) (writing took 2.9576874999984284 seconds)
[2025-07-04 03:51:55,298][fairseq_cli.train][INFO] - end of epoch 59 (average epoch stats below)
[2025-07-04 03:51:55,323][train][INFO] - {"epoch": 59, "train_loss": "5.296", "train_nll_loss": "0.014", "train_loss_recon": "0.103", "train_loss_info_nce": "4.268", "train_ppl": "1.01", "train_wps": "1109.2", "train_ups": "0.38", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "14278", "train_lr": "7.76565e-05", "train_gnorm": "1.4", "train_clip": "0", "train_loss_scale": "8192", "train_train_wall": "638", "train_gb_free": "1.5", "train_wall": "25878"}
[2025-07-04 03:51:55,520][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 03:51:55,520][fairseq.trainer][INFO] - begin training epoch 60
[2025-07-04 03:51:55,520][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 03:57:19,178][train_inner][INFO] - {"epoch": 60, "update": 59.504, "loss": "5.292", "nll_loss": "0.014", "loss_recon": "0.103", "loss_info_nce": "4.264", "ppl": "1.01", "wps": "1113.6", "ups": "0.38", "wpb": "2956.5", "bsz": "385.3", "num_updates": "14400", "lr": "7.73913e-05", "gnorm": "1.25", "clip": "0", "loss_scale": "8192", "train_wall": "525", "gb_free": "1.5", "wall": "26202"}
[2025-07-04 04:02:34,537][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-04 04:03:04,895][valid][INFO] - {"epoch": 60, "valid_loss": "5.018", "valid_nll_loss": "0.014", "valid_loss_recon": "0.099", "valid_loss_info_nce": "4.033", "valid_ppl": "1.01", "valid_wps": "2727.5", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "14520", "valid_best_loss": "5.018"}
[2025-07-04 04:03:04,895][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 60 @ 14520 updates
[2025-07-04 04:03:04,895][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint60.pt
[2025-07-04 04:03:07,075][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint60.pt
[2025-07-04 04:03:08,513][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint60.pt (epoch 60 @ 14520 updates, score 5.018) (writing took 3.624729399998614 seconds)
[2025-07-04 04:03:08,513][fairseq_cli.train][INFO] - end of epoch 60 (average epoch stats below)
[2025-07-04 04:03:08,529][train][INFO] - {"epoch": 60, "train_loss": "5.284", "train_nll_loss": "0.014", "train_loss_recon": "0.103", "train_loss_info_nce": "4.258", "train_ppl": "1.01", "train_wps": "1062.9", "train_ups": "0.36", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "14520", "train_lr": "7.71304e-05", "train_gnorm": "1.169", "train_clip": "0", "train_loss_scale": "8192", "train_train_wall": "636", "train_gb_free": "1.5", "train_wall": "26551"}
[2025-07-04 04:03:08,741][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 04:03:08,741][fairseq.trainer][INFO] - begin training epoch 61
[2025-07-04 04:03:08,741][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 04:06:42,577][train_inner][INFO] - {"epoch": 61, "update": 60.331, "loss": "5.274", "nll_loss": "0.014", "loss_recon": "0.102", "loss_info_nce": "4.253", "ppl": "1.01", "wps": "1048.9", "ups": "0.35", "wpb": "2954.6", "bsz": "385.9", "num_updates": "14600", "lr": "7.69565e-05", "gnorm": "1.204", "clip": "0", "loss_scale": "8192", "train_wall": "526", "gb_free": "1.5", "wall": "26765"}
[2025-07-04 04:13:49,521][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 61 @ 14762 updates
[2025-07-04 04:13:49,521][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint61.pt
[2025-07-04 04:13:51,649][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint61.pt
[2025-07-04 04:13:52,365][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint61.pt (epoch 61 @ 14762 updates, score None) (writing took 2.849894899998617 seconds)
[2025-07-04 04:13:52,365][fairseq_cli.train][INFO] - end of epoch 61 (average epoch stats below)
[2025-07-04 04:13:52,381][train][INFO] - {"epoch": 61, "train_loss": "5.271", "train_nll_loss": "0.014", "train_loss_recon": "0.102", "train_loss_info_nce": "4.247", "train_ppl": "1.01", "train_wps": "1111.4", "train_ups": "0.38", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "14762", "train_lr": "7.66043e-05", "train_gnorm": "1.381", "train_clip": "0", "train_loss_scale": "8192", "train_train_wall": "637", "train_gb_free": "1.5", "train_wall": "27195"}
[2025-07-04 04:13:52,564][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 04:13:52,564][fairseq.trainer][INFO] - begin training epoch 62
[2025-07-04 04:13:52,564][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 04:15:35,453][train_inner][INFO] - {"epoch": 62, "update": 61.157, "loss": "5.269", "nll_loss": "0.014", "loss_recon": "0.102", "loss_info_nce": "4.246", "ppl": "1.01", "wps": "1109.6", "ups": "0.38", "wpb": "2956.5", "bsz": "385.4", "num_updates": "14800", "lr": "7.65217e-05", "gnorm": "1.398", "clip": "0", "loss_scale": "8192", "train_wall": "527", "gb_free": "1.5", "wall": "27298"}
[2025-07-04 04:24:25,207][train_inner][INFO] - {"epoch": 62, "update": 61.983, "loss": "5.256", "nll_loss": "0.014", "loss_recon": "0.102", "loss_info_nce": "4.235", "ppl": "1.01", "wps": "1119", "ups": "0.38", "wpb": "2963.9", "bsz": "386.8", "num_updates": "15000", "lr": "7.6087e-05", "gnorm": "1.292", "clip": "0", "loss_scale": "8192", "train_wall": "528", "gb_free": "1.5", "wall": "27828"}
[2025-07-04 04:24:33,943][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 62 @ 15004 updates
[2025-07-04 04:24:33,943][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint62.pt
[2025-07-04 04:24:36,135][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint62.pt
[2025-07-04 04:24:36,897][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint62.pt (epoch 62 @ 15004 updates, score None) (writing took 2.958444200001395 seconds)
[2025-07-04 04:24:36,897][fairseq_cli.train][INFO] - end of epoch 62 (average epoch stats below)
[2025-07-04 04:24:36,907][train][INFO] - {"epoch": 62, "train_loss": "5.257", "train_nll_loss": "0.014", "train_loss_recon": "0.102", "train_loss_info_nce": "4.236", "train_ppl": "1.01", "train_wps": "1110.2", "train_ups": "0.38", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "15004", "train_lr": "7.60783e-05", "train_gnorm": "1.337", "train_clip": "0", "train_loss_scale": "8192", "train_train_wall": "638", "train_gb_free": "1.5", "train_wall": "27839"}
[2025-07-04 04:24:37,104][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 04:24:37,104][fairseq.trainer][INFO] - begin training epoch 63
[2025-07-04 04:24:37,104][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 04:33:18,008][train_inner][INFO] - {"epoch": 63, "update": 62.81, "loss": "5.248", "nll_loss": "0.014", "loss_recon": "0.102", "loss_info_nce": "4.228", "ppl": "1.01", "wps": "1109.1", "ups": "0.38", "wpb": "2954.6", "bsz": "385.6", "num_updates": "15200", "lr": "7.56522e-05", "gnorm": "1.301", "clip": "0", "loss_scale": "8192", "train_wall": "526", "gb_free": "1.5", "wall": "28360"}
[2025-07-04 04:35:17,906][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 63 @ 15246 updates
[2025-07-04 04:35:17,906][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint63.pt
[2025-07-04 04:35:20,088][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint63.pt
[2025-07-04 04:35:20,786][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint63.pt (epoch 63 @ 15246 updates, score None) (writing took 2.885217200000625 seconds)
[2025-07-04 04:35:20,786][fairseq_cli.train][INFO] - end of epoch 63 (average epoch stats below)
[2025-07-04 04:35:20,853][train][INFO] - {"epoch": 63, "train_loss": "5.246", "train_nll_loss": "0.014", "train_loss_recon": "0.102", "train_loss_info_nce": "4.226", "train_ppl": "1.01", "train_wps": "1111.3", "train_ups": "0.38", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "15246", "train_lr": "7.55522e-05", "train_gnorm": "1.252", "train_clip": "0", "train_loss_scale": "8192", "train_train_wall": "637", "train_gb_free": "1.5", "train_wall": "28483"}
[2025-07-04 04:35:21,075][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 04:35:21,075][fairseq.trainer][INFO] - begin training epoch 64
[2025-07-04 04:35:21,075][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 04:42:11,139][train_inner][INFO] - {"epoch": 64, "update": 63.636, "loss": "5.242", "nll_loss": "0.014", "loss_recon": "0.102", "loss_info_nce": "4.222", "ppl": "1.01", "wps": "1108.8", "ups": "0.38", "wpb": "2955.5", "bsz": "385.6", "num_updates": "15400", "lr": "7.52174e-05", "gnorm": "1.261", "clip": "0", "loss_scale": "16384", "train_wall": "527", "gb_free": "1.5", "wall": "28894"}
[2025-07-04 04:46:01,870][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 64 @ 15488 updates
[2025-07-04 04:46:01,870][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint64.pt
[2025-07-04 04:46:04,157][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint64.pt
[2025-07-04 04:46:05,106][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint64.pt (epoch 64 @ 15488 updates, score None) (writing took 3.2281819000018004 seconds)
[2025-07-04 04:46:05,106][fairseq_cli.train][INFO] - end of epoch 64 (average epoch stats below)
[2025-07-04 04:46:05,122][train][INFO] - {"epoch": 64, "train_loss": "5.236", "train_nll_loss": "0.014", "train_loss_recon": "0.102", "train_loss_info_nce": "4.219", "train_ppl": "1.01", "train_wps": "1110.7", "train_ups": "0.38", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "15488", "train_lr": "7.50261e-05", "train_gnorm": "1.268", "train_clip": "0", "train_loss_scale": "16384", "train_train_wall": "637", "train_gb_free": "1.5", "train_wall": "29127"}
[2025-07-04 04:46:05,367][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 04:46:05,383][fairseq.trainer][INFO] - begin training epoch 65
[2025-07-04 04:46:05,383][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 04:51:02,378][train_inner][INFO] - {"epoch": 65, "update": 64.463, "loss": "5.226", "nll_loss": "0.014", "loss_recon": "0.101", "loss_info_nce": "4.212", "ppl": "1.01", "wps": "1114.1", "ups": "0.38", "wpb": "2959.3", "bsz": "384.4", "num_updates": "15600", "lr": "7.47826e-05", "gnorm": "1.319", "clip": "0", "loss_scale": "16384", "train_wall": "525", "gb_free": "1.5", "wall": "29425"}
[2025-07-04 04:56:45,752][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-04 04:57:15,545][valid][INFO] - {"epoch": 65, "valid_loss": "4.946", "valid_nll_loss": "0.013", "valid_loss_recon": "0.096", "valid_loss_info_nce": "3.988", "valid_ppl": "1.01", "valid_wps": "2777.8", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "15730", "valid_best_loss": "4.946"}
[2025-07-04 04:57:15,545][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 65 @ 15730 updates
[2025-07-04 04:57:15,545][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint65.pt
[2025-07-04 04:57:17,751][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint65.pt
[2025-07-04 04:57:19,215][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint65.pt (epoch 65 @ 15730 updates, score 4.946) (writing took 3.665110999998433 seconds)
[2025-07-04 04:57:19,215][fairseq_cli.train][INFO] - end of epoch 65 (average epoch stats below)
[2025-07-04 04:57:19,215][train][INFO] - {"epoch": 65, "train_loss": "5.225", "train_nll_loss": "0.014", "train_loss_recon": "0.102", "train_loss_info_nce": "4.209", "train_ppl": "1.01", "train_wps": "1061.5", "train_ups": "0.36", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "15730", "train_lr": "7.45e-05", "train_gnorm": "1.293", "train_clip": "0", "train_loss_scale": "16384", "train_train_wall": "637", "train_gb_free": "1.5", "train_wall": "29802"}
[2025-07-04 04:57:19,455][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 04:57:19,455][fairseq.trainer][INFO] - begin training epoch 66
[2025-07-04 04:57:19,455][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 05:00:26,396][train_inner][INFO] - {"epoch": 66, "update": 65.289, "loss": "5.22", "nll_loss": "0.014", "loss_recon": "0.102", "loss_info_nce": "4.205", "ppl": "1.01", "wps": "1047.1", "ups": "0.35", "wpb": "2952.8", "bsz": "386.5", "num_updates": "15800", "lr": "7.43478e-05", "gnorm": "1.173", "clip": "0", "loss_scale": "16384", "train_wall": "527", "gb_free": "1.5", "wall": "29989"}
[2025-07-04 05:08:00,304][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 66 @ 15972 updates
[2025-07-04 05:08:00,304][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint66.pt
[2025-07-04 05:08:02,478][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint66.pt
[2025-07-04 05:08:03,212][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint66.pt (epoch 66 @ 15972 updates, score None) (writing took 2.9107614000022295 seconds)
[2025-07-04 05:08:03,212][fairseq_cli.train][INFO] - end of epoch 66 (average epoch stats below)
[2025-07-04 05:08:03,228][train][INFO] - {"epoch": 66, "train_loss": "5.209", "train_nll_loss": "0.014", "train_loss_recon": "0.101", "train_loss_info_nce": "4.197", "train_ppl": "1.01", "train_wps": "1111.1", "train_ups": "0.38", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "15972", "train_lr": "7.39739e-05", "train_gnorm": "1.139", "train_clip": "0", "train_loss_scale": "16384", "train_train_wall": "638", "train_gb_free": "1.5", "train_wall": "30446"}
[2025-07-04 05:08:03,484][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 05:08:03,484][fairseq.trainer][INFO] - begin training epoch 67
[2025-07-04 05:08:03,484][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 05:09:20,255][train_inner][INFO] - {"epoch": 67, "update": 66.116, "loss": "5.207", "nll_loss": "0.014", "loss_recon": "0.101", "loss_info_nce": "4.193", "ppl": "1.01", "wps": "1106.2", "ups": "0.37", "wpb": "2952.8", "bsz": "386.2", "num_updates": "16000", "lr": "7.3913e-05", "gnorm": "1.136", "clip": "0", "loss_scale": "16384", "train_wall": "528", "gb_free": "1.5", "wall": "30523"}
[2025-07-04 05:18:10,553][train_inner][INFO] - {"epoch": 67, "update": 66.942, "loss": "5.197", "nll_loss": "0.014", "loss_recon": "0.101", "loss_info_nce": "4.189", "ppl": "1.01", "wps": "1118.2", "ups": "0.38", "wpb": "2964.8", "bsz": "386.6", "num_updates": "16200", "lr": "7.34783e-05", "gnorm": "1.219", "clip": "0", "loss_scale": "16384", "train_wall": "529", "gb_free": "1.5", "wall": "31053"}
[2025-07-04 05:18:45,437][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 67 @ 16214 updates
[2025-07-04 05:18:45,453][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint67.pt
[2025-07-04 05:18:47,726][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint67.pt
[2025-07-04 05:18:48,433][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint67.pt (epoch 67 @ 16214 updates, score None) (writing took 2.9871444999989762 seconds)
[2025-07-04 05:18:48,433][fairseq_cli.train][INFO] - end of epoch 67 (average epoch stats below)
[2025-07-04 05:18:48,450][train][INFO] - {"epoch": 67, "train_loss": "5.2", "train_nll_loss": "0.014", "train_loss_recon": "0.101", "train_loss_info_nce": "4.189", "train_ppl": "1.01", "train_wps": "1109", "train_ups": "0.38", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "16214", "train_lr": "7.34478e-05", "train_gnorm": "1.206", "train_clip": "0", "train_loss_scale": "16384", "train_train_wall": "638", "train_gb_free": "1.5", "train_wall": "31091"}
[2025-07-04 05:18:48,672][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 05:18:48,688][fairseq.trainer][INFO] - begin training epoch 68
[2025-07-04 05:18:48,688][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 05:27:02,930][train_inner][INFO] - {"epoch": 68, "update": 67.769, "loss": "5.195", "nll_loss": "0.014", "loss_recon": "0.101", "loss_info_nce": "4.185", "ppl": "1.01", "wps": "1110", "ups": "0.38", "wpb": "2954.6", "bsz": "385.6", "num_updates": "16400", "lr": "7.30435e-05", "gnorm": "1.152", "clip": "0", "loss_scale": "16384", "train_wall": "526", "gb_free": "1.5", "wall": "31585"}
[2025-07-04 05:29:28,594][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 68 @ 16456 updates
[2025-07-04 05:29:28,594][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint68.pt
[2025-07-04 05:29:30,766][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint68.pt
[2025-07-04 05:29:31,515][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint68.pt (epoch 68 @ 16456 updates, score None) (writing took 2.916303699999844 seconds)
[2025-07-04 05:29:31,515][fairseq_cli.train][INFO] - end of epoch 68 (average epoch stats below)
[2025-07-04 05:29:31,515][train][INFO] - {"epoch": 68, "train_loss": "5.193", "train_nll_loss": "0.014", "train_loss_recon": "0.101", "train_loss_info_nce": "4.183", "train_ppl": "1.01", "train_wps": "1112.7", "train_ups": "0.38", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "16456", "train_lr": "7.29217e-05", "train_gnorm": "1.148", "train_clip": "0", "train_loss_scale": "16384", "train_train_wall": "637", "train_gb_free": "1.5", "train_wall": "31734"}
[2025-07-04 05:29:31,728][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 05:29:31,728][fairseq.trainer][INFO] - begin training epoch 69
[2025-07-04 05:29:31,728][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 05:35:55,495][train_inner][INFO] - {"epoch": 69, "update": 68.595, "loss": "5.182", "nll_loss": "0.014", "loss_recon": "0.101", "loss_info_nce": "4.176", "ppl": "1.01", "wps": "1110", "ups": "0.38", "wpb": "2955.5", "bsz": "385.7", "num_updates": "16600", "lr": "7.26087e-05", "gnorm": "1.128", "clip": "0", "loss_scale": "16384", "train_wall": "527", "gb_free": "1.5", "wall": "32118"}
[2025-07-04 05:40:12,335][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 69 @ 16698 updates
[2025-07-04 05:40:12,335][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint69.pt
[2025-07-04 05:40:14,535][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint69.pt
[2025-07-04 05:40:15,299][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint69.pt (epoch 69 @ 16698 updates, score None) (writing took 2.9616769999993267 seconds)
[2025-07-04 05:40:15,299][fairseq_cli.train][INFO] - end of epoch 69 (average epoch stats below)
[2025-07-04 05:40:15,310][train][INFO] - {"epoch": 69, "train_loss": "5.18", "train_nll_loss": "0.014", "train_loss_recon": "0.101", "train_loss_info_nce": "4.173", "train_ppl": "1.01", "train_wps": "1111.5", "train_ups": "0.38", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "16698", "train_lr": "7.23957e-05", "train_gnorm": "1.116", "train_clip": "0", "train_loss_scale": "16384", "train_train_wall": "637", "train_gb_free": "1.5", "train_wall": "32378"}
[2025-07-04 05:40:15,507][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 05:40:15,507][fairseq.trainer][INFO] - begin training epoch 70
[2025-07-04 05:40:15,507][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 05:44:47,228][train_inner][INFO] - {"epoch": 70, "update": 69.421, "loss": "5.177", "nll_loss": "0.014", "loss_recon": "0.101", "loss_info_nce": "4.17", "ppl": "1.01", "wps": "1112", "ups": "0.38", "wpb": "2956.5", "bsz": "385.2", "num_updates": "16800", "lr": "7.21739e-05", "gnorm": "1.122", "clip": "0", "loss_scale": "16384", "train_wall": "525", "gb_free": "1.5", "wall": "32650"}
[2025-07-04 05:50:56,979][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-04 05:51:26,820][valid][INFO] - {"epoch": 70, "valid_loss": "4.936", "valid_nll_loss": "0.013", "valid_loss_recon": "0.098", "valid_loss_info_nce": "3.959", "valid_ppl": "1.01", "valid_wps": "2764.2", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "16940", "valid_best_loss": "4.936"}
[2025-07-04 05:51:26,820][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 70 @ 16940 updates
[2025-07-04 05:51:26,820][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint70.pt
[2025-07-04 05:51:29,055][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint70.pt
[2025-07-04 05:51:30,476][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint70.pt (epoch 70 @ 16940 updates, score 4.936) (writing took 3.6574391999965883 seconds)
[2025-07-04 05:51:30,476][fairseq_cli.train][INFO] - end of epoch 70 (average epoch stats below)
[2025-07-04 05:51:30,492][train][INFO] - {"epoch": 70, "train_loss": "5.17", "train_nll_loss": "0.014", "train_loss_recon": "0.101", "train_loss_info_nce": "4.165", "train_ppl": "1.01", "train_wps": "1059.8", "train_ups": "0.36", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "16940", "train_lr": "7.18696e-05", "train_gnorm": "1.173", "train_clip": "0", "train_loss_scale": "16384", "train_train_wall": "638", "train_gb_free": "1.5", "train_wall": "33053"}
[2025-07-04 05:51:30,690][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 05:51:30,690][fairseq.trainer][INFO] - begin training epoch 71
[2025-07-04 05:51:30,690][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 05:54:11,781][train_inner][INFO] - {"epoch": 71, "update": 70.248, "loss": "5.168", "nll_loss": "0.014", "loss_recon": "0.101", "loss_info_nce": "4.16", "ppl": "1.01", "wps": "1046.1", "ups": "0.35", "wpb": "2952.8", "bsz": "386.2", "num_updates": "17000", "lr": "7.17391e-05", "gnorm": "1.167", "clip": "0", "loss_scale": "16384", "train_wall": "528", "gb_free": "1.5", "wall": "33214"}
[2025-07-04 06:02:23,296][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 71 @ 17182 updates
[2025-07-04 06:02:23,297][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint71.pt
[2025-07-04 06:02:25,442][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint71.pt
[2025-07-04 06:02:26,356][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint71.pt (epoch 71 @ 17182 updates, score None) (writing took 3.0635289999991073 seconds)
[2025-07-04 06:02:26,356][fairseq_cli.train][INFO] - end of epoch 71 (average epoch stats below)
[2025-07-04 06:02:26,388][train][INFO] - {"epoch": 71, "train_loss": "5.163", "train_nll_loss": "0.014", "train_loss_recon": "0.101", "train_loss_info_nce": "4.158", "train_ppl": "1.01", "train_wps": "1091", "train_ups": "0.37", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "17182", "train_lr": "7.13435e-05", "train_gnorm": "1.175", "train_clip": "0", "train_loss_scale": "16384", "train_train_wall": "649", "train_gb_free": "1.5", "train_wall": "33709"}
[2025-07-04 06:02:26,629][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 06:02:26,629][fairseq.trainer][INFO] - begin training epoch 72
[2025-07-04 06:02:26,629][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 06:03:18,175][train_inner][INFO] - {"epoch": 72, "update": 71.074, "loss": "5.161", "nll_loss": "0.014", "loss_recon": "0.1", "loss_info_nce": "4.157", "ppl": "1.01", "wps": "1082.2", "ups": "0.37", "wpb": "2956.5", "bsz": "385.4", "num_updates": "17200", "lr": "7.13043e-05", "gnorm": "1.198", "clip": "0", "loss_scale": "16384", "train_wall": "540", "gb_free": "1.5", "wall": "33761"}
[2025-07-04 06:14:32,513][train_inner][INFO] - {"epoch": 72, "update": 71.901, "loss": "5.151", "nll_loss": "0.014", "loss_recon": "0.1", "loss_info_nce": "4.149", "ppl": "1.01", "wps": "879.1", "ups": "0.3", "wpb": "2963.9", "bsz": "386.8", "num_updates": "17400", "lr": "7.08696e-05", "gnorm": "1.171", "clip": "0", "loss_scale": "32768", "train_wall": "673", "gb_free": "1.5", "wall": "34435"}
[2025-07-04 06:15:46,872][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 72 @ 17424 updates
[2025-07-04 06:15:46,872][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint72.pt
[2025-07-04 06:15:49,151][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint72.pt
[2025-07-04 06:15:49,926][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint72.pt (epoch 72 @ 17424 updates, score None) (writing took 3.066582399995241 seconds)
[2025-07-04 06:15:49,941][fairseq_cli.train][INFO] - end of epoch 72 (average epoch stats below)
[2025-07-04 06:15:49,951][train][INFO] - {"epoch": 72, "train_loss": "5.152", "train_nll_loss": "0.014", "train_loss_recon": "0.1", "train_loss_info_nce": "4.149", "train_ppl": "1.01", "train_wps": "890.5", "train_ups": "0.3", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "17424", "train_lr": "7.08174e-05", "train_gnorm": "1.161", "train_clip": "0", "train_loss_scale": "32768", "train_train_wall": "797", "train_gb_free": "1.5", "train_wall": "34512"}
[2025-07-04 06:15:50,129][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 06:15:50,129][fairseq.trainer][INFO] - begin training epoch 73
[2025-07-04 06:15:50,129][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 06:24:28,659][train_inner][INFO] - {"epoch": 73, "update": 72.727, "loss": "5.147", "nll_loss": "0.014", "loss_recon": "0.1", "loss_info_nce": "4.145", "ppl": "1.01", "wps": "992.2", "ups": "0.34", "wpb": "2957.4", "bsz": "385.3", "num_updates": "17600", "lr": "7.04348e-05", "gnorm": "1.134", "clip": "0", "loss_scale": "32768", "train_wall": "590", "gb_free": "1.5", "wall": "35031"}
[2025-07-04 06:27:25,909][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 73 @ 17666 updates
[2025-07-04 06:27:25,909][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint73.pt
[2025-07-04 06:27:28,190][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint73.pt
[2025-07-04 06:27:29,154][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint73.pt (epoch 73 @ 17666 updates, score None) (writing took 3.244821200001752 seconds)
[2025-07-04 06:27:29,155][fairseq_cli.train][INFO] - end of epoch 73 (average epoch stats below)
[2025-07-04 06:27:29,165][train][INFO] - {"epoch": 73, "train_loss": "5.145", "train_nll_loss": "0.014", "train_loss_recon": "0.1", "train_loss_info_nce": "4.143", "train_ppl": "1.01", "train_wps": "1023.4", "train_ups": "0.35", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "17666", "train_lr": "7.02913e-05", "train_gnorm": "1.152", "train_clip": "0", "train_loss_scale": "32768", "train_train_wall": "692", "train_gb_free": "1.5", "train_wall": "35212"}
[2025-07-04 06:27:29,363][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 06:27:29,363][fairseq.trainer][INFO] - begin training epoch 74
[2025-07-04 06:27:29,363][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 06:33:33,260][train_inner][INFO] - {"epoch": 74, "update": 73.554, "loss": "5.142", "nll_loss": "0.014", "loss_recon": "0.1", "loss_info_nce": "4.138", "ppl": "1.01", "wps": "1085.4", "ups": "0.37", "wpb": "2955.5", "bsz": "385.2", "num_updates": "17800", "lr": "7e-05", "gnorm": "1.143", "clip": "0", "loss_scale": "32768", "train_wall": "538", "gb_free": "1.5", "wall": "35576"}
[2025-07-04 06:38:19,254][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 74 @ 17908 updates
[2025-07-04 06:38:19,264][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint74.pt
[2025-07-04 06:38:21,490][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint74.pt
[2025-07-04 06:38:22,201][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint74.pt (epoch 74 @ 17908 updates, score None) (writing took 2.9453472000022884 seconds)
[2025-07-04 06:38:22,201][fairseq_cli.train][INFO] - end of epoch 74 (average epoch stats below)
[2025-07-04 06:38:22,227][train][INFO] - {"epoch": 74, "train_loss": "5.137", "train_nll_loss": "0.014", "train_loss_recon": "0.1", "train_loss_info_nce": "4.136", "train_ppl": "1.01", "train_wps": "1095.7", "train_ups": "0.37", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "17908", "train_lr": "6.97652e-05", "train_gnorm": "1.074", "train_clip": "0", "train_loss_scale": "32768", "train_train_wall": "646", "train_gb_free": "1.5", "train_wall": "35865"}
[2025-07-04 06:38:22,410][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 06:38:22,410][fairseq.trainer][INFO] - begin training epoch 75
[2025-07-04 06:38:22,410][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 06:42:29,696][train_inner][INFO] - {"epoch": 75, "update": 74.38, "loss": "5.132", "nll_loss": "0.014", "loss_recon": "0.1", "loss_info_nce": "4.134", "ppl": "1.01", "wps": "1101.6", "ups": "0.37", "wpb": "2954.6", "bsz": "386.1", "num_updates": "18000", "lr": "6.95652e-05", "gnorm": "0.97", "clip": "0", "loss_scale": "32768", "train_wall": "530", "gb_free": "1.5", "wall": "36112"}
[2025-07-04 06:49:07,602][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-04 06:49:38,451][valid][INFO] - {"epoch": 75, "valid_loss": "4.882", "valid_nll_loss": "0.013", "valid_loss_recon": "0.095", "valid_loss_info_nce": "3.929", "valid_ppl": "1.01", "valid_wps": "2666.8", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "18150", "valid_best_loss": "4.882"}
[2025-07-04 06:49:38,451][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 75 @ 18150 updates
[2025-07-04 06:49:38,461][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint75.pt
[2025-07-04 06:49:40,655][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint75.pt
[2025-07-04 06:49:43,705][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint75.pt (epoch 75 @ 18150 updates, score 4.882) (writing took 5.248640699996031 seconds)
[2025-07-04 06:49:43,705][fairseq_cli.train][INFO] - end of epoch 75 (average epoch stats below)
[2025-07-04 06:49:43,731][train][INFO] - {"epoch": 75, "train_loss": "5.132", "train_nll_loss": "0.014", "train_loss_recon": "0.1", "train_loss_info_nce": "4.132", "train_ppl": "1.01", "train_wps": "1050", "train_ups": "0.36", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "18150", "train_lr": "6.92391e-05", "train_gnorm": "1.083", "train_clip": "0", "train_loss_scale": "32768", "train_train_wall": "642", "train_gb_free": "1.5", "train_wall": "36546"}
[2025-07-04 06:49:43,954][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 06:49:43,954][fairseq.trainer][INFO] - begin training epoch 76
[2025-07-04 06:49:43,954][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 06:51:59,412][train_inner][INFO] - {"epoch": 76, "update": 75.207, "loss": "5.129", "nll_loss": "0.014", "loss_recon": "0.1", "loss_info_nce": "4.129", "ppl": "1.01", "wps": "1036.6", "ups": "0.35", "wpb": "2952.8", "bsz": "385.7", "num_updates": "18200", "lr": "6.91304e-05", "gnorm": "1.169", "clip": "0", "loss_scale": "32768", "train_wall": "530", "gb_free": "1.5", "wall": "36682"}
[2025-07-04 07:00:29,637][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 76 @ 18392 updates
[2025-07-04 07:00:29,637][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint76.pt
[2025-07-04 07:00:31,911][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint76.pt
[2025-07-04 07:00:32,675][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint76.pt (epoch 76 @ 18392 updates, score None) (writing took 3.0429484999986016 seconds)
[2025-07-04 07:00:32,675][fairseq_cli.train][INFO] - end of epoch 76 (average epoch stats below)
[2025-07-04 07:00:32,691][train][INFO] - {"epoch": 76, "train_loss": "5.122", "train_nll_loss": "0.014", "train_loss_recon": "0.1", "train_loss_info_nce": "4.125", "train_ppl": "1.01", "train_wps": "1102.6", "train_ups": "0.37", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "18392", "train_lr": "6.8713e-05", "train_gnorm": "1.056", "train_clip": "0", "train_loss_scale": "32768", "train_train_wall": "642", "train_gb_free": "1.5", "train_wall": "37195"}
[2025-07-04 07:00:32,905][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 07:00:32,905][fairseq.trainer][INFO] - begin training epoch 77
[2025-07-04 07:00:32,905][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 07:00:56,223][train_inner][INFO] - {"epoch": 77, "update": 76.033, "loss": "5.122", "nll_loss": "0.014", "loss_recon": "0.1", "loss_info_nce": "4.125", "ppl": "1.01", "wps": "1101.9", "ups": "0.37", "wpb": "2957.4", "bsz": "385.3", "num_updates": "18400", "lr": "6.86957e-05", "gnorm": "1.051", "clip": "0", "loss_scale": "32768", "train_wall": "530", "gb_free": "1.5", "wall": "37219"}
[2025-07-04 07:09:50,066][train_inner][INFO] - {"epoch": 77, "update": 76.86, "loss": "5.116", "nll_loss": "0.014", "loss_recon": "0.1", "loss_info_nce": "4.119", "ppl": "1.01", "wps": "1110.4", "ups": "0.37", "wpb": "2963.9", "bsz": "386.8", "num_updates": "18600", "lr": "6.82609e-05", "gnorm": "1.041", "clip": "0", "loss_scale": "32768", "train_wall": "532", "gb_free": "1.5", "wall": "37752"}
[2025-07-04 07:11:18,910][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 77 @ 18634 updates
[2025-07-04 07:11:18,910][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint77.pt
[2025-07-04 07:11:21,081][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint77.pt
[2025-07-04 07:11:21,905][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint77.pt (epoch 77 @ 18634 updates, score None) (writing took 2.990818999998737 seconds)
[2025-07-04 07:11:21,905][fairseq_cli.train][INFO] - end of epoch 77 (average epoch stats below)
[2025-07-04 07:11:21,915][train][INFO] - {"epoch": 77, "train_loss": "5.114", "train_nll_loss": "0.014", "train_loss_recon": "0.1", "train_loss_info_nce": "4.119", "train_ppl": "1.01", "train_wps": "1102.2", "train_ups": "0.37", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "18634", "train_lr": "6.8187e-05", "train_gnorm": "1.027", "train_clip": "0", "train_loss_scale": "32768", "train_train_wall": "643", "train_gb_free": "1.6", "train_wall": "37844"}
[2025-07-04 07:11:22,134][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 07:11:22,134][fairseq.trainer][INFO] - begin training epoch 78
[2025-07-04 07:11:22,134][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 07:19:02,472][train_inner][INFO] - {"epoch": 78, "update": 77.686, "loss": "5.107", "nll_loss": "0.014", "loss_recon": "0.099", "loss_info_nce": "4.114", "ppl": "1.01", "wps": "1069.1", "ups": "0.36", "wpb": "2952.8", "bsz": "386.2", "num_updates": "18800", "lr": "6.78261e-05", "gnorm": "1.149", "clip": "0", "loss_scale": "32768", "train_wall": "546", "gb_free": "1.5", "wall": "38305"}
[2025-07-04 07:22:31,305][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 78 @ 18876 updates
[2025-07-04 07:22:31,305][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint78.pt
[2025-07-04 07:22:33,473][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint78.pt
[2025-07-04 07:22:34,294][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint78.pt (epoch 78 @ 18876 updates, score None) (writing took 2.9969873999943957 seconds)
[2025-07-04 07:22:34,294][fairseq_cli.train][INFO] - end of epoch 78 (average epoch stats below)
[2025-07-04 07:22:34,304][train][INFO] - {"epoch": 78, "train_loss": "5.109", "train_nll_loss": "0.014", "train_loss_recon": "0.099", "train_loss_info_nce": "4.114", "train_ppl": "1.01", "train_wps": "1064.2", "train_ups": "0.36", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "18876", "train_lr": "6.76609e-05", "train_gnorm": "1.21", "train_clip": "0", "train_loss_scale": "32768", "train_train_wall": "665", "train_gb_free": "1.5", "train_wall": "38517"}
[2025-07-04 07:22:34,476][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 07:22:34,476][fairseq.trainer][INFO] - begin training epoch 79
[2025-07-04 07:22:34,476][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 07:28:23,682][train_inner][INFO] - {"epoch": 79, "update": 78.512, "loss": "5.107", "nll_loss": "0.014", "loss_recon": "0.099", "loss_info_nce": "4.112", "ppl": "1.01", "wps": "1054.3", "ups": "0.36", "wpb": "2958.3", "bsz": "385", "num_updates": "19000", "lr": "6.73913e-05", "gnorm": "1.018", "clip": "0", "loss_scale": "32768", "train_wall": "555", "gb_free": "1.5", "wall": "38866"}
[2025-07-04 07:33:38,108][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 79 @ 19118 updates
[2025-07-04 07:33:38,108][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint79.pt
[2025-07-04 07:33:40,332][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint79.pt
[2025-07-04 07:33:41,206][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint79.pt (epoch 79 @ 19118 updates, score None) (writing took 3.091189200000372 seconds)
[2025-07-04 07:33:41,206][fairseq_cli.train][INFO] - end of epoch 79 (average epoch stats below)
[2025-07-04 07:33:41,206][train][INFO] - {"epoch": 79, "train_loss": "5.102", "train_nll_loss": "0.014", "train_loss_recon": "0.099", "train_loss_info_nce": "4.108", "train_ppl": "1.01", "train_wps": "1072.9", "train_ups": "0.36", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "19118", "train_lr": "6.71348e-05", "train_gnorm": "0.942", "train_clip": "0", "train_loss_scale": "32768", "train_train_wall": "660", "train_gb_free": "1.5", "train_wall": "39184"}
[2025-07-04 07:33:41,394][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 07:33:41,404][fairseq.trainer][INFO] - begin training epoch 80
[2025-07-04 07:33:41,404][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 07:37:34,748][train_inner][INFO] - {"epoch": 80, "update": 79.339, "loss": "5.098", "nll_loss": "0.014", "loss_recon": "0.099", "loss_info_nce": "4.105", "ppl": "1.01", "wps": "1071.7", "ups": "0.36", "wpb": "2952.8", "bsz": "386.5", "num_updates": "19200", "lr": "6.69565e-05", "gnorm": "1.003", "clip": "0", "loss_scale": "32768", "train_wall": "545", "gb_free": "1.5", "wall": "39417"}
[2025-07-04 07:44:52,638][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-04 07:45:22,208][valid][INFO] - {"epoch": 80, "valid_loss": "4.852", "valid_nll_loss": "0.013", "valid_loss_recon": "0.094", "valid_loss_info_nce": "3.915", "valid_ppl": "1.01", "valid_wps": "2797.1", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "19360", "valid_best_loss": "4.852"}
[2025-07-04 07:45:22,208][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 80 @ 19360 updates
[2025-07-04 07:45:22,208][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint80.pt
[2025-07-04 07:45:24,493][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint80.pt
[2025-07-04 07:45:26,088][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint80.pt (epoch 80 @ 19360 updates, score 4.852) (writing took 3.8747832999943057 seconds)
[2025-07-04 07:45:26,088][fairseq_cli.train][INFO] - end of epoch 80 (average epoch stats below)
[2025-07-04 07:45:26,114][train][INFO] - {"epoch": 80, "train_loss": "5.095", "train_nll_loss": "0.014", "train_loss_recon": "0.099", "train_loss_info_nce": "4.103", "train_ppl": "1.01", "train_wps": "1015.1", "train_ups": "0.34", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "19360", "train_lr": "6.66087e-05", "train_gnorm": "1.004", "train_clip": "0", "train_loss_scale": "32768", "train_train_wall": "668", "train_gb_free": "1.5", "train_wall": "39888"}
[2025-07-04 07:45:26,301][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 07:45:26,301][fairseq.trainer][INFO] - begin training epoch 81
[2025-07-04 07:45:26,301][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 07:47:16,813][train_inner][INFO] - {"epoch": 81, "update": 80.165, "loss": "5.091", "nll_loss": "0.014", "loss_recon": "0.099", "loss_info_nce": "4.103", "ppl": "1.01", "wps": "1016.5", "ups": "0.34", "wpb": "2958.3", "bsz": "384.5", "num_updates": "19400", "lr": "6.65217e-05", "gnorm": "1.017", "clip": "0", "loss_scale": "32768", "train_wall": "545", "gb_free": "1.5", "wall": "39999"}
[2025-07-04 07:56:21,771][train_inner][INFO] - {"epoch": 81, "update": 80.992, "loss": "5.089", "nll_loss": "0.014", "loss_recon": "0.099", "loss_info_nce": "4.098", "ppl": "1.01", "wps": "1087.4", "ups": "0.37", "wpb": "2963", "bsz": "387.1", "num_updates": "19600", "lr": "6.6087e-05", "gnorm": "1.082", "clip": "0", "loss_scale": "65536", "train_wall": "544", "gb_free": "1.5", "wall": "40544"}
[2025-07-04 07:56:25,412][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 81 @ 19602 updates
[2025-07-04 07:56:25,412][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint81.pt
[2025-07-04 07:56:27,657][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint81.pt
[2025-07-04 07:56:28,439][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint81.pt (epoch 81 @ 19602 updates, score None) (writing took 3.0174762999959057 seconds)
[2025-07-04 07:56:28,439][fairseq_cli.train][INFO] - end of epoch 81 (average epoch stats below)
[2025-07-04 07:56:28,439][train][INFO] - {"epoch": 81, "train_loss": "5.089", "train_nll_loss": "0.014", "train_loss_recon": "0.099", "train_loss_info_nce": "4.099", "train_ppl": "1.01", "train_wps": "1080.4", "train_ups": "0.37", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "19602", "train_lr": "6.60826e-05", "train_gnorm": "1.074", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "656", "train_gb_free": "1.5", "train_wall": "40551"}
[2025-07-04 07:56:28,679][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 07:56:28,695][fairseq.trainer][INFO] - begin training epoch 82
[2025-07-04 07:56:28,695][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 08:05:28,403][train_inner][INFO] - {"epoch": 82, "update": 81.818, "loss": "5.084", "nll_loss": "0.014", "loss_recon": "0.099", "loss_info_nce": "4.093", "ppl": "1.01", "wps": "1080.7", "ups": "0.37", "wpb": "2953.7", "bsz": "385.9", "num_updates": "19800", "lr": "6.56522e-05", "gnorm": "0.981", "clip": "0", "loss_scale": "65536", "train_wall": "541", "gb_free": "1.5", "wall": "41091"}
[2025-07-04 08:07:25,810][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 82 @ 19844 updates
[2025-07-04 08:07:25,810][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint82.pt
[2025-07-04 08:07:34,442][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint82.pt
[2025-07-04 08:07:35,214][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint82.pt (epoch 82 @ 19844 updates, score None) (writing took 9.407522100002097 seconds)
[2025-07-04 08:07:35,214][fairseq_cli.train][INFO] - end of epoch 82 (average epoch stats below)
[2025-07-04 08:07:35,236][train][INFO] - {"epoch": 82, "train_loss": "5.082", "train_nll_loss": "0.014", "train_loss_recon": "0.099", "train_loss_info_nce": "4.093", "train_ppl": "1.01", "train_wps": "1073.1", "train_ups": "0.36", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "19844", "train_lr": "6.55565e-05", "train_gnorm": "0.968", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "654", "train_gb_free": "1.5", "train_wall": "41218"}
[2025-07-04 08:07:35,435][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 08:07:35,435][fairseq.trainer][INFO] - begin training epoch 83
[2025-07-04 08:07:35,435][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 08:14:43,732][train_inner][INFO] - {"epoch": 83, "update": 82.645, "loss": "5.078", "nll_loss": "0.014", "loss_recon": "0.099", "loss_info_nce": "4.092", "ppl": "1.01", "wps": "1065.1", "ups": "0.36", "wpb": "2957.4", "bsz": "385.3", "num_updates": "20000", "lr": "6.52174e-05", "gnorm": "1.029", "clip": "0", "loss_scale": "65536", "train_wall": "543", "gb_free": "1.5", "wall": "41646"}
[2025-07-04 08:14:43,732][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-04 08:15:15,477][valid][INFO] - {"epoch": 83, "valid_loss": "4.799", "valid_nll_loss": "0.013", "valid_loss_recon": "0.089", "valid_loss_info_nce": "3.908", "valid_ppl": "1.01", "valid_wps": "2587.4", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "20000", "valid_best_loss": "4.799"}
[2025-07-04 08:19:38,290][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 83 @ 20086 updates
[2025-07-04 08:19:38,290][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint83.pt
[2025-07-04 08:19:40,616][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint83.pt
[2025-07-04 08:19:41,495][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint83.pt (epoch 83 @ 20086 updates, score None) (writing took 3.1964789999983623 seconds)
[2025-07-04 08:19:41,496][fairseq_cli.train][INFO] - end of epoch 83 (average epoch stats below)
[2025-07-04 08:19:41,518][train][INFO] - {"epoch": 83, "train_loss": "5.079", "train_nll_loss": "0.014", "train_loss_recon": "0.099", "train_loss_info_nce": "4.091", "train_ppl": "1.01", "train_wps": "985.2", "train_ups": "0.33", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "20086", "train_lr": "6.50304e-05", "train_gnorm": "1.061", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "688", "train_gb_free": "1.5", "train_wall": "41944"}
[2025-07-04 08:19:41,700][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 08:19:41,704][fairseq.trainer][INFO] - begin training epoch 84
[2025-07-04 08:19:41,704][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 08:25:17,179][train_inner][INFO] - {"epoch": 84, "update": 83.471, "loss": "5.078", "nll_loss": "0.014", "loss_recon": "0.099", "loss_info_nce": "4.089", "ppl": "1.01", "wps": "932.9", "ups": "0.32", "wpb": "2954.6", "bsz": "386", "num_updates": "20200", "lr": "6.47826e-05", "gnorm": "1.041", "clip": "0", "loss_scale": "65536", "train_wall": "594", "gb_free": "1.5", "wall": "42280"}
[2025-07-04 08:30:59,560][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 84 @ 20328 updates
[2025-07-04 08:30:59,560][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint84.pt
[2025-07-04 08:31:01,739][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint84.pt
[2025-07-04 08:31:02,513][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint84.pt (epoch 84 @ 20328 updates, score None) (writing took 2.95133419999911 seconds)
[2025-07-04 08:31:02,513][fairseq_cli.train][INFO] - end of epoch 84 (average epoch stats below)
[2025-07-04 08:31:02,528][train][INFO] - {"epoch": 84, "train_loss": "5.074", "train_nll_loss": "0.014", "train_loss_recon": "0.099", "train_loss_info_nce": "4.087", "train_ppl": "1.01", "train_wps": "1050.7", "train_ups": "0.36", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "20328", "train_lr": "6.45043e-05", "train_gnorm": "1.01", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "673", "train_gb_free": "1.5", "train_wall": "42625"}
[2025-07-04 08:31:02,784][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 08:31:02,784][fairseq.trainer][INFO] - begin training epoch 85
[2025-07-04 08:31:02,784][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 08:34:17,762][train_inner][INFO] - {"epoch": 85, "update": 84.298, "loss": "5.069", "nll_loss": "0.014", "loss_recon": "0.099", "loss_info_nce": "4.084", "ppl": "1.01", "wps": "1093.5", "ups": "0.37", "wpb": "2955.5", "bsz": "385.4", "num_updates": "20400", "lr": "6.43478e-05", "gnorm": "0.998", "clip": "0", "loss_scale": "65536", "train_wall": "534", "gb_free": "1.5", "wall": "42820"}
[2025-07-04 08:42:58,768][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-04 08:43:53,173][valid][INFO] - {"epoch": 85, "valid_loss": "4.818", "valid_nll_loss": "0.013", "valid_loss_recon": "0.092", "valid_loss_info_nce": "3.898", "valid_ppl": "1.01", "valid_wps": "1510", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "20570", "valid_best_loss": "4.818"}
[2025-07-04 08:43:53,173][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 85 @ 20570 updates
[2025-07-04 08:43:53,173][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint85.pt
[2025-07-04 08:43:55,528][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint85.pt
[2025-07-04 08:43:58,555][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint85.pt (epoch 85 @ 20570 updates, score 4.818) (writing took 5.373638400000345 seconds)
[2025-07-04 08:43:58,555][fairseq_cli.train][INFO] - end of epoch 85 (average epoch stats below)
[2025-07-04 08:43:58,581][train][INFO] - {"epoch": 85, "train_loss": "5.069", "train_nll_loss": "0.014", "train_loss_recon": "0.099", "train_loss_info_nce": "4.083", "train_ppl": "1.01", "train_wps": "922.1", "train_ups": "0.31", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "20570", "train_lr": "6.39783e-05", "train_gnorm": "1.062", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "712", "train_gb_free": "1.5", "train_wall": "43401"}
[2025-07-04 08:43:58,964][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 08:43:58,964][fairseq.trainer][INFO] - begin training epoch 86
[2025-07-04 08:43:58,974][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 08:45:48,906][train_inner][INFO] - {"epoch": 86, "update": 85.124, "loss": "5.07", "nll_loss": "0.014", "loss_recon": "0.099", "loss_info_nce": "4.082", "ppl": "1.01", "wps": "854.7", "ups": "0.29", "wpb": "2953.7", "bsz": "385.9", "num_updates": "20600", "lr": "6.3913e-05", "gnorm": "1.085", "clip": "0", "loss_scale": "65536", "train_wall": "626", "gb_free": "1.5", "wall": "43511"}
[2025-07-04 08:56:17,656][train_inner][INFO] - {"epoch": 86, "update": 85.95, "loss": "5.063", "nll_loss": "0.014", "loss_recon": "0.099", "loss_info_nce": "4.078", "ppl": "1.01", "wps": "943.4", "ups": "0.32", "wpb": "2965.8", "bsz": "386.3", "num_updates": "20800", "lr": "6.34783e-05", "gnorm": "1.034", "clip": "0", "loss_scale": "65536", "train_wall": "628", "gb_free": "1.5", "wall": "44140"}
[2025-07-04 08:56:49,079][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 86 @ 20812 updates
[2025-07-04 08:56:49,081][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint86.pt
[2025-07-04 08:56:51,350][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint86.pt
[2025-07-04 08:56:52,239][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint86.pt (epoch 86 @ 20812 updates, score None) (writing took 3.1621454000051017 seconds)
[2025-07-04 08:56:52,239][fairseq_cli.train][INFO] - end of epoch 86 (average epoch stats below)
[2025-07-04 08:56:52,239][train][INFO] - {"epoch": 86, "train_loss": "5.063", "train_nll_loss": "0.014", "train_loss_recon": "0.098", "train_loss_info_nce": "4.078", "train_ppl": "1.01", "train_wps": "924.9", "train_ups": "0.31", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "20812", "train_lr": "6.34522e-05", "train_gnorm": "1.033", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "765", "train_gb_free": "1.5", "train_wall": "44175"}
[2025-07-04 08:56:52,440][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 08:56:52,455][fairseq.trainer][INFO] - begin training epoch 87
[2025-07-04 08:56:52,456][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 09:05:35,634][train_inner][INFO] - {"epoch": 87, "update": 86.777, "loss": "5.061", "nll_loss": "0.014", "loss_recon": "0.099", "loss_info_nce": "4.075", "ppl": "1.01", "wps": "1059", "ups": "0.36", "wpb": "2954.6", "bsz": "385.6", "num_updates": "21000", "lr": "6.30435e-05", "gnorm": "0.981", "clip": "0", "loss_scale": "65536", "train_wall": "552", "gb_free": "1.5", "wall": "44698"}
[2025-07-04 09:08:00,452][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 87 @ 21054 updates
[2025-07-04 09:08:00,452][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint87.pt
[2025-07-04 09:08:02,688][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint87.pt
[2025-07-04 09:08:03,462][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint87.pt (epoch 87 @ 21054 updates, score None) (writing took 3.011435000000347 seconds)
[2025-07-04 09:08:03,462][fairseq_cli.train][INFO] - end of epoch 87 (average epoch stats below)
[2025-07-04 09:08:03,477][train][INFO] - {"epoch": 87, "train_loss": "5.059", "train_nll_loss": "0.014", "train_loss_recon": "0.098", "train_loss_info_nce": "4.075", "train_ppl": "1.01", "train_wps": "1066", "train_ups": "0.36", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "21054", "train_lr": "6.29261e-05", "train_gnorm": "0.971", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "665", "train_gb_free": "1.5", "train_wall": "44846"}
[2025-07-04 09:08:03,675][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 09:08:03,675][fairseq.trainer][INFO] - begin training epoch 88
[2025-07-04 09:08:03,675][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 09:14:45,025][train_inner][INFO] - {"epoch": 88, "update": 87.603, "loss": "5.055", "nll_loss": "0.014", "loss_recon": "0.098", "loss_info_nce": "4.073", "ppl": "1.01", "wps": "1076.6", "ups": "0.36", "wpb": "2957.4", "bsz": "385.6", "num_updates": "21200", "lr": "6.26087e-05", "gnorm": "0.997", "clip": "0", "loss_scale": "65536", "train_wall": "543", "gb_free": "1.5", "wall": "45247"}
[2025-07-04 09:19:40,816][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 88 @ 21296 updates
[2025-07-04 09:19:40,832][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint88.pt
[2025-07-04 09:19:43,147][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint88.pt
[2025-07-04 09:19:44,435][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint88.pt (epoch 88 @ 21296 updates, score None) (writing took 3.605651600002602 seconds)
[2025-07-04 09:19:44,435][fairseq_cli.train][INFO] - end of epoch 88 (average epoch stats below)
[2025-07-04 09:19:44,435][train][INFO] - {"epoch": 88, "train_loss": "5.054", "train_nll_loss": "0.014", "train_loss_recon": "0.098", "train_loss_info_nce": "4.071", "train_ppl": "1.01", "train_wps": "1020.8", "train_ups": "0.35", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "21296", "train_lr": "6.24e-05", "train_gnorm": "0.987", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "693", "train_gb_free": "1.5", "train_wall": "45547"}
[2025-07-04 09:19:44,829][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 09:19:44,829][fairseq.trainer][INFO] - begin training epoch 89
[2025-07-04 09:19:44,829][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 09:24:53,955][train_inner][INFO] - {"epoch": 89, "update": 88.43, "loss": "5.05", "nll_loss": "0.014", "loss_recon": "0.098", "loss_info_nce": "4.069", "ppl": "1.01", "wps": "969.8", "ups": "0.33", "wpb": "2952.8", "bsz": "385.9", "num_updates": "21400", "lr": "6.21739e-05", "gnorm": "0.959", "clip": "0", "loss_scale": "65536", "train_wall": "600", "gb_free": "1.5", "wall": "45856"}
[2025-07-04 09:31:57,528][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 65536.0
[2025-07-04 09:32:47,875][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 89 @ 21537 updates
[2025-07-04 09:32:47,875][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint89.pt
[2025-07-04 09:32:50,130][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint89.pt
[2025-07-04 09:32:50,884][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint89.pt (epoch 89 @ 21537 updates, score None) (writing took 3.012483599995903 seconds)
[2025-07-04 09:32:50,884][fairseq_cli.train][INFO] - end of epoch 89 (average epoch stats below)
[2025-07-04 09:32:50,900][train][INFO] - {"epoch": 89, "train_loss": "5.05", "train_nll_loss": "0.014", "train_loss_recon": "0.098", "train_loss_info_nce": "4.068", "train_ppl": "1.01", "train_wps": "906.1", "train_ups": "0.31", "train_wpb": "2956.7", "train_bsz": "385.8", "train_num_updates": "21537", "train_lr": "6.18761e-05", "train_gnorm": "0.985", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "777", "train_gb_free": "1.5", "train_wall": "46333"}
[2025-07-04 09:32:51,125][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 09:32:51,125][fairseq.trainer][INFO] - begin training epoch 90
[2025-07-04 09:32:51,125][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 09:35:47,453][train_inner][INFO] - {"epoch": 90, "update": 89.26, "loss": "5.048", "nll_loss": "0.014", "loss_recon": "0.098", "loss_info_nce": "4.066", "ppl": "1.01", "wps": "904.5", "ups": "0.31", "wpb": "2955.5", "bsz": "385.6", "num_updates": "21600", "lr": "6.17391e-05", "gnorm": "1.012", "clip": "0", "loss_scale": "65536", "train_wall": "645", "gb_free": "1.5", "wall": "46510"}
[2025-07-04 09:45:32,086][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-04 09:46:03,966][valid][INFO] - {"epoch": 90, "valid_loss": "4.795", "valid_nll_loss": "0.013", "valid_loss_recon": "0.091", "valid_loss_info_nce": "3.882", "valid_ppl": "1.01", "valid_wps": "2584.9", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "21779", "valid_best_loss": "4.795"}
[2025-07-04 09:46:03,966][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 90 @ 21779 updates
[2025-07-04 09:46:03,966][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint90.pt
[2025-07-04 09:46:06,168][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint90.pt
[2025-07-04 09:46:08,096][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint90.pt (epoch 90 @ 21779 updates, score 4.795) (writing took 4.130464399997436 seconds)
[2025-07-04 09:46:08,096][fairseq_cli.train][INFO] - end of epoch 90 (average epoch stats below)
[2025-07-04 09:46:08,121][train][INFO] - {"epoch": 90, "train_loss": "5.045", "train_nll_loss": "0.014", "train_loss_recon": "0.098", "train_loss_info_nce": "4.064", "train_ppl": "1.01", "train_wps": "897.6", "train_ups": "0.3", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "21779", "train_lr": "6.135e-05", "train_gnorm": "0.978", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "756", "train_gb_free": "1.5", "train_wall": "47130"}
[2025-07-04 09:46:08,351][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 09:46:08,351][fairseq.trainer][INFO] - begin training epoch 91
[2025-07-04 09:46:08,351][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 09:47:10,406][train_inner][INFO] - {"epoch": 91, "update": 90.087, "loss": "5.046", "nll_loss": "0.014", "loss_recon": "0.098", "loss_info_nce": "4.064", "ppl": "1.01", "wps": "865.8", "ups": "0.29", "wpb": "2956.5", "bsz": "385.1", "num_updates": "21800", "lr": "6.13043e-05", "gnorm": "0.93", "clip": "0", "loss_scale": "65536", "train_wall": "643", "gb_free": "1.5", "wall": "47193"}
[2025-07-04 09:57:32,619][train_inner][INFO] - {"epoch": 91, "update": 90.913, "loss": "5.041", "nll_loss": "0.014", "loss_recon": "0.098", "loss_info_nce": "4.06", "ppl": "1.01", "wps": "952.4", "ups": "0.32", "wpb": "2963", "bsz": "387.4", "num_updates": "22000", "lr": "6.08696e-05", "gnorm": "0.928", "clip": "0", "loss_scale": "65536", "train_wall": "620", "gb_free": "1.5", "wall": "47815"}
[2025-07-04 09:58:41,219][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 91 @ 22021 updates
[2025-07-04 09:58:41,219][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint91.pt
[2025-07-04 09:58:43,486][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint91.pt
[2025-07-04 09:58:44,305][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint91.pt (epoch 91 @ 22021 updates, score None) (writing took 3.09332820000418 seconds)
[2025-07-04 09:58:44,305][fairseq_cli.train][INFO] - end of epoch 91 (average epoch stats below)
[2025-07-04 09:58:44,321][train][INFO] - {"epoch": 91, "train_loss": "5.041", "train_nll_loss": "0.014", "train_loss_recon": "0.098", "train_loss_info_nce": "4.061", "train_ppl": "1.01", "train_wps": "946.2", "train_ups": "0.32", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "22021", "train_lr": "6.08239e-05", "train_gnorm": "0.937", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "749", "train_gb_free": "1.5", "train_wall": "47887"}
[2025-07-04 09:58:44,519][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 09:58:44,519][fairseq.trainer][INFO] - begin training epoch 92
[2025-07-04 09:58:44,519][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 10:07:54,586][train_inner][INFO] - {"epoch": 92, "update": 91.74, "loss": "5.035", "nll_loss": "0.014", "loss_recon": "0.098", "loss_info_nce": "4.057", "ppl": "1.01", "wps": "950.1", "ups": "0.32", "wpb": "2954.6", "bsz": "385.8", "num_updates": "22200", "lr": "6.04348e-05", "gnorm": "0.983", "clip": "0", "loss_scale": "65536", "train_wall": "615", "gb_free": "1.5", "wall": "48437"}
[2025-07-04 10:11:02,073][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 92 @ 22263 updates
[2025-07-04 10:11:02,073][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint92.pt
[2025-07-04 10:11:04,354][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint92.pt
[2025-07-04 10:11:05,070][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint92.pt (epoch 92 @ 22263 updates, score None) (writing took 3.004401999998663 seconds)
[2025-07-04 10:11:05,070][fairseq_cli.train][INFO] - end of epoch 92 (average epoch stats below)
[2025-07-04 10:11:05,085][train][INFO] - {"epoch": 92, "train_loss": "5.036", "train_nll_loss": "0.014", "train_loss_recon": "0.098", "train_loss_info_nce": "4.056", "train_ppl": "1.01", "train_wps": "966", "train_ups": "0.33", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "22263", "train_lr": "6.02978e-05", "train_gnorm": "0.924", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "734", "train_gb_free": "1.5", "train_wall": "48627"}
[2025-07-04 10:11:05,281][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 10:11:05,281][fairseq.trainer][INFO] - begin training epoch 93
[2025-07-04 10:11:05,281][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 10:17:38,733][train_inner][INFO] - {"epoch": 93, "update": 92.566, "loss": "5.033", "nll_loss": "0.014", "loss_recon": "0.098", "loss_info_nce": "4.054", "ppl": "1.01", "wps": "1012.2", "ups": "0.34", "wpb": "2956.5", "bsz": "385.4", "num_updates": "22400", "lr": "6e-05", "gnorm": "0.903", "clip": "0", "loss_scale": "65536", "train_wall": "578", "gb_free": "1.5", "wall": "49021"}
[2025-07-04 10:22:42,652][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 93 @ 22505 updates
[2025-07-04 10:22:42,652][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint93.pt
[2025-07-04 10:22:44,852][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint93.pt
[2025-07-04 10:22:45,632][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint93.pt (epoch 93 @ 22505 updates, score None) (writing took 2.974469400003727 seconds)
[2025-07-04 10:22:45,632][fairseq_cli.train][INFO] - end of epoch 93 (average epoch stats below)
[2025-07-04 10:22:45,648][train][INFO] - {"epoch": 93, "train_loss": "5.031", "train_nll_loss": "0.014", "train_loss_recon": "0.098", "train_loss_info_nce": "4.053", "train_ppl": "1.01", "train_wps": "1021.4", "train_ups": "0.35", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "22505", "train_lr": "5.97717e-05", "train_gnorm": "0.962", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "694", "train_gb_free": "1.5", "train_wall": "49328"}
[2025-07-04 10:22:45,848][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 10:22:45,848][fairseq.trainer][INFO] - begin training epoch 94
[2025-07-04 10:22:45,848][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 10:28:02,107][train_inner][INFO] - {"epoch": 94, "update": 93.393, "loss": "5.033", "nll_loss": "0.014", "loss_recon": "0.098", "loss_info_nce": "4.055", "ppl": "1.01", "wps": "949.1", "ups": "0.32", "wpb": "2958.3", "bsz": "384", "num_updates": "22600", "lr": "5.95652e-05", "gnorm": "1.049", "clip": "0", "loss_scale": "65536", "train_wall": "616", "gb_free": "1.5", "wall": "49644"}
[2025-07-04 10:36:09,360][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 94 @ 22747 updates
[2025-07-04 10:36:09,360][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint94.pt
[2025-07-04 10:36:11,734][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint94.pt
[2025-07-04 10:36:13,263][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint94.pt (epoch 94 @ 22747 updates, score None) (writing took 3.889289800004917 seconds)
[2025-07-04 10:36:13,264][fairseq_cli.train][INFO] - end of epoch 94 (average epoch stats below)
[2025-07-04 10:36:13,271][train][INFO] - {"epoch": 94, "train_loss": "5.03", "train_nll_loss": "0.014", "train_loss_recon": "0.098", "train_loss_info_nce": "4.052", "train_ppl": "1.01", "train_wps": "886", "train_ups": "0.3", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "22747", "train_lr": "5.92457e-05", "train_gnorm": "1.02", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "798", "train_gb_free": "1.5", "train_wall": "50136"}
[2025-07-04 10:36:13,636][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 10:36:13,636][fairseq.trainer][INFO] - begin training epoch 95
[2025-07-04 10:36:13,636][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 10:39:25,887][train_inner][INFO] - {"epoch": 95, "update": 94.219, "loss": "5.026", "nll_loss": "0.014", "loss_recon": "0.098", "loss_info_nce": "4.048", "ppl": "1.01", "wps": "863.1", "ups": "0.29", "wpb": "2950.9", "bsz": "387.2", "num_updates": "22800", "lr": "5.91304e-05", "gnorm": "0.932", "clip": "0", "loss_scale": "65536", "train_wall": "673", "gb_free": "1.5", "wall": "50328"}
[2025-07-04 10:49:32,353][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-04 10:50:25,905][valid][INFO] - {"epoch": 95, "valid_loss": "4.806", "valid_nll_loss": "0.013", "valid_loss_recon": "0.094", "valid_loss_info_nce": "3.87", "valid_ppl": "1.01", "valid_wps": "1533.2", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "22989", "valid_best_loss": "4.795"}
[2025-07-04 10:50:25,915][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 95 @ 22989 updates
[2025-07-04 10:50:25,915][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint95.pt
[2025-07-04 10:50:28,371][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint95.pt
[2025-07-04 10:50:29,715][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint95.pt (epoch 95 @ 22989 updates, score 4.806) (writing took 3.8079681000017445 seconds)
[2025-07-04 10:50:29,715][fairseq_cli.train][INFO] - end of epoch 95 (average epoch stats below)
[2025-07-04 10:50:29,731][train][INFO] - {"epoch": 95, "train_loss": "5.022", "train_nll_loss": "0.014", "train_loss_recon": "0.098", "train_loss_info_nce": "4.046", "train_ppl": "1.01", "train_wps": "835.5", "train_ups": "0.28", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "22989", "train_lr": "5.87196e-05", "train_gnorm": "0.899", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "792", "train_gb_free": "1.5", "train_wall": "50992"}
[2025-07-04 10:50:30,110][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 10:50:30,126][fairseq.trainer][INFO] - begin training epoch 96
[2025-07-04 10:50:30,126][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 10:51:09,555][train_inner][INFO] - {"epoch": 96, "update": 95.045, "loss": "5.022", "nll_loss": "0.014", "loss_recon": "0.098", "loss_info_nce": "4.046", "ppl": "1.01", "wps": "840.3", "ups": "0.28", "wpb": "2956.5", "bsz": "385.1", "num_updates": "23000", "lr": "5.86957e-05", "gnorm": "0.934", "clip": "0", "loss_scale": "65536", "train_wall": "640", "gb_free": "1.5", "wall": "51032"}
[2025-07-04 11:01:43,457][train_inner][INFO] - {"epoch": 96, "update": 95.872, "loss": "5.021", "nll_loss": "0.014", "loss_recon": "0.098", "loss_info_nce": "4.045", "ppl": "1.01", "wps": "935.1", "ups": "0.32", "wpb": "2963.9", "bsz": "386.8", "num_updates": "23200", "lr": "5.82609e-05", "gnorm": "0.949", "clip": "0", "loss_scale": "65536", "train_wall": "631", "gb_free": "1.5", "wall": "51666"}
[2025-07-04 11:03:19,878][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 96 @ 23231 updates
[2025-07-04 11:03:19,878][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint96.pt
[2025-07-04 11:03:22,134][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint96.pt
[2025-07-04 11:03:22,941][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint96.pt (epoch 96 @ 23231 updates, score None) (writing took 3.063209599997208 seconds)
[2025-07-04 11:03:22,941][fairseq_cli.train][INFO] - end of epoch 96 (average epoch stats below)
[2025-07-04 11:03:22,957][train][INFO] - {"epoch": 96, "train_loss": "5.021", "train_nll_loss": "0.014", "train_loss_recon": "0.098", "train_loss_info_nce": "4.045", "train_ppl": "1.01", "train_wps": "925.4", "train_ups": "0.31", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "23231", "train_lr": "5.81935e-05", "train_gnorm": "0.962", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "764", "train_gb_free": "1.7", "train_wall": "51765"}
[2025-07-04 11:03:23,197][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 11:03:23,197][fairseq.trainer][INFO] - begin training epoch 97
[2025-07-04 11:03:23,197][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 11:12:23,804][train_inner][INFO] - {"epoch": 97, "update": 96.698, "loss": "5.016", "nll_loss": "0.014", "loss_recon": "0.097", "loss_info_nce": "4.043", "ppl": "1.01", "wps": "923.4", "ups": "0.31", "wpb": "2956.5", "bsz": "385.4", "num_updates": "23400", "lr": "5.78261e-05", "gnorm": "0.86", "clip": "0", "loss_scale": "65536", "train_wall": "633", "gb_free": "1.5", "wall": "52306"}
[2025-07-04 11:16:20,098][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 97 @ 23473 updates
[2025-07-04 11:16:20,108][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint97.pt
[2025-07-04 11:16:22,319][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint97.pt
[2025-07-04 11:16:23,083][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint97.pt (epoch 97 @ 23473 updates, score None) (writing took 2.984645800002909 seconds)
[2025-07-04 11:16:23,083][fairseq_cli.train][INFO] - end of epoch 97 (average epoch stats below)
[2025-07-04 11:16:23,109][train][INFO] - {"epoch": 97, "train_loss": "5.016", "train_nll_loss": "0.014", "train_loss_recon": "0.097", "train_loss_info_nce": "4.041", "train_ppl": "1.01", "train_wps": "917.2", "train_ups": "0.31", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "23473", "train_lr": "5.76674e-05", "train_gnorm": "0.856", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "772", "train_gb_free": "1.6", "train_wall": "52545"}
[2025-07-04 11:16:23,366][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 11:16:23,366][fairseq.trainer][INFO] - begin training epoch 98
[2025-07-04 11:16:23,366][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 11:23:26,394][train_inner][INFO] - {"epoch": 98, "update": 97.525, "loss": "5.014", "nll_loss": "0.014", "loss_recon": "0.097", "loss_info_nce": "4.038", "ppl": "1.01", "wps": "892.1", "ups": "0.3", "wpb": "2955.5", "bsz": "385.4", "num_updates": "23600", "lr": "5.73913e-05", "gnorm": "0.919", "clip": "0", "loss_scale": "131072", "train_wall": "655", "gb_free": "1.5", "wall": "52969"}
[2025-07-04 11:27:01,046][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 65536.0
[2025-07-04 11:29:18,971][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 98 @ 23714 updates
[2025-07-04 11:29:18,972][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint98.pt
[2025-07-04 11:29:21,155][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint98.pt
[2025-07-04 11:29:22,047][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint98.pt (epoch 98 @ 23714 updates, score None) (writing took 3.0757568000044557 seconds)
[2025-07-04 11:29:22,048][fairseq_cli.train][INFO] - end of epoch 98 (average epoch stats below)
[2025-07-04 11:29:22,069][train][INFO] - {"epoch": 98, "train_loss": "5.011", "train_nll_loss": "0.014", "train_loss_recon": "0.097", "train_loss_info_nce": "4.037", "train_ppl": "1.01", "train_wps": "915", "train_ups": "0.31", "train_wpb": "2957.5", "train_bsz": "385.6", "train_num_updates": "23714", "train_lr": "5.71435e-05", "train_gnorm": "0.97", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "771", "train_gb_free": "1.5", "train_wall": "53324"}
[2025-07-04 11:29:22,315][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 11:29:22,318][fairseq.trainer][INFO] - begin training epoch 99
[2025-07-04 11:29:22,318][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 11:33:44,328][train_inner][INFO] - {"epoch": 99, "update": 98.355, "loss": "5.008", "nll_loss": "0.014", "loss_recon": "0.097", "loss_info_nce": "4.035", "ppl": "1.01", "wps": "956.3", "ups": "0.32", "wpb": "2954.6", "bsz": "385.6", "num_updates": "23800", "lr": "5.69565e-05", "gnorm": "0.94", "clip": "0", "loss_scale": "65536", "train_wall": "611", "gb_free": "1.5", "wall": "53587"}
[2025-07-04 11:41:19,913][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 99 @ 23956 updates
[2025-07-04 11:41:19,913][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint99.pt
[2025-07-04 11:41:22,047][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint99.pt
[2025-07-04 11:41:22,834][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint99.pt (epoch 99 @ 23956 updates, score None) (writing took 2.9215179000020726 seconds)
[2025-07-04 11:41:22,834][fairseq_cli.train][INFO] - end of epoch 99 (average epoch stats below)
[2025-07-04 11:41:22,844][train][INFO] - {"epoch": 99, "train_loss": "5.008", "train_nll_loss": "0.014", "train_loss_recon": "0.097", "train_loss_info_nce": "4.034", "train_ppl": "1.01", "train_wps": "992.8", "train_ups": "0.34", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "23956", "train_lr": "5.66174e-05", "train_gnorm": "0.901", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "714", "train_gb_free": "1.5", "train_wall": "54045"}
[2025-07-04 11:41:23,049][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 11:41:23,052][fairseq.trainer][INFO] - begin training epoch 100
[2025-07-04 11:41:23,052][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 11:43:37,210][train_inner][INFO] - {"epoch": 100, "update": 99.182, "loss": "5.011", "nll_loss": "0.014", "loss_recon": "0.097", "loss_info_nce": "4.036", "ppl": "1.01", "wps": "997.3", "ups": "0.34", "wpb": "2956.5", "bsz": "385.1", "num_updates": "24000", "lr": "5.65217e-05", "gnorm": "0.885", "clip": "0", "loss_scale": "65536", "train_wall": "587", "gb_free": "1.5", "wall": "54180"}
[2025-07-04 11:54:15,007][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-04 11:55:08,298][valid][INFO] - {"epoch": 100, "valid_loss": "4.751", "valid_nll_loss": "0.013", "valid_loss_recon": "0.089", "valid_loss_info_nce": "3.862", "valid_ppl": "1.01", "valid_wps": "1548.9", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "24198", "valid_best_loss": "4.751"}
[2025-07-04 11:55:08,308][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 100 @ 24198 updates
[2025-07-04 11:55:08,308][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint100.pt
[2025-07-04 11:55:10,715][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint100.pt
[2025-07-04 11:55:13,533][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint100.pt (epoch 100 @ 24198 updates, score 4.751) (writing took 5.227610100002494 seconds)
[2025-07-04 11:55:13,533][fairseq_cli.train][INFO] - end of epoch 100 (average epoch stats below)
[2025-07-04 11:55:13,575][train][INFO] - {"epoch": 100, "train_loss": "5.006", "train_nll_loss": "0.014", "train_loss_recon": "0.097", "train_loss_info_nce": "4.033", "train_ppl": "1.01", "train_wps": "861.4", "train_ups": "0.29", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "24198", "train_lr": "5.60913e-05", "train_gnorm": "0.842", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "768", "train_gb_free": "1.5", "train_wall": "54876"}
[2025-07-04 11:55:13,970][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 11:55:13,986][fairseq.trainer][INFO] - begin training epoch 101
[2025-07-04 11:55:13,986][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 11:55:24,852][train_inner][INFO] - {"epoch": 101, "update": 100.008, "loss": "5.003", "nll_loss": "0.014", "loss_recon": "0.097", "loss_info_nce": "4.031", "ppl": "1.01", "wps": "834.8", "ups": "0.28", "wpb": "2953.7", "bsz": "386.4", "num_updates": "24200", "lr": "5.6087e-05", "gnorm": "0.851", "clip": "0", "loss_scale": "65536", "train_wall": "644", "gb_free": "1.5", "wall": "54887"}
[2025-07-04 12:06:44,272][train_inner][INFO] - {"epoch": 101, "update": 100.835, "loss": "4.998", "nll_loss": "0.013", "loss_recon": "0.097", "loss_info_nce": "4.027", "ppl": "1.01", "wps": "871.9", "ups": "0.29", "wpb": "2962.1", "bsz": "387.4", "num_updates": "24400", "lr": "5.56522e-05", "gnorm": "0.856", "clip": "0", "loss_scale": "65536", "train_wall": "676", "gb_free": "1.5", "wall": "55567"}
[2025-07-04 12:08:48,504][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 101 @ 24440 updates
[2025-07-04 12:08:48,504][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint101.pt
[2025-07-04 12:08:50,673][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint101.pt
[2025-07-04 12:08:51,512][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint101.pt (epoch 101 @ 24440 updates, score None) (writing took 3.010822199998074 seconds)
[2025-07-04 12:08:51,512][fairseq_cli.train][INFO] - end of epoch 101 (average epoch stats below)
[2025-07-04 12:08:51,528][train][INFO] - {"epoch": 101, "train_loss": "5.001", "train_nll_loss": "0.013", "train_loss_recon": "0.097", "train_loss_info_nce": "4.029", "train_ppl": "1.01", "train_wps": "874.8", "train_ups": "0.3", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "24440", "train_lr": "5.55652e-05", "train_gnorm": "0.85", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "808", "train_gb_free": "1.5", "train_wall": "55694"}
[2025-07-04 12:08:51,725][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 12:08:51,725][fairseq.trainer][INFO] - begin training epoch 102
[2025-07-04 12:08:51,725][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 12:17:35,665][train_inner][INFO] - {"epoch": 102, "update": 101.661, "loss": "5", "nll_loss": "0.013", "loss_recon": "0.097", "loss_info_nce": "4.028", "ppl": "1.01", "wps": "908", "ups": "0.31", "wpb": "2957.4", "bsz": "384.8", "num_updates": "24600", "lr": "5.52174e-05", "gnorm": "0.836", "clip": "0", "loss_scale": "65536", "train_wall": "644", "gb_free": "1.5", "wall": "56218"}
[2025-07-04 12:21:35,967][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 102 @ 24682 updates
[2025-07-04 12:21:35,967][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint102.pt
[2025-07-04 12:21:38,209][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint102.pt
[2025-07-04 12:21:39,019][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint102.pt (epoch 102 @ 24682 updates, score None) (writing took 3.0536728000006406 seconds)
[2025-07-04 12:21:39,019][fairseq_cli.train][INFO] - end of epoch 102 (average epoch stats below)
[2025-07-04 12:21:39,019][train][INFO] - {"epoch": 102, "train_loss": "4.997", "train_nll_loss": "0.013", "train_loss_recon": "0.097", "train_loss_info_nce": "4.026", "train_ppl": "1.01", "train_wps": "932.3", "train_ups": "0.32", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "24682", "train_lr": "5.50391e-05", "train_gnorm": "0.845", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "760", "train_gb_free": "1.5", "train_wall": "56461"}
[2025-07-04 12:21:39,232][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 12:21:39,248][fairseq.trainer][INFO] - begin training epoch 103
[2025-07-04 12:21:39,248][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 12:27:48,531][train_inner][INFO] - {"epoch": 103, "update": 102.488, "loss": "4.996", "nll_loss": "0.013", "loss_recon": "0.097", "loss_info_nce": "4.025", "ppl": "1.01", "wps": "964.2", "ups": "0.33", "wpb": "2954.6", "bsz": "385.9", "num_updates": "24800", "lr": "5.47826e-05", "gnorm": "0.825", "clip": "0", "loss_scale": "65536", "train_wall": "607", "gb_free": "1.5", "wall": "56831"}
[2025-07-04 12:34:15,550][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 103 @ 24924 updates
[2025-07-04 12:34:15,550][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint103.pt
[2025-07-04 12:34:17,854][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint103.pt
[2025-07-04 12:34:18,702][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint103.pt (epoch 103 @ 24924 updates, score None) (writing took 3.1521173999935854 seconds)
[2025-07-04 12:34:18,702][fairseq_cli.train][INFO] - end of epoch 103 (average epoch stats below)
[2025-07-04 12:34:18,710][train][INFO] - {"epoch": 103, "train_loss": "4.994", "train_nll_loss": "0.013", "train_loss_recon": "0.097", "train_loss_info_nce": "4.024", "train_ppl": "1.01", "train_wps": "941.9", "train_ups": "0.32", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "24924", "train_lr": "5.4513e-05", "train_gnorm": "0.81", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "753", "train_gb_free": "1.5", "train_wall": "57221"}
[2025-07-04 12:34:18,947][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 12:34:18,950][fairseq.trainer][INFO] - begin training epoch 104
[2025-07-04 12:34:18,951][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 12:37:45,040][train_inner][INFO] - {"epoch": 104, "update": 103.314, "loss": "4.993", "nll_loss": "0.013", "loss_recon": "0.097", "loss_info_nce": "4.022", "ppl": "1.01", "wps": "991.3", "ups": "0.34", "wpb": "2956.5", "bsz": "385.6", "num_updates": "25000", "lr": "5.43478e-05", "gnorm": "0.841", "clip": "0", "loss_scale": "65536", "train_wall": "590", "gb_free": "1.5", "wall": "57427"}
[2025-07-04 12:37:45,041][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-04 12:38:14,677][valid][INFO] - {"epoch": 104, "valid_loss": "4.776", "valid_nll_loss": "0.013", "valid_loss_recon": "0.092", "valid_loss_info_nce": "3.853", "valid_ppl": "1.01", "valid_wps": "2792.4", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "25000", "valid_best_loss": "4.751"}
[2025-07-04 12:38:14,679][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 104 @ 25000 updates
[2025-07-04 12:38:14,680][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint_104_25000.pt
[2025-07-04 12:38:16,950][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint_104_25000.pt
[2025-07-04 12:38:17,745][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint_104_25000.pt (epoch 104 @ 25000 updates, score 4.776) (writing took 3.0665078000020003 seconds)
[2025-07-04 12:45:40,922][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 104 @ 25166 updates
[2025-07-04 12:45:40,924][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint104.pt
[2025-07-04 12:45:43,118][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint104.pt
[2025-07-04 12:45:43,927][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint104.pt (epoch 104 @ 25166 updates, score None) (writing took 3.0041807999950834 seconds)
[2025-07-04 12:45:43,927][fairseq_cli.train][INFO] - end of epoch 104 (average epoch stats below)
[2025-07-04 12:45:43,939][train][INFO] - {"epoch": 104, "train_loss": "4.99", "train_nll_loss": "0.013", "train_loss_recon": "0.097", "train_loss_info_nce": "4.021", "train_ppl": "1.01", "train_wps": "1044.3", "train_ups": "0.35", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "25166", "train_lr": "5.3987e-05", "train_gnorm": "0.856", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "646", "train_gb_free": "1.5", "train_wall": "57906"}
[2025-07-04 12:45:44,162][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 12:45:44,166][fairseq.trainer][INFO] - begin training epoch 105
[2025-07-04 12:45:44,166][fairseq_cli.train][INFO] - Start iterating over samples
[2025-07-04 12:47:16,962][train_inner][INFO] - {"epoch": 105, "update": 104.14, "loss": "4.989", "nll_loss": "0.013", "loss_recon": "0.097", "loss_info_nce": "4.021", "ppl": "1.01", "wps": "1033.6", "ups": "0.35", "wpb": "2955.5", "bsz": "385.4", "num_updates": "25200", "lr": "5.3913e-05", "gnorm": "0.87", "clip": "0", "loss_scale": "65536", "train_wall": "533", "gb_free": "1.5", "wall": "57999"}
[2025-07-04 12:56:21,707][train_inner][INFO] - {"epoch": 105, "update": 104.967, "loss": "4.987", "nll_loss": "0.013", "loss_recon": "0.097", "loss_info_nce": "4.019", "ppl": "1.01", "wps": "1088.2", "ups": "0.37", "wpb": "2963.9", "bsz": "386.8", "num_updates": "25400", "lr": "5.34783e-05", "gnorm": "0.811", "clip": "0", "loss_scale": "65536", "train_wall": "544", "gb_free": "1.5", "wall": "58544"}
[2025-07-04 12:56:41,511][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-07-04 12:57:12,401][valid][INFO] - {"epoch": 105, "valid_loss": "4.782", "valid_nll_loss": "0.013", "valid_loss_recon": "0.093", "valid_loss_info_nce": "3.853", "valid_ppl": "1.01", "valid_wps": "2671.7", "valid_wpb": "368.6", "valid_bsz": "48", "valid_num_updates": "25408", "valid_best_loss": "4.751"}
[2025-07-04 12:57:12,402][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 105 @ 25408 updates
[2025-07-04 12:57:12,403][fairseq.trainer][INFO] - Saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint105.pt
[2025-07-04 12:57:14,672][fairseq.trainer][INFO] - Finished saving checkpoint to D:\MBARI 2KHz\training\250702 6en 1de w norm\output_model\checkpoints\checkpoint105.pt
[2025-07-04 12:57:15,547][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints\checkpoint105.pt (epoch 105 @ 25408 updates, score 4.782) (writing took 3.1445114000016474 seconds)
[2025-07-04 12:57:15,548][fairseq_cli.train][INFO] - end of epoch 105 (average epoch stats below)
[2025-07-04 12:57:15,568][train][INFO] - {"epoch": 105, "train_loss": "4.988", "train_nll_loss": "0.013", "train_loss_recon": "0.097", "train_loss_info_nce": "4.019", "train_ppl": "1.01", "train_wps": "1034.6", "train_ups": "0.35", "train_wpb": "2956.8", "train_bsz": "385.8", "train_num_updates": "25408", "train_lr": "5.34609e-05", "train_gnorm": "0.841", "train_clip": "0", "train_loss_scale": "65536", "train_train_wall": "654", "train_gb_free": "1.5", "train_wall": "58598"}
[2025-07-04 12:57:15,766][fairseq.data.iterators][INFO] - grouped total_num_itrs = 242
[2025-07-04 12:57:15,769][fairseq.trainer][INFO] - begin training epoch 106
[2025-07-04 12:57:15,769][fairseq_cli.train][INFO] - Start iterating over samples
