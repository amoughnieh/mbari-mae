{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "zqEAfZZYIaiX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/incantator/Documents/mbari-mae\n"
     ]
    }
   ],
   "source": [
    "# Standard library\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "os.chdir(\"../..\")\n",
    "print(os.getcwd())\n",
    "\n",
    "# Numerical computing\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader\n",
    "import torchaudio.transforms as T\n",
    "import torchaudio.functional as F\n",
    "\n",
    "# Datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "# PyTorch Lightning\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.utilities.model_summary import ModelSummary\n",
    "\n",
    "# Metrics\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torchmetrics.classification import (\n",
    "    MulticlassF1Score,\n",
    "    MulticlassPrecision,\n",
    "    MulticlassRecall,\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ==== BACKBONE ====\n",
    "# Hugging Face Transformers\n",
    "from transformers import (\n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    Wav2Vec2Model,\n",
    "    ViTFeatureExtractor, \n",
    "    ViTModel\n",
    ")\n",
    "\n",
    "# MAE-AST Library\n",
    "from s3prl.nn.upstream import S3PRLUpstream\n",
    "\n",
    "# Fix the length of the input audio to the same length\n",
    "def _match_length_force(self, xs, target_max_len):\n",
    "    xs_max_len = xs.size(1)\n",
    "    if xs_max_len > target_max_len:\n",
    "        xs = xs[:, :target_max_len, :]\n",
    "    elif xs_max_len < target_max_len:\n",
    "        pad_len = target_max_len - xs_max_len\n",
    "        xs = torch.cat(\n",
    "            (xs, xs[:, -1:, :].repeat(1, pad_len, 1)),\n",
    "            dim=1\n",
    "        )\n",
    "    return xs\n",
    "\n",
    "S3PRLUpstream._match_length = _match_length_force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "C5aQ8ZsGkD7s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 1017 examples, 31 classes\n",
      "  Clymene_Dolphin: 38\n",
      "  Bottlenose_Dolphin: 15\n",
      "  Spinner_Dolphin: 69\n",
      "  Beluga,_White_Whale: 30\n",
      "  Bearded_Seal: 22\n",
      "  Minke_Whale: 10\n",
      "  Humpback_Whale: 38\n",
      "  Southern_Right_Whale: 15\n",
      "  White-sided_Dolphin: 33\n",
      "  Narwhal: 30\n",
      "  White-beaked_Dolphin: 34\n",
      "  Northern_Right_Whale: 32\n",
      "  Frasers_Dolphin: 52\n",
      "  Grampus,_Rissos_Dolphin: 40\n",
      "  Harp_Seal: 28\n",
      "  Atlantic_Spotted_Dolphin: 35\n",
      "  Fin,_Finback_Whale: 30\n",
      "  Ross_Seal: 30\n",
      "  Rough-Toothed_Dolphin: 30\n",
      "  Killer_Whale: 21\n",
      "  Pantropical_Spotted_Dolphin: 40\n",
      "  Short-Finned_Pacific_Pilot_Whale: 40\n",
      "  Bowhead_Whale: 36\n",
      "  False_Killer_Whale: 35\n",
      "  Melon_Headed_Whale: 38\n",
      "  Long-Finned_Pilot_Whale: 42\n",
      "  Striped_Dolphin: 49\n",
      "  Leopard_Seal: 6\n",
      "  Walrus: 23\n",
      "  Sperm_Whale: 45\n",
      "  Common_Dolphin: 31\n",
      "Validation dataset: 339 examples, 31 classes\n",
      "  Clymene_Dolphin: 12\n",
      "  Bottlenose_Dolphin: 5\n",
      "  Spinner_Dolphin: 23\n",
      "  Beluga,_White_Whale: 10\n",
      "  Bearded_Seal: 7\n",
      "  Minke_Whale: 4\n",
      "  Humpback_Whale: 13\n",
      "  Southern_Right_Whale: 5\n",
      "  White-sided_Dolphin: 11\n",
      "  Narwhal: 10\n",
      "  White-beaked_Dolphin: 11\n",
      "  Northern_Right_Whale: 11\n",
      "  Frasers_Dolphin: 18\n",
      "  Grampus,_Rissos_Dolphin: 13\n",
      "  Harp_Seal: 9\n",
      "  Atlantic_Spotted_Dolphin: 12\n",
      "  Fin,_Finback_Whale: 10\n",
      "  Ross_Seal: 10\n",
      "  Rough-Toothed_Dolphin: 10\n",
      "  Killer_Whale: 7\n",
      "  Pantropical_Spotted_Dolphin: 13\n",
      "  Short-Finned_Pacific_Pilot_Whale: 13\n",
      "  Bowhead_Whale: 12\n",
      "  False_Killer_Whale: 12\n",
      "  Melon_Headed_Whale: 13\n",
      "  Long-Finned_Pilot_Whale: 14\n",
      "  Striped_Dolphin: 16\n",
      "  Leopard_Seal: 2\n",
      "  Walrus: 8\n",
      "  Sperm_Whale: 15\n",
      "  Common_Dolphin: 10\n",
      "Test dataset: 339 examples, 31 classes\n",
      "  Clymene_Dolphin: 13\n",
      "  Bottlenose_Dolphin: 4\n",
      "  Spinner_Dolphin: 22\n",
      "  Beluga,_White_Whale: 10\n",
      "  Bearded_Seal: 8\n",
      "  Minke_Whale: 3\n",
      "  Humpback_Whale: 13\n",
      "  Southern_Right_Whale: 5\n",
      "  White-sided_Dolphin: 11\n",
      "  Narwhal: 10\n",
      "  White-beaked_Dolphin: 12\n",
      "  Northern_Right_Whale: 11\n",
      "  Frasers_Dolphin: 17\n",
      "  Grampus,_Rissos_Dolphin: 14\n",
      "  Harp_Seal: 10\n",
      "  Atlantic_Spotted_Dolphin: 11\n",
      "  Fin,_Finback_Whale: 10\n",
      "  Ross_Seal: 10\n",
      "  Rough-Toothed_Dolphin: 10\n",
      "  Killer_Whale: 7\n",
      "  Pantropical_Spotted_Dolphin: 13\n",
      "  Short-Finned_Pacific_Pilot_Whale: 14\n",
      "  Bowhead_Whale: 12\n",
      "  False_Killer_Whale: 12\n",
      "  Melon_Headed_Whale: 12\n",
      "  Long-Finned_Pilot_Whale: 14\n",
      "  Striped_Dolphin: 16\n",
      "  Leopard_Seal: 2\n",
      "  Walrus: 7\n",
      "  Sperm_Whale: 15\n",
      "  Common_Dolphin: 11\n"
     ]
    }
   ],
   "source": [
    "# loading the dataset\n",
    "data_dir = \"data/watkins\"\n",
    "annotations_file_train = os.path.join(data_dir, \"annotations.train.csv\")\n",
    "annotations_file_valid = os.path.join(data_dir, \"annotations.valid.csv\")\n",
    "annotations_file_test = os.path.join(data_dir, \"annotations.test.csv\")\n",
    "\n",
    "ds = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\"train\": annotations_file_train,\n",
    "                \"validation\": annotations_file_valid,\n",
    "                \"test\": annotations_file_test},\n",
    ")\n",
    "\n",
    "for split_name in [\"train\", \"validation\", \"test\"]:\n",
    "    split_dataset = ds[split_name]\n",
    "    labels = split_dataset[\"label\"]\n",
    "    total = len(labels)\n",
    "    counts = Counter(labels)\n",
    "\n",
    "    print(f\"{split_name.capitalize()} dataset: {total} examples, {len(counts)} classes\")\n",
    "    if \"label\" in split_dataset.features and hasattr(split_dataset.features[\"label\"], \"names\"):\n",
    "        class_names = split_dataset.features[\"label\"].names\n",
    "        for idx, name in enumerate(class_names):\n",
    "            print(f\"  {idx} ({name}): {counts.get(name, 0)}\")\n",
    "    else:\n",
    "        for label, count in counts.items():\n",
    "            print(f\"  {label}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class weights calculation\n",
    "train_labels = ds[\"train\"][\"label\"]\n",
    "unique_labels = sorted(set(train_labels))\n",
    "label_to_int = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "y_train = [label_to_int[lbl] for lbl in train_labels]\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.arange(len(unique_labels)),\n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "num_classes = len(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6JYR66KeIUsg"
   },
   "outputs": [],
   "source": [
    "# model definition\n",
    "class WMMDClassifier(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int,\n",
    "        lr: float = 1e-3,\n",
    "        backbone: str = \"facebook/wav2vec2-base\",\n",
    "        ckpt_path: str = \"\",\n",
    "        finetune: bool = False,\n",
    "        class_weights=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # ========== WAV2VEC2 ==========\n",
    "        if backbone == \"facebook/wav2vec2-base\":\n",
    "            self.backbone     = Wav2Vec2Model.from_pretrained(backbone)\n",
    "            self.embedding_dim = self.backbone.config.hidden_size\n",
    "        \n",
    "        # ========== TINY WAV2VEC2 ==========\n",
    "        elif backbone == \"patrickvonplaten/tiny-wav2vec2-no-tokenizer\":\n",
    "            self.backbone     = Wav2Vec2Model.from_pretrained(backbone)\n",
    "            self.embedding_dim = self.backbone.config.hidden_size\n",
    "        \n",
    "        # ========== MAE-AST ==========\n",
    "        elif backbone == \"mae-ast\":\n",
    "            up_kwargs = {\"name\": \"mae_ast_patch\"}\n",
    "            s3 = S3PRLUpstream(**up_kwargs)\n",
    "\n",
    "            enc = s3.upstream.model.encoder\n",
    "            enc.layers = nn.ModuleList(list(enc.layers)[:4])\n",
    "            s3.upstream.model.dec_sine_pos_embed = None\n",
    "            s3.upstream.model.decoder = None\n",
    "            s3.upstream.model.final_proj_reconstruction = None\n",
    "            s3.upstream.model.final_proj_classification  = None\n",
    "\n",
    "            new_n = len(enc.layers)\n",
    "            s3._num_layers       = new_n\n",
    "            s3._hidden_sizes     = s3._hidden_sizes[:new_n]\n",
    "            s3._downsample_rates = s3._downsample_rates[:new_n]\n",
    "\n",
    "            self.backbone      = s3\n",
    "            self.embedding_dim = s3.hidden_sizes[-1]\n",
    "\n",
    "            # Load the checkpoint for mae ast\n",
    "            if ckpt_path:\n",
    "                self.load_mae_ckpt(ckpt_path)\n",
    "            \n",
    "            mel_transform = T.MelSpectrogram(\n",
    "                sample_rate=2000, \n",
    "                n_fft=1024, \n",
    "                win_length=512,\n",
    "                hop_length=20, \n",
    "                n_mels=128,\n",
    "            )\n",
    "\n",
    "            class DBWithDeltas(nn.Module):\n",
    "                def __init__(self):\n",
    "                    super().__init__()\n",
    "\n",
    "                def forward(self, spec):\n",
    "                    spec_db = F.amplitude_to_DB(\n",
    "                        spec,\n",
    "                        multiplier=10.0,\n",
    "                        amin=1e-10,\n",
    "                        db_multiplier=0\n",
    "                    )\n",
    "                    t = spec_db.transpose(0, 1)\n",
    "                    d1 = F.compute_deltas(t.transpose(0,1))\n",
    "                    d2 = F.compute_deltas(d1)\n",
    "                    return torch.cat([\n",
    "                        t,\n",
    "                        d1.transpose(0,1),\n",
    "                        d2.transpose(0,1)\n",
    "                    ], dim=1)\n",
    "            \n",
    "            s3.upstream.model.fbank   = mel_transform\n",
    "            s3.upstream.model.db_norm = DBWithDeltas()\n",
    "        \n",
    "        # ========== CNN-SPECTROGRAM ==========\n",
    "        elif backbone == \"cnn-spect\":\n",
    "            self.mel_spec = torchaudio.transforms.MelSpectrogram(\n",
    "                sample_rate=2000,\n",
    "                n_fft=1024,\n",
    "                hop_length=20,\n",
    "                win_length=512,\n",
    "                n_mels=128\n",
    "            )\n",
    "            self.to_db = torchaudio.transforms.AmplitudeToDB(top_db=80)\n",
    "\n",
    "            self.backbone = nn.Sequential(\n",
    "                nn.Conv2d(3, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(),\n",
    "                nn.Conv2d(32,32,3,padding=1), nn.BatchNorm2d(32), nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "\n",
    "                nn.Conv2d(32,64,3,padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "                nn.Conv2d(64,64,3,padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "\n",
    "                nn.Conv2d(64,128,3,padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
    "                nn.Conv2d(128,128,3,padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "\n",
    "                nn.Conv2d(128,256,3,padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n",
    "                nn.Conv2d(256,256,3,padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "\n",
    "                nn.Conv2d(256,512,3,padding=1), nn.BatchNorm2d(512), nn.ReLU(),\n",
    "                nn.Conv2d(512,512,3,padding=1), nn.BatchNorm2d(512), nn.ReLU(),\n",
    "                nn.AdaptiveAvgPool2d((1,1)),\n",
    "                nn.Flatten()\n",
    "            )\n",
    "            self.embedding_dim = 512\n",
    "        \n",
    "        # ========== VIT-IMAGENET ==========\n",
    "        elif backbone == \"vit-imagenet\":\n",
    "            self.mel_spec = torchaudio.transforms.MelSpectrogram(\n",
    "                sample_rate=2000,\n",
    "                n_fft=1024,\n",
    "                hop_length=20,\n",
    "                win_length=512,\n",
    "                n_mels=128\n",
    "            )\n",
    "            self.to_db = torchaudio.transforms.AmplitudeToDB(top_db=80)\n",
    "\n",
    "            self.feature_extractor = ViTFeatureExtractor.from_pretrained(\n",
    "                \"google/vit-base-patch16-224-in21k\"\n",
    "            )\n",
    "\n",
    "            self.backbone = ViTModel.from_pretrained(\n",
    "                \"google/vit-base-patch16-224-in21k\",\n",
    "                add_pooling_layer=False\n",
    "            )\n",
    "            self.embedding_dim = self.backbone.config.hidden_size\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported backbone '{backbone}'\")\n",
    "\n",
    "        try:\n",
    "            self.backbone.gradient_checkpointing_enable()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = finetune\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(self.embedding_dim, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, num_classes),\n",
    "        )\n",
    "\n",
    "        if class_weights is not None:\n",
    "            cw = torch.tensor(class_weights, dtype=torch.float)\n",
    "            self.criterion = nn.CrossEntropyLoss(weight=cw)\n",
    "        else:\n",
    "            self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        metrics_kwargs = dict(num_classes=num_classes, average='macro')\n",
    "        self.train_precision = MulticlassPrecision(**metrics_kwargs)\n",
    "        self.train_recall = MulticlassRecall(**metrics_kwargs)\n",
    "        self.train_f1 = MulticlassF1Score(**metrics_kwargs)\n",
    "        self.val_precision = MulticlassPrecision(**metrics_kwargs)\n",
    "        self.val_recall = MulticlassRecall(**metrics_kwargs)\n",
    "        self.val_f1 = MulticlassF1Score(**metrics_kwargs)\n",
    "        self.test_precision = MulticlassPrecision(**metrics_kwargs)\n",
    "        self.test_recall = MulticlassRecall(**metrics_kwargs)\n",
    "        self.test_f1 = MulticlassF1Score(**metrics_kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bname = self.hparams.backbone.lower()\n",
    "        if bname in {\"facebook/wav2vec2-base\",\n",
    "             \"patrickvonplaten/tiny-wav2vec2-no-tokenizer\"}:\n",
    "            hidden = self.backbone(x).last_hidden_state\n",
    "\n",
    "        elif bname == \"mae-ast\":\n",
    "            if x.dim() == 3:\n",
    "                x = x.squeeze(-1)\n",
    "            wav_lens = torch.full((x.size(0),), x.size(1),\n",
    "                                dtype=torch.long, device=x.device)\n",
    "            hidden = self.backbone(x, wav_lens)[0][-1]\n",
    "\n",
    "        elif bname == \"cnn-spect\":\n",
    "            spec = self._safe_logmelspec(x, self.mel_spec, self.to_db)  # (B,1,F,L) in [0,1]\n",
    "            hidden = self.backbone(spec)\n",
    "\n",
    "        elif bname == \"vit-imagenet\":\n",
    "            spec = self._safe_logmelspec(x, self.mel_spec, self.to_db)  # (B,1,F,L) in [0,1]\n",
    "\n",
    "            img = torch.nn.functional.interpolate(\n",
    "                spec, size=(224, 224), mode=\"bilinear\", align_corners=False\n",
    "            )\n",
    "\n",
    "            pixel_values = self.feature_extractor(images=img, return_tensors=\"pt\").pixel_values.to(img.device)\n",
    "            outputs = self.backbone(pixel_values=pixel_values)\n",
    "            hidden = outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported backbone '{self.hparams.backbone}'\")\n",
    "\n",
    "        emb = hidden if hidden.dim() == 2 else hidden.mean(dim=1)\n",
    "        return self.classifier(emb)\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        self.log_batch_metrics(loss, preds, y, prefix='train')\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        self.log_batch_metrics(loss, preds, y, prefix='val')\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        self.log_batch_metrics(loss, preds, y, prefix='test')\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "    def log_batch_metrics(self, loss, preds, targets, prefix):\n",
    "        self.log(f'{prefix}_loss', loss, prog_bar=True, on_epoch=True)\n",
    "        acc = (preds == targets).float().mean()\n",
    "        self.log(f'{prefix}_acc', acc, prog_bar=True, on_epoch=True)\n",
    "        precision = getattr(self, f'{prefix}_precision')(preds, targets)\n",
    "        recall = getattr(self, f'{prefix}_recall')(preds, targets)\n",
    "        f1 = getattr(self, f'{prefix}_f1')(preds, targets)\n",
    "        self.log(f'{prefix}_precision', precision, on_epoch=True)\n",
    "        self.log(f'{prefix}_recall', recall, on_epoch=True)\n",
    "        self.log(f'{prefix}_f1', f1, on_epoch=True)\n",
    "\n",
    "    def on_train_end(self):\n",
    "        save_dir = getattr(self, 'save_dir', None)\n",
    "        if save_dir:\n",
    "            self.save_model(save_dir)\n",
    "\n",
    "    def save_model(self):\n",
    "        base_dir = 'model'\n",
    "        bn = self.hparams.backbone.replace('/', '_')\n",
    "        cw = getattr(self.hparams, 'class_weights', None)\n",
    "        balance_flag = 'imbalance' if cw is not None else 'balance'\n",
    "        timestamp = getattr(self, 'finish_time', datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "        folder = os.path.join(base_dir, f\"{bn}_{balance_flag}\", timestamp)\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "        ckpt_path = os.path.join(folder, f\"{timestamp}.pt\")\n",
    "        payload = {\n",
    "            'state_dict': self.state_dict(),\n",
    "            'hparams': dict(self.hparams)\n",
    "        }\n",
    "        for attr in ('test_results', 'finish_time', 'epochs_trained'):\n",
    "            if hasattr(self, attr):\n",
    "                payload[attr] = getattr(self, attr)\n",
    "        torch.save(payload, ckpt_path)\n",
    "\n",
    "        stats_path = os.path.join(folder, f\"{timestamp}.txt\")\n",
    "        raw_hparams = dict(self.hparams)\n",
    "        serializable_hparams = {}\n",
    "        for k, v in raw_hparams.items():\n",
    "            if isinstance(v, np.ndarray):\n",
    "                serializable_hparams[k] = v.tolist()\n",
    "            elif isinstance(v, torch.Tensor):\n",
    "                serializable_hparams[k] = v.cpu().item() if v.ndim == 0 else v.cpu().tolist()\n",
    "            else:\n",
    "                serializable_hparams[k] = v\n",
    "\n",
    "        serializable_results = {}\n",
    "        if hasattr(self, 'test_results'):\n",
    "            for k, v in self.test_results.items():\n",
    "                serializable_results[k] = v.cpu().item() if isinstance(v, torch.Tensor) else v\n",
    "\n",
    "        with open(stats_path, 'w') as f:\n",
    "            f.write(f\"Model architecture:\\n{self}\\n\\n\")\n",
    "            f.write(\"Hyperparameters:\\n\")\n",
    "            f.write(json.dumps(serializable_hparams, indent=4))\n",
    "            f.write(\"\\n\\n\")\n",
    "            if serializable_results:\n",
    "                f.write(\"Test results:\\n\")\n",
    "                f.write(json.dumps(serializable_results, indent=4))\n",
    "                f.write(\"\\n\\n\")\n",
    "            if hasattr(self, 'epochs_trained'):\n",
    "                f.write(f\"Epochs trained: {self.epochs_trained}\\n\")\n",
    "\n",
    "        self._last_save_dir = folder\n",
    "        self._last_timestamp = timestamp\n",
    "        print(f\"Artifacts saved to {folder}/\")\n",
    "    \n",
    "    def load_mae_ckpt(self, ckpt_source: str):\n",
    "        \"\"\"\n",
    "        Load MAE-AST checkpoint into the truncated upstream model\n",
    "        and verify exact weight equality for each loaded parameter.\n",
    "        \"\"\"\n",
    "        loaded = torch.load(ckpt_source)\n",
    "        state_dict = loaded.get('model', loaded)\n",
    "\n",
    "        up = self.backbone.upstream.model\n",
    "        up_state = up.state_dict()\n",
    "\n",
    "        to_load = {k: v for k, v in state_dict.items()\n",
    "                   if k in up_state and v.shape == up_state[k].shape}\n",
    "\n",
    "        missing, unexpected = up.load_state_dict(to_load, strict=False)\n",
    "\n",
    "        for k, v in to_load.items():\n",
    "            if not torch.equal(up_state[k], v):\n",
    "                raise RuntimeError(f\"Weight mismatch at '{k}' after loading checkpoint\")\n",
    "\n",
    "        print(f\"Successfully loaded {len(to_load)} parameters; missing: {len(missing)}, unexpected: {len(unexpected)}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def _safe_logmelspec(waveform, mel_spec, to_db, db_range=80.0):\n",
    "        if waveform.dim() == 2:\n",
    "            waveform = waveform.unsqueeze(1)\n",
    "        spec = mel_spec(waveform) + 1e-10\n",
    "        spec_db = to_db(spec)\n",
    "        spec_db = torch.nan_to_num(spec_db, neginf=-db_range)\n",
    "        t = spec_db\n",
    "        d1 = F.compute_deltas(t.squeeze(1)).unsqueeze(1)\n",
    "        d2 = F.compute_deltas(d1.squeeze(1)).unsqueeze(1)\n",
    "        spec3 = torch.cat([t, d1, d2], dim=1)\n",
    "        spec3 = ((spec3 + db_range)/db_range).clamp(0.0, 1.0)\n",
    "        return spec3\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(cls, load_dir: str, map_location=None):\n",
    "        \"\"\"\n",
    "        Load a model checkpoint and hyperparameters from a directory.\n",
    "\n",
    "        Returns:\n",
    "            model (MammalClassifier): Loaded model\n",
    "        \"\"\"\n",
    "        hparams_path = os.path.join(load_dir, 'hparams.json')\n",
    "        with open(hparams_path, 'r') as f:\n",
    "            hparams = json.load(f)\n",
    "\n",
    "        model = cls(**hparams)\n",
    "        ckpt_path = os.path.join(load_dir, f'{cls.__name__}.ckpt')\n",
    "        state = torch.load(ckpt_path, map_location=map_location)\n",
    "        model.load_state_dict(state['state_dict'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   | Name            | Type                | Params | Mode \n",
      "-----------------------------------------------------------------\n",
      "0  | mel_spec        | MelSpectrogram      | 0      | train\n",
      "1  | to_db           | AmplitudeToDB       | 0      | train\n",
      "2  | backbone        | Sequential          | 4.7 M  | train\n",
      "3  | classifier      | Sequential          | 557 K  | train\n",
      "4  | criterion       | CrossEntropyLoss    | 0      | train\n",
      "5  | train_precision | MulticlassPrecision | 0      | train\n",
      "6  | train_recall    | MulticlassRecall    | 0      | train\n",
      "7  | train_f1        | MulticlassF1Score   | 0      | train\n",
      "8  | val_precision   | MulticlassPrecision | 0      | train\n",
      "9  | val_recall      | MulticlassRecall    | 0      | train\n",
      "10 | val_f1          | MulticlassF1Score   | 0      | train\n",
      "11 | test_precision  | MulticlassPrecision | 0      | train\n",
      "12 | test_recall     | MulticlassRecall    | 0      | train\n",
      "13 | test_f1         | MulticlassF1Score   | 0      | train\n",
      "-----------------------------------------------------------------\n",
      "5.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.3 M     Total params\n",
      "21.093    Total estimated model params size (MB)\n",
      "57        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "model = WMMDClassifier(\n",
    "        num_classes=31, lr=1e-3,\n",
    "        backbone=\"cnn-spect\", finetune=True,\n",
    "        class_weights=class_weights,\n",
    "        ckpt_path=\"\"\n",
    "    )\n",
    "\n",
    "print(ModelSummary(model, max_depth=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ycDcNmcwQ_ON"
   },
   "outputs": [],
   "source": [
    "def WMMD_Collate(batch):\n",
    "    waveforms, labels = zip(*batch)\n",
    "\n",
    "    lengths    = [w.shape[0] for w in waveforms]\n",
    "    raw_max    = max(lengths)\n",
    "\n",
    "    min_len    = 5_000\n",
    "    max_len    = 25_000\n",
    "    target_len = min(max(raw_max, min_len), max_len)\n",
    "\n",
    "    padded_waveforms = []\n",
    "    for w in waveforms:\n",
    "        L = w.shape[0]\n",
    "\n",
    "        if L > target_len:\n",
    "            start = random.randint(0, L - target_len)\n",
    "            w2    = w[start : start + target_len]\n",
    "\n",
    "        elif L < target_len:\n",
    "            pad_amt = target_len - L\n",
    "            w2      = torch.nn.functional.pad(w, (0, pad_amt))\n",
    "\n",
    "        else:\n",
    "            w2 = w\n",
    "\n",
    "        padded_waveforms.append(w2)\n",
    "\n",
    "    batch_waveforms = torch.stack(padded_waveforms, dim=0)\n",
    "    batch_labels    = torch.tensor(labels, dtype=torch.long)\n",
    "    return batch_waveforms, batch_labels\n",
    "\n",
    "class WMMDSoundDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, backbone: str, target_sr: int = 2000):\n",
    "        \"\"\"\n",
    "        dataset: list of dicts with keys 'path' & 'label'\n",
    "        backbone: 'facebook/wav2vec2-base' or 'mae-ast'\n",
    "        target_sr: sampling rate (e.g. 2000)\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.backbone = backbone\n",
    "        self.target_sr = target_sr\n",
    "        self.resampler_cache = {}\n",
    "\n",
    "        if self.backbone in {\"facebook/wav2vec2-base\", \"patrickvonplaten/tiny-wav2vec2-no-tokenizer\"}:\n",
    "            self.processor = Wav2Vec2FeatureExtractor.from_pretrained(\n",
    "                self.backbone, return_attention_mask=False, sampling_rate=self.target_sr\n",
    "            )\n",
    "        elif self.backbone in {\"mae-ast\", \"cnn-spect\", \"vit-imagenet\"}:\n",
    "            self.processor = None\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported backbone '{self.backbone}'\")\n",
    "\n",
    "        labels = sorted({item['label'] for item in dataset})\n",
    "        self.label_to_int = {lbl: i for i, lbl in enumerate(labels)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        audio_path = item[\"path\"]\n",
    "        waveform, orig_sr = torchaudio.load(audio_path)\n",
    "\n",
    "        if orig_sr != self.target_sr:\n",
    "            if orig_sr not in self.resampler_cache:\n",
    "                self.resampler_cache[orig_sr] = torchaudio.transforms.Resample(orig_sr, self.target_sr)\n",
    "            waveform = self.resampler_cache[orig_sr](waveform)\n",
    "\n",
    "        waveform = waveform / (waveform.abs().max() + 1e-6)\n",
    "        wav_1d = waveform.squeeze(0)  \n",
    "        \n",
    "        if self.backbone in {\"facebook/wav2vec2-base\", \"patrickvonplaten/tiny-wav2vec2-no-tokenizer\"}:\n",
    "            arr = wav_1d.numpy()\n",
    "            feats = self.processor(arr, sampling_rate=self.target_sr, return_tensors=\"pt\")\n",
    "            inp = feats.input_values.squeeze(0)\n",
    "        elif self.backbone in {\"cnn-spect\", \"vit-imagenet\", \"mae-ast\"}:\n",
    "            inp = wav_1d            \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported backbone '{self.backbone}'\")\n",
    "\n",
    "        lbl = self.label_to_int[item['label']]\n",
    "        return inp, lbl\n",
    "\n",
    "class WMMDDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_dict,\n",
    "        backbone: str,\n",
    "        batch_size: int = 2,\n",
    "        num_workers: int = 1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dataset_dict = dataset_dict\n",
    "        self.backbone = backbone\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_ds = WMMDSoundDataset(self.dataset_dict[\"train\"], backbone=self.backbone)\n",
    "        self.val_ds   = WMMDSoundDataset(self.dataset_dict[\"validation\"], backbone=self.backbone)\n",
    "        self.test_ds  = WMMDSoundDataset(self.dataset_dict[\"test\"], backbone=self.backbone)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            collate_fn=WMMD_Collate\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            collate_fn=WMMD_Collate\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            collate_fn=WMMD_Collate\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "kQ2XWE-pRKAM"
   },
   "outputs": [],
   "source": [
    "# callback for logging metrics\n",
    "class MetricsLogger(pl.Callback):\n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accs = []\n",
    "        self.val_accs = []\n",
    "        self.train_precisions = []\n",
    "        self.val_precisions = []\n",
    "        self.train_recalls = []\n",
    "        self.val_recalls = []\n",
    "        self.train_f1s = []\n",
    "        self.val_f1s = []\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        m = trainer.callback_metrics\n",
    "        self.train_losses.append(m['train_loss'].item())\n",
    "        self.train_accs.append(m['train_acc'].item())\n",
    "        self.train_precisions.append(m['train_precision'].item())\n",
    "        self.train_recalls.append(m['train_recall'].item())\n",
    "        self.train_f1s.append(m['train_f1'].item())\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        m = trainer.callback_metrics\n",
    "        self.val_losses.append(m['val_loss'].item())\n",
    "        self.val_accs.append(m['val_acc'].item())\n",
    "        self.val_precisions.append(m['val_precision'].item())\n",
    "        self.val_recalls.append(m['val_recall'].item())\n",
    "        self.val_f1s.append(m['val_f1'].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = [\n",
    "    # Wav2Vec 2.0 (fine-tune)\n",
    "    {\n",
    "        \"num_classes\": num_classes,\n",
    "        \"lr\": 1e-6,\n",
    "        \"backbone\": \"facebook/wav2vec2-base\",\n",
    "        \"finetune\": True,\n",
    "        \"class_weights\": class_weights,\n",
    "        \"max_epochs\": 500,\n",
    "        \"ckpt_path\": \"\",\n",
    "        \"batch_size\": 2,\n",
    "    },\n",
    "    # # Tiny‐Wav2Vec 2.0 (fine-tune)\n",
    "    # {\n",
    "    #     \"num_classes\": num_classes,\n",
    "    #     \"lr\": 1e-6,\n",
    "    #     \"backbone\": \"patrickvonplaten/tiny-wav2vec2-no-tokenizer\",\n",
    "    #     \"finetune\": True,\n",
    "    #     \"class_weights\": class_weights,\n",
    "    #     \"max_epochs\": 500,\n",
    "    #     \"ckpt_path\": \"\",\n",
    "    #     \"batch_size\": 2,\n",
    "    # },\n",
    "    # # CNN-Spectrogram (train from scratch)\n",
    "    # {\n",
    "    #     \"num_classes\": num_classes,\n",
    "    #     \"lr\": 1e-3,\n",
    "    #     \"backbone\": \"cnn-spect\",\n",
    "    #     \"finetune\": True,\n",
    "    #     \"class_weights\": class_weights,\n",
    "    #     \"max_epochs\": 500,\n",
    "    #     \"ckpt_path\": \"\",\n",
    "    #     \"batch_size\": 2,\n",
    "    # },\n",
    "    # ViT-ImageNet (fine-tune)\n",
    "    {\n",
    "        \"num_classes\": num_classes,\n",
    "        \"lr\": 1e-5,\n",
    "        \"backbone\": \"vit-imagenet\",\n",
    "        \"finetune\": True,\n",
    "        \"class_weights\": class_weights,\n",
    "        \"max_epochs\": 500,\n",
    "        \"ckpt_path\": \"\",\n",
    "        \"batch_size\": 2,\n",
    "    },\n",
    "    # MAE-AST (fine-tune)\n",
    "    # {\n",
    "    #     \"num_classes\": num_classes,\n",
    "    #     \"lr\": 1e-6,\n",
    "    #     \"backbone\": \"mae-ast\",\n",
    "    #     \"finetune\": True,\n",
    "    #     \"class_weights\": class_weights,\n",
    "    #     \"max_epochs\": 500,\n",
    "    #     \"ckpt_path\": \"4Enc_1Dec-61epoch-0.103loss.pt\",\n",
    "    #     \"batch_size\": 2,\n",
    "    # },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9k7VBnf5TWfW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/incantator/Documents/mbari-mae/venv/lib/python3.9/site-packages/transformers/configuration_utils.py:309: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 10\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cfg \u001b[38;5;129;01min\u001b[39;00m model_configs:\n\u001b[1;32m      3\u001b[0m     dm \u001b[38;5;241m=\u001b[39m WMMDDataModule(\n\u001b[1;32m      4\u001b[0m         dataset_dict\u001b[38;5;241m=\u001b[39mds,\n\u001b[1;32m      5\u001b[0m         backbone\u001b[38;5;241m=\u001b[39mcfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackbone\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      6\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m cfg\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m      7\u001b[0m         num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      8\u001b[0m     )\n\u001b[0;32m---> 10\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mWMMDClassifier\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_classes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbackbone\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbackbone\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinetune\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfinetune\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclass_weights\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mckpt_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     metrics_cb \u001b[38;5;241m=\u001b[39m MetricsLogger()\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# callback for early stopping\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 17\u001b[0m, in \u001b[0;36mWMMDClassifier.__init__\u001b[0;34m(self, num_classes, lr, backbone, ckpt_path, finetune, class_weights)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# ========== WAV2VEC2 ==========\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backbone \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/wav2vec2-base\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone     \u001b[38;5;241m=\u001b[39m \u001b[43mWav2Vec2Model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhidden_size\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# ========== TINY WAV2VEC2 ==========\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/mbari-mae/venv/lib/python3.9/site-packages/transformers/modeling_utils.py:311\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    313\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m~/Documents/mbari-mae/venv/lib/python3.9/site-packages/transformers/modeling_utils.py:4833\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4823\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4824\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   4826\u001b[0m     (\n\u001b[1;32m   4827\u001b[0m         model,\n\u001b[1;32m   4828\u001b[0m         missing_keys,\n\u001b[1;32m   4829\u001b[0m         unexpected_keys,\n\u001b[1;32m   4830\u001b[0m         mismatched_keys,\n\u001b[1;32m   4831\u001b[0m         offload_index,\n\u001b[1;32m   4832\u001b[0m         error_msgs,\n\u001b[0;32m-> 4833\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4834\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4835\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4836\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4837\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4838\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4839\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4842\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4846\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4849\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4851\u001b[0m \u001b[38;5;66;03m# record tp degree the model sharded to\u001b[39;00m\n\u001b[1;32m   4852\u001b[0m model\u001b[38;5;241m.\u001b[39m_tp_size \u001b[38;5;241m=\u001b[39m tp_size\n",
      "File \u001b[0;32m~/Documents/mbari-mae/venv/lib/python3.9/site-packages/transformers/modeling_utils.py:5099\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[0m\n\u001b[1;32m   5096\u001b[0m     original_checkpoint_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(state_dict\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m   5097\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5098\u001b[0m     original_checkpoint_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m-> 5099\u001b[0m         \u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m   5100\u001b[0m     )\n\u001b[1;32m   5102\u001b[0m \u001b[38;5;66;03m# Check if we are in a special state, i.e. loading from a state dict coming from a different architecture\u001b[39;00m\n\u001b[1;32m   5103\u001b[0m prefix \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mbase_model_prefix\n",
      "File \u001b[0;32m~/Documents/mbari-mae/venv/lib/python3.9/site-packages/transformers/modeling_utils.py:556\u001b[0m, in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file, is_quantized, map_location, weights_only)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;66;03m# Fallback to torch.load (if weights_only was explicitly False, do not check safety as this is known to be unsafe)\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[0;32m--> 556\u001b[0m     \u001b[43mcheck_torch_load_is_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m map_location \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/mbari-mae/venv/lib/python3.9/site-packages/transformers/utils/import_utils.py:1517\u001b[0m, in \u001b[0;36mcheck_torch_load_is_safe\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcheck_torch_load_is_safe\u001b[39m():\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_greater_or_equal(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.6\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1517\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1518\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDue to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1519\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1520\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhen loading files with safetensors.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1521\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1522\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434"
     ]
    }
   ],
   "source": [
    "# training Loops\n",
    "for cfg in model_configs:\n",
    "    dm = WMMDDataModule(\n",
    "        dataset_dict=ds,\n",
    "        backbone=cfg[\"backbone\"],\n",
    "        batch_size = cfg.get(\"batch_size\", 2),\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    model = WMMDClassifier(\n",
    "        num_classes=cfg['num_classes'], lr=cfg['lr'],\n",
    "        backbone=cfg['backbone'], finetune=cfg['finetune'],\n",
    "        class_weights=cfg['class_weights'], \n",
    "        ckpt_path=cfg['ckpt_path']\n",
    "    )\n",
    "    metrics_cb = MetricsLogger()\n",
    "    # callback for early stopping\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        patience=20,\n",
    "        min_delta=0.01,\n",
    "        verbose=True\n",
    "    )\n",
    "    callbacks = [metrics_cb, early_stopping]\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=cfg['max_epochs'],\n",
    "        accelerator='gpu', devices=1,\n",
    "        precision='16-mixed', \n",
    "        accumulate_grad_batches=2,\n",
    "        check_val_every_n_epoch=1,\n",
    "        num_sanity_val_steps=0,\n",
    "        enable_progress_bar=True,\n",
    "        log_every_n_steps=1,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, dm)\n",
    "    test_res = trainer.test(model, dm)[0]\n",
    "\n",
    "    model.test_results = test_res\n",
    "    model.finish_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model.epochs_trained = trainer.current_epoch + 1\n",
    "    model.save_model()\n",
    "\n",
    "    metrics_map = {\n",
    "        'accuracy':   ('train_accs',      'val_accs'),\n",
    "        'precision':  ('train_precisions','val_precisions'),\n",
    "        'recall':     ('train_recalls',   'val_recalls'),\n",
    "        'f1_score':   ('train_f1s',       'val_f1s'),\n",
    "    }\n",
    "    \n",
    "    for metric_name, (train_attr, val_attr) in metrics_map.items():\n",
    "        train_vals = getattr(metrics_cb, train_attr)\n",
    "        val_vals = getattr(metrics_cb, val_attr)\n",
    "        epochs = list(range(1, len(train_vals) + 1))\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(epochs, train_vals, label=f'train_{metric_name}')\n",
    "        plt.plot(epochs, val_vals,   label=f'val_{metric_name}')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(metric_name.replace('_', ' ').title())\n",
    "        plt.title(f\"{metric_name.replace('_', ' ').title()} over Epochs {model._last_timestamp}\")\n",
    "        plt.grid(True)\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "        plot_file = os.path.join(model._last_save_dir, f\"{model._last_timestamp}_{metric_name}.png\")\n",
    "        plt.savefig(plot_file)\n",
    "        plt.close()\n",
    "\n",
    "    print(f\"Completed {cfg['backbone']} ({'FT' if cfg['finetune'] else 'Frozen'}), artifacts in {model._last_save_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
