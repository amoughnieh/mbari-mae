{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zqEAfZZYIaiX"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "c:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\s3prl\\upstream\\byol_s\\byol_a\\common.py:20: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
            "  torchaudio.set_audio_backend(\"sox_io\")\n",
            "ESPnet is not installed, cannot use espnet_hubert upstream\n"
          ]
        }
      ],
      "source": [
        "# Standard library\n",
        "import datetime\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "from collections import Counter\n",
        "\n",
        "# Numerical computing\n",
        "import numpy as np\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchaudio\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Hugging Face Transformers\n",
        "from transformers import (\n",
        "    HubertModel,\n",
        "    Wav2Vec2FeatureExtractor,\n",
        "    Wav2Vec2Model,\n",
        ")\n",
        "\n",
        "# Datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "# PyTorch Lightning\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import EarlyStopping\n",
        "\n",
        "# Metrics\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from torchmetrics.classification import (\n",
        "    MulticlassF1Score,\n",
        "    MulticlassPrecision,\n",
        "    MulticlassRecall,\n",
        ")\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# MAE-AST Library\n",
        "from s3prl.nn.upstream import S3PRLUpstream, Featurizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "C5aQ8ZsGkD7s"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dataset: 1017 examples, 31 classes\n",
            "  Clymene_Dolphin: 38\n",
            "  Bottlenose_Dolphin: 15\n",
            "  Spinner_Dolphin: 69\n",
            "  Beluga,_White_Whale: 30\n",
            "  Bearded_Seal: 22\n",
            "  Minke_Whale: 10\n",
            "  Humpback_Whale: 38\n",
            "  Southern_Right_Whale: 15\n",
            "  White-sided_Dolphin: 33\n",
            "  Narwhal: 30\n",
            "  White-beaked_Dolphin: 34\n",
            "  Northern_Right_Whale: 32\n",
            "  Frasers_Dolphin: 52\n",
            "  Grampus,_Rissos_Dolphin: 40\n",
            "  Harp_Seal: 28\n",
            "  Atlantic_Spotted_Dolphin: 35\n",
            "  Fin,_Finback_Whale: 30\n",
            "  Ross_Seal: 30\n",
            "  Rough-Toothed_Dolphin: 30\n",
            "  Killer_Whale: 21\n",
            "  Pantropical_Spotted_Dolphin: 40\n",
            "  Short-Finned_Pacific_Pilot_Whale: 40\n",
            "  Bowhead_Whale: 36\n",
            "  False_Killer_Whale: 35\n",
            "  Melon_Headed_Whale: 38\n",
            "  Long-Finned_Pilot_Whale: 42\n",
            "  Striped_Dolphin: 49\n",
            "  Leopard_Seal: 6\n",
            "  Walrus: 23\n",
            "  Sperm_Whale: 45\n",
            "  Common_Dolphin: 31\n",
            "Validation dataset: 339 examples, 31 classes\n",
            "  Clymene_Dolphin: 12\n",
            "  Bottlenose_Dolphin: 5\n",
            "  Spinner_Dolphin: 23\n",
            "  Beluga,_White_Whale: 10\n",
            "  Bearded_Seal: 7\n",
            "  Minke_Whale: 4\n",
            "  Humpback_Whale: 13\n",
            "  Southern_Right_Whale: 5\n",
            "  White-sided_Dolphin: 11\n",
            "  Narwhal: 10\n",
            "  White-beaked_Dolphin: 11\n",
            "  Northern_Right_Whale: 11\n",
            "  Frasers_Dolphin: 18\n",
            "  Grampus,_Rissos_Dolphin: 13\n",
            "  Harp_Seal: 9\n",
            "  Atlantic_Spotted_Dolphin: 12\n",
            "  Fin,_Finback_Whale: 10\n",
            "  Ross_Seal: 10\n",
            "  Rough-Toothed_Dolphin: 10\n",
            "  Killer_Whale: 7\n",
            "  Pantropical_Spotted_Dolphin: 13\n",
            "  Short-Finned_Pacific_Pilot_Whale: 13\n",
            "  Bowhead_Whale: 12\n",
            "  False_Killer_Whale: 12\n",
            "  Melon_Headed_Whale: 13\n",
            "  Long-Finned_Pilot_Whale: 14\n",
            "  Striped_Dolphin: 16\n",
            "  Leopard_Seal: 2\n",
            "  Walrus: 8\n",
            "  Sperm_Whale: 15\n",
            "  Common_Dolphin: 10\n",
            "Test dataset: 339 examples, 31 classes\n",
            "  Clymene_Dolphin: 13\n",
            "  Bottlenose_Dolphin: 4\n",
            "  Spinner_Dolphin: 22\n",
            "  Beluga,_White_Whale: 10\n",
            "  Bearded_Seal: 8\n",
            "  Minke_Whale: 3\n",
            "  Humpback_Whale: 13\n",
            "  Southern_Right_Whale: 5\n",
            "  White-sided_Dolphin: 11\n",
            "  Narwhal: 10\n",
            "  White-beaked_Dolphin: 12\n",
            "  Northern_Right_Whale: 11\n",
            "  Frasers_Dolphin: 17\n",
            "  Grampus,_Rissos_Dolphin: 14\n",
            "  Harp_Seal: 10\n",
            "  Atlantic_Spotted_Dolphin: 11\n",
            "  Fin,_Finback_Whale: 10\n",
            "  Ross_Seal: 10\n",
            "  Rough-Toothed_Dolphin: 10\n",
            "  Killer_Whale: 7\n",
            "  Pantropical_Spotted_Dolphin: 13\n",
            "  Short-Finned_Pacific_Pilot_Whale: 14\n",
            "  Bowhead_Whale: 12\n",
            "  False_Killer_Whale: 12\n",
            "  Melon_Headed_Whale: 12\n",
            "  Long-Finned_Pilot_Whale: 14\n",
            "  Striped_Dolphin: 16\n",
            "  Leopard_Seal: 2\n",
            "  Walrus: 7\n",
            "  Sperm_Whale: 15\n",
            "  Common_Dolphin: 11\n"
          ]
        }
      ],
      "source": [
        "# Loading the dataset\n",
        "data_dir = \"data/watkins\"\n",
        "annotations_file_train = os.path.join(data_dir, \"annotations.train.csv\")\n",
        "annotations_file_valid = os.path.join(data_dir, \"annotations.valid.csv\")\n",
        "annotations_file_test = os.path.join(data_dir, \"annotations.test.csv\")\n",
        "\n",
        "ds = load_dataset(\n",
        "    \"csv\",\n",
        "    data_files={\"train\": annotations_file_train,\n",
        "                \"validation\": annotations_file_valid,\n",
        "                \"test\": annotations_file_test},\n",
        ")\n",
        "\n",
        "for split_name in [\"train\", \"validation\", \"test\"]:\n",
        "    split_dataset = ds[split_name]\n",
        "    labels = split_dataset[\"label\"]\n",
        "    total = len(labels)\n",
        "    counts = Counter(labels)\n",
        "\n",
        "    print(f\"{split_name.capitalize()} dataset: {total} examples, {len(counts)} classes\")\n",
        "    if \"label\" in split_dataset.features and hasattr(split_dataset.features[\"label\"], \"names\"):\n",
        "        class_names = split_dataset.features[\"label\"].names\n",
        "        for idx, name in enumerate(class_names):\n",
        "            print(f\"  {idx} ({name}): {counts.get(name, 0)}\")\n",
        "    else:\n",
        "        for label, count in counts.items():\n",
        "            print(f\"  {label}: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Class weights calculation\n",
        "train_labels = ds[\"train\"][\"label\"]\n",
        "unique_labels = sorted(set(train_labels))\n",
        "label_to_int = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "y_train = [label_to_int[lbl] for lbl in train_labels]\n",
        "\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.arange(len(unique_labels)),\n",
        "    y=y_train\n",
        ")\n",
        "\n",
        "num_classes = len(class_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JYR66KeIUsg"
      },
      "outputs": [],
      "source": [
        "# Model definition\n",
        "class WMMDClassifier(pl.LightningModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes: int,\n",
        "        lr: float = 1e-4,\n",
        "        backbone: str = \"facebook/wav2vec2-base\",\n",
        "        ckpt_path: str = \"\",\n",
        "        finetune: bool = False,\n",
        "        class_weights=None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        if backbone == \"facebook/wav2vec2-base\":\n",
        "            self.backbone     = Wav2Vec2Model.from_pretrained(backbone)\n",
        "            self.embedding_dim = self.backbone.config.hidden_size\n",
        "        \n",
        "        elif backbone.lower() == \"mae-ast\":\n",
        "            up_kwargs = {\"name\": \"mae_ast_patch\"}\n",
        "            if ckpt_path:\n",
        "                up_kwargs[\"path_or_url\"] = ckpt_path\n",
        "            s3 = S3PRLUpstream(**up_kwargs)\n",
        "\n",
        "            enc = s3.upstream.model.encoder\n",
        "            enc.layers = nn.ModuleList(list(enc.layers)[:4])\n",
        "            s3.upstream.model.dec_sine_pos_embed = None\n",
        "            s3.upstream.model.decoder = None\n",
        "            s3.upstream.model.final_proj_reconstruction = None\n",
        "            s3.upstream.model.final_proj_classification  = None\n",
        "\n",
        "            new_n = len(enc.layers) + 1\n",
        "            s3._num_layers       = new_n\n",
        "            s3._hidden_sizes     = s3._hidden_sizes[:new_n]\n",
        "            s3._downsample_rates = s3._downsample_rates[:new_n]\n",
        "\n",
        "            factor = 8                                            \n",
        "            s3._downsample_rates = [max(1, d // factor)           \n",
        "                                    for d in s3._downsample_rates]\n",
        "\n",
        "            print(\"hidden_states :\", len(s3._hidden_sizes))\n",
        "            print(\"_downsample_rates:\", s3._downsample_rates) \n",
        "\n",
        "            self.backbone      = s3\n",
        "            self.embedding_dim = s3.hidden_sizes[-1]\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported backbone '{backbone}'\")\n",
        "\n",
        "        try:\n",
        "            self.backbone.gradient_checkpointing_enable()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        for param in self.backbone.parameters():\n",
        "            param.requires_grad = finetune\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(self.embedding_dim, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(1024, num_classes),\n",
        "        )\n",
        "\n",
        "        if class_weights is not None:\n",
        "            cw = torch.tensor(class_weights, dtype=torch.float)\n",
        "            self.criterion = nn.CrossEntropyLoss(weight=cw)\n",
        "        else:\n",
        "            self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        metrics_kwargs = dict(num_classes=num_classes, average='macro')\n",
        "        self.train_precision = MulticlassPrecision(**metrics_kwargs)\n",
        "        self.train_recall = MulticlassRecall(**metrics_kwargs)\n",
        "        self.train_f1 = MulticlassF1Score(**metrics_kwargs)\n",
        "        self.val_precision = MulticlassPrecision(**metrics_kwargs)\n",
        "        self.val_recall = MulticlassRecall(**metrics_kwargs)\n",
        "        self.val_f1 = MulticlassF1Score(**metrics_kwargs)\n",
        "        self.test_precision = MulticlassPrecision(**metrics_kwargs)\n",
        "        self.test_recall = MulticlassRecall(**metrics_kwargs)\n",
        "        self.test_f1 = MulticlassF1Score(**metrics_kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        bname = self.hparams.backbone.lower()\n",
        "        if bname == \"facebook/wav2vec2-base\":\n",
        "            out = self.backbone(x)\n",
        "            hidden = out.last_hidden_state\n",
        "        elif bname == \"mae-ast\":\n",
        "            if x.dim() == 3:\n",
        "                x = x.squeeze(-1)\n",
        "            wav_lens = torch.full((x.size(0),), x.size(1),\n",
        "                                dtype=torch.long, device=x.device)\n",
        "            all_hs, _ = self.backbone(x, wav_lens)\n",
        "            hidden   = all_hs[-1]\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported backbone in forward(): '{self.hparams.backbone}'\")\n",
        "\n",
        "        emb = hidden.mean(dim=1)\n",
        "        return self.classifier(emb)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = self.criterion(logits, y)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        self.log_batch_metrics(loss, preds, y, prefix='train')\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = self.criterion(logits, y)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        self.log_batch_metrics(loss, preds, y, prefix='val')\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = self.criterion(logits, y)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        self.log_batch_metrics(loss, preds, y, prefix='test')\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.AdamW(self.parameters(), lr=self.hparams.lr)\n",
        "\n",
        "    def log_batch_metrics(self, loss, preds, targets, prefix):\n",
        "        self.log(f'{prefix}_loss', loss, prog_bar=True, on_epoch=True)\n",
        "        acc = (preds == targets).float().mean()\n",
        "        self.log(f'{prefix}_acc', acc, prog_bar=True, on_epoch=True)\n",
        "        precision = getattr(self, f'{prefix}_precision')(preds, targets)\n",
        "        recall = getattr(self, f'{prefix}_recall')(preds, targets)\n",
        "        f1 = getattr(self, f'{prefix}_f1')(preds, targets)\n",
        "        self.log(f'{prefix}_precision', precision, on_epoch=True)\n",
        "        self.log(f'{prefix}_recall', recall, on_epoch=True)\n",
        "        self.log(f'{prefix}_f1', f1, on_epoch=True)\n",
        "\n",
        "    # 2. Save the model state at the end of training\n",
        "    def on_train_end(self):\n",
        "        save_dir = getattr(self, 'save_dir', None)\n",
        "        if save_dir:\n",
        "            self.save_model(save_dir)\n",
        "\n",
        "    def save_model(self):\n",
        "        # New folder structure: /model/<model_name>_<balance_flag>/<timestamp>/\n",
        "        base_dir = 'model'\n",
        "        bn = self.hparams.backbone.replace('/', '_')\n",
        "        # Determine balance flag based on class_weights\n",
        "        cw = getattr(self.hparams, 'class_weights', None)\n",
        "        balance_flag = 'imbalance' if cw is not None else 'balance'\n",
        "        timestamp = getattr(self, 'finish_time', datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
        "        folder = os.path.join(base_dir, f\"{bn}_{balance_flag}\", timestamp)\n",
        "        os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "        # 1) checkpoint\n",
        "        ckpt_path = os.path.join(folder, f\"{timestamp}.pt\")\n",
        "        payload = {\n",
        "            'state_dict': self.state_dict(),\n",
        "            'hparams': dict(self.hparams)\n",
        "        }\n",
        "        for attr in ('test_results', 'finish_time', 'epochs_trained'):\n",
        "            if hasattr(self, attr):\n",
        "                payload[attr] = getattr(self, attr)\n",
        "        torch.save(payload, ckpt_path)\n",
        "\n",
        "        # 2) human-readable stats\n",
        "        stats_path = os.path.join(folder, f\"{timestamp}.txt\")\n",
        "        raw_hparams = dict(self.hparams)\n",
        "        serializable_hparams = {}\n",
        "        for k, v in raw_hparams.items():\n",
        "            if isinstance(v, np.ndarray):\n",
        "                serializable_hparams[k] = v.tolist()\n",
        "            elif isinstance(v, torch.Tensor):\n",
        "                serializable_hparams[k] = v.cpu().item() if v.ndim == 0 else v.cpu().tolist()\n",
        "            else:\n",
        "                serializable_hparams[k] = v\n",
        "\n",
        "        serializable_results = {}\n",
        "        if hasattr(self, 'test_results'):\n",
        "            for k, v in self.test_results.items():\n",
        "                serializable_results[k] = v.cpu().item() if isinstance(v, torch.Tensor) else v\n",
        "\n",
        "        with open(stats_path, 'w') as f:\n",
        "            f.write(f\"Model architecture:\\n{self}\\n\\n\")\n",
        "            f.write(\"Hyperparameters:\\n\")\n",
        "            f.write(json.dumps(serializable_hparams, indent=4))\n",
        "            f.write(\"\\n\\n\")\n",
        "            if serializable_results:\n",
        "                f.write(\"Test results:\\n\")\n",
        "                f.write(json.dumps(serializable_results, indent=4))\n",
        "                f.write(\"\\n\\n\")\n",
        "            if hasattr(self, 'epochs_trained'):\n",
        "                f.write(f\"Epochs trained: {self.epochs_trained}\\n\")\n",
        "\n",
        "        # store for downstream use\n",
        "        self._last_save_dir = folder\n",
        "        self._last_timestamp = timestamp\n",
        "        print(f\"Artifacts saved to {folder}/\")\n",
        "\n",
        "    # 3. Load back the model for inference or continuation\n",
        "    @classmethod\n",
        "    def load_model(cls, load_dir: str, map_location=None):\n",
        "        \"\"\"\n",
        "        Load a model checkpoint and hyperparameters from a directory.\n",
        "\n",
        "        Returns:\n",
        "            model (MammalClassifier): Loaded model\n",
        "        \"\"\"\n",
        "        # Load hyperparameters\n",
        "        hparams_path = os.path.join(load_dir, 'hparams.json')\n",
        "        with open(hparams_path, 'r') as f:\n",
        "            hparams = json.load(f)\n",
        "\n",
        "        # Instantiate model\n",
        "        model = cls(**hparams)\n",
        "        # Load checkpoint\n",
        "        ckpt_path = os.path.join(load_dir, f'{cls.__name__}.ckpt')\n",
        "        state = torch.load(ckpt_path, map_location=map_location)\n",
        "        model.load_state_dict(state['state_dict'])\n",
        "        return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hidden_states : 5\n",
            "_downsample_rates: [320, 320, 320, 320, 320]\n"
          ]
        }
      ],
      "source": [
        "model = WMMDClassifier(\n",
        "        num_classes=31, lr=1e-3,\n",
        "        backbone=\"mae-ast\", finetune=True,\n",
        "        class_weights=class_weights\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "WMMDClassifier(\n",
              "  (backbone): S3PRLUpstream(\n",
              "    (upstream): UpstreamExpert(\n",
              "      (model): MAE_AST(\n",
              "        (feature_extractor): Identity()\n",
              "        (post_extract_proj): Linear(in_features=256, out_features=768, bias=True)\n",
              "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (batch_norm): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
              "        (unfold): Unfold(kernel_size=(16, 16), dilation=1, padding=0, stride=(16, 16))\n",
              "        (dropout_input): Dropout(p=0.1, inplace=False)\n",
              "        (enc_sine_pos_embed): SinusoidalPositionalEncoding()\n",
              "        (encoder): TransformerEncoder(\n",
              "          (layers): ModuleList(\n",
              "            (0-3): 4 x TransformerSentenceEncoderLayer(\n",
              "              (self_attn): MultiheadAttention(\n",
              "                (dropout_module): FairseqDropout()\n",
              "                (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              )\n",
              "              (dropout1): Dropout(p=0.1, inplace=False)\n",
              "              (dropout2): Dropout(p=0.0, inplace=False)\n",
              "              (dropout3): Dropout(p=0.1, inplace=False)\n",
              "              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "          )\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (dec_sine_pos_embed): None\n",
              "        (decoder): None\n",
              "        (final_proj_reconstruction): None\n",
              "        (final_proj_classification): None\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.3, inplace=False)\n",
              "    (1): Linear(in_features=6144, out_features=1024, bias=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Dropout(p=0.3, inplace=False)\n",
              "    (4): Linear(in_features=1024, out_features=31, bias=True)\n",
              "  )\n",
              "  (criterion): CrossEntropyLoss()\n",
              "  (train_precision): MulticlassPrecision()\n",
              "  (train_recall): MulticlassRecall()\n",
              "  (train_f1): MulticlassF1Score()\n",
              "  (val_precision): MulticlassPrecision()\n",
              "  (val_recall): MulticlassRecall()\n",
              "  (val_f1): MulticlassF1Score()\n",
              "  (test_precision): MulticlassPrecision()\n",
              "  (test_recall): MulticlassRecall()\n",
              "  (test_f1): MulticlassF1Score()\n",
              ")"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ycDcNmcwQ_ON"
      },
      "outputs": [],
      "source": [
        "def WMMD_Collate(batch):\n",
        "    waveforms, labels = zip(*batch)\n",
        "    max_len = max(w.shape[0] for w in waveforms)\n",
        "    min_len = 5000\n",
        "    padded_len = max(max_len, min_len)\n",
        "\n",
        "    padded_waveforms = []\n",
        "    for waveform in waveforms:\n",
        "        padding_needed = padded_len - waveform.shape[0]\n",
        "        padded_waveform = torch.nn.functional.pad(waveform, (0, padding_needed))\n",
        "        padded_waveforms.append(padded_waveform)\n",
        "\n",
        "    padded = torch.stack(padded_waveforms, dim=0)\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "    return padded, labels\n",
        "\n",
        "class WMMDSoundDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset, backbone: str, target_sr: int = 2000):\n",
        "        \"\"\"\n",
        "        dataset: list of dicts with keys 'path' & 'label'\n",
        "        backbone: 'facebook/wav2vec2-base' or 'mae-ast'\n",
        "        target_sr: sampling rate (e.g. 2000)\n",
        "        \"\"\"\n",
        "        self.dataset = dataset\n",
        "        self.backbone = backbone.lower()\n",
        "        self.target_sr = target_sr\n",
        "        self.resampler_cache = {}\n",
        "\n",
        "        if self.backbone == \"facebook/wav2vec2-base\":\n",
        "            self.processor = Wav2Vec2FeatureExtractor.from_pretrained(\n",
        "                \"facebook/wav2vec2-base\", return_attention_mask=False, sampling_rate=target_sr\n",
        "            )\n",
        "        elif self.backbone == \"mae-ast\":\n",
        "            self.processor = None\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported backbone '{backbone}'\")\n",
        "\n",
        "        labels = sorted({item['label'] for item in dataset})\n",
        "        self.label_to_int = {lbl: i for i, lbl in enumerate(labels)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "        audio_path = item[\"path\"]\n",
        "        waveform, orig_sr = torchaudio.load(audio_path)\n",
        "\n",
        "        if orig_sr != self.target_sr:\n",
        "            if orig_sr not in self.resampler_cache:\n",
        "                self.resampler_cache[orig_sr] = torchaudio.transforms.Resample(orig_sr, self.target_sr)\n",
        "            waveform = self.resampler_cache[orig_sr](waveform)\n",
        "\n",
        "        waveform = waveform / (waveform.abs().max() + 1e-6)\n",
        "        wav_1d = waveform.squeeze(0)  \n",
        "        \n",
        "        if self.backbone == \"facebook/wav2vec2-base\":\n",
        "            arr = wav_1d.numpy()\n",
        "            feats = self.processor(arr, sampling_rate=self.target_sr, return_tensors=\"pt\")\n",
        "            inp = feats.input_values.squeeze(0)\n",
        "        elif self.backbone == \"mae-ast\":\n",
        "            # MAE-AST: feed raw waveform, expert converts internally\n",
        "            inp = wav_1d\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported backbone '{self.backbone}'\")\n",
        "\n",
        "        lbl = self.label_to_int[item['label']]\n",
        "        return inp, lbl\n",
        "\n",
        "class WMMDDataModule(pl.LightningDataModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dataset_dict,\n",
        "        backbone: str,\n",
        "        batch_size: int = 2,\n",
        "        num_workers: int = 1\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dataset_dict = dataset_dict\n",
        "        self.backbone = backbone   # ‚Üê store it here\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        # pass backbone into each split\n",
        "        self.train_ds = WMMDSoundDataset(self.dataset_dict[\"train\"], backbone=self.backbone)\n",
        "        self.val_ds   = WMMDSoundDataset(self.dataset_dict[\"validation\"], backbone=self.backbone)\n",
        "        self.test_ds  = WMMDSoundDataset(self.dataset_dict[\"test\"], backbone=self.backbone)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_ds,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=self.num_workers,\n",
        "            pin_memory=True,\n",
        "            collate_fn=WMMD_Collate\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.val_ds,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=self.num_workers,\n",
        "            pin_memory=True,\n",
        "            collate_fn=WMMD_Collate\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        # Use the test split for testing\n",
        "        return DataLoader(\n",
        "            self.test_ds,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=self.num_workers,\n",
        "            pin_memory=True,\n",
        "            collate_fn=WMMD_Collate\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "kQ2XWE-pRKAM"
      },
      "outputs": [],
      "source": [
        "#callback for logging metrics\n",
        "class MetricsLogger(pl.Callback):\n",
        "    def __init__(self):\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.train_accs = []\n",
        "        self.val_accs = []\n",
        "        self.train_precisions = []\n",
        "        self.val_precisions = []\n",
        "        self.train_recalls = []\n",
        "        self.val_recalls = []\n",
        "        self.train_f1s = []\n",
        "        self.val_f1s = []\n",
        "\n",
        "    def on_train_epoch_end(self, trainer, pl_module):\n",
        "        m = trainer.callback_metrics\n",
        "        self.train_losses.append(m['train_loss'].item())\n",
        "        self.train_accs.append(m['train_acc'].item())\n",
        "        self.train_precisions.append(m['train_precision'].item())\n",
        "        self.train_recalls.append(m['train_recall'].item())\n",
        "        self.train_f1s.append(m['train_f1'].item())\n",
        "\n",
        "    def on_validation_epoch_end(self, trainer, pl_module):\n",
        "        m = trainer.callback_metrics\n",
        "        self.val_losses.append(m['val_loss'].item())\n",
        "        self.val_accs.append(m['val_acc'].item())\n",
        "        self.val_precisions.append(m['val_precision'].item())\n",
        "        self.val_recalls.append(m['val_recall'].item())\n",
        "        self.val_f1s.append(m['val_f1'].item())\n",
        "\n",
        "# callback for early stopping\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    patience=10,\n",
        "    min_delta=0.01,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model configurations\n",
        "model_configs = [\n",
        "    # {\"num_classes\": num_classes, \"lr\": 1e-3, \"backbone\": \"facebook/wav2vec2-base\",  \"finetune\": False, \"class_weights\": class_weights, \"max_epochs\": 3, \"ckpt_path\": \"\"},\n",
        "    {\"num_classes\": num_classes, \"lr\": 1e-3, \"backbone\": \"mae-ast\",  \"finetune\": False, \"class_weights\": class_weights, \"max_epochs\": 3, \"ckpt_path\": \"4Enc_1Dec-61epoch-0.103loss.pt\"},\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9k7VBnf5TWfW"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using 16bit Automatic Mixed Precision (AMP)\n",
            "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "You are using a CUDA device ('NVIDIA RTX 2000 Ada Generation Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hidden_states : 5\n",
            "_downsample_rates: [320, 320, 320, 320, 320]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "   | Name            | Type                | Params | Mode \n",
            "-----------------------------------------------------------------\n",
            "0  | backbone        | S3PRLUpstream       | 28.6 M | train\n",
            "1  | classifier      | Sequential          | 6.3 M  | train\n",
            "2  | criterion       | CrossEntropyLoss    | 0      | train\n",
            "3  | train_precision | MulticlassPrecision | 0      | train\n",
            "4  | train_recall    | MulticlassRecall    | 0      | train\n",
            "5  | train_f1        | MulticlassF1Score   | 0      | train\n",
            "6  | val_precision   | MulticlassPrecision | 0      | train\n",
            "7  | val_recall      | MulticlassRecall    | 0      | train\n",
            "8  | val_f1          | MulticlassF1Score   | 0      | train\n",
            "9  | test_precision  | MulticlassPrecision | 0      | train\n",
            "10 | test_recall     | MulticlassRecall    | 0      | train\n",
            "11 | test_f1         | MulticlassF1Score   | 0      | train\n",
            "-----------------------------------------------------------------\n",
            "6.3 M     Trainable params\n",
            "28.6 M    Non-trainable params\n",
            "34.9 M    Total params\n",
            "139.506   Total estimated model params size (MB)\n",
            "85        Modules in train mode\n",
            "0         Modules in eval mode\n",
            "c:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=21` in the `DataLoader` to improve performance.\n",
            "c:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=21` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0:   0%|          | 0/509 [00:00<?, ?it/s] "
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "4, 5",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     18\u001b[39m trainer = pl.Trainer(\n\u001b[32m     19\u001b[39m     max_epochs=cfg[\u001b[33m'\u001b[39m\u001b[33mmax_epochs\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     20\u001b[39m     accelerator=\u001b[33m'\u001b[39m\u001b[33mgpu\u001b[39m\u001b[33m'\u001b[39m, devices=\u001b[32m1\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m     callbacks=callbacks\n\u001b[32m     26\u001b[39m )\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Train and test\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m test_res = trainer.test(model, dm)[\u001b[32m0\u001b[39m]\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Attach metadata and save\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:561\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:48\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.strategy.launcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[32m     51\u001b[39m     _call_teardown_hook(trainer)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:599\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    592\u001b[39m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    593\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    594\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    595\u001b[39m     ckpt_path,\n\u001b[32m    596\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    597\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    598\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n\u001b[32m    602\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1012\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1007\u001b[39m \u001b[38;5;28mself\u001b[39m._signal_connector.register_signal_handlers()\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1016\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1017\u001b[39m log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: trainer tearing down\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1056\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1054\u001b[39m         \u001b[38;5;28mself\u001b[39m._run_sanity_check()\n\u001b[32m   1055\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m-> \u001b[39m\u001b[32m1056\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1057\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1058\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.state\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:216\u001b[39m, in \u001b[36m_FitLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    215\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_start()\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_end()\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:455\u001b[39m, in \u001b[36m_FitLoop.advance\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.trainer.profiler.profile(\u001b[33m\"\u001b[39m\u001b[33mrun_training_epoch\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepoch_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:152\u001b[39m, in \u001b[36m_TrainingEpochLoop.run\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done:\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m         \u001b[38;5;28mself\u001b[39m.on_advance_end(data_fetcher)\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:344\u001b[39m, in \u001b[36m_TrainingEpochLoop.advance\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33m\"\u001b[39m\u001b[33mrun_training_batch\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    342\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.lightning_module.automatic_optimization:\n\u001b[32m    343\u001b[39m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m         batch_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mautomatic_optimization\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    345\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    346\u001b[39m         batch_output = \u001b[38;5;28mself\u001b[39m.manual_optimization.run(kwargs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:185\u001b[39m, in \u001b[36m_AutomaticOptimization.run\u001b[39m\u001b[34m(self, optimizer, batch_idx, kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    175\u001b[39m     \u001b[38;5;66;03m# when the strategy handles accumulation, we want to always call the optimizer step\u001b[39;00m\n\u001b[32m    176\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.trainer.strategy.handles_gradient_accumulation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.trainer.fit_loop._should_accumulate()\n\u001b[32m   (...)\u001b[39m\u001b[32m    182\u001b[39m     \u001b[38;5;66;03m# -------------------\u001b[39;00m\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# automatic_optimization=True: perform ddp sync only when performing optimizer_step\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m _block_parallel_sync_behavior(\u001b[38;5;28mself\u001b[39m.trainer.strategy, block=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m         \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    192\u001b[39m     \u001b[38;5;28mself\u001b[39m._optimizer_step(batch_idx, closure)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:146\u001b[39m, in \u001b[36mClosure.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> Optional[Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     \u001b[38;5;28mself\u001b[39m._result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result.loss\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:131\u001b[39m, in \u001b[36mClosure.closure\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;129m@torch\u001b[39m.enable_grad()\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> ClosureResult:\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m step_output.closure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    134\u001b[39m         \u001b[38;5;28mself\u001b[39m.warning_cache.warn(\u001b[33m\"\u001b[39m\u001b[33m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:319\u001b[39m, in \u001b[36m_AutomaticOptimization._training_step\u001b[39m\u001b[34m(self, kwargs)\u001b[39m\n\u001b[32m    308\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Performs the actual train step with the tied hooks.\u001b[39;00m\n\u001b[32m    309\u001b[39m \n\u001b[32m    310\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    315\u001b[39m \n\u001b[32m    316\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    317\u001b[39m trainer = \u001b[38;5;28mself\u001b[39m.trainer\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m training_step_output = \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtraining_step\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer.strategy.post_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m training_step_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m trainer.world_size > \u001b[32m1\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:328\u001b[39m, in \u001b[36m_call_strategy_hook\u001b[39m\u001b[34m(trainer, hook_name, *args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer.strategy.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[32m    331\u001b[39m pl_module._current_fx_name = prev_fx_name\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:391\u001b[39m, in \u001b[36mStrategy.training_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model != \u001b[38;5;28mself\u001b[39m.lightning_module:\n\u001b[32m    390\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_redirection(\u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.lightning_module, \u001b[33m\"\u001b[39m\u001b[33mtraining_step\u001b[39m\u001b[33m\"\u001b[39m, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m391\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlightning_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 103\u001b[39m, in \u001b[36mWMMDClassifier.training_step\u001b[39m\u001b[34m(self, batch, batch_idx)\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[32m    102\u001b[39m     x, y = batch\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m     logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    104\u001b[39m     loss = \u001b[38;5;28mself\u001b[39m.criterion(logits, y)\n\u001b[32m    105\u001b[39m     preds = logits.argmax(dim=\u001b[32m1\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 93\u001b[39m, in \u001b[36mWMMDClassifier.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     90\u001b[39m         x = x.squeeze(-\u001b[32m1\u001b[39m)\n\u001b[32m     91\u001b[39m     wav_lens = torch.full((x.size(\u001b[32m0\u001b[39m),), x.size(\u001b[32m1\u001b[39m),\n\u001b[32m     92\u001b[39m                         dtype=torch.long, device=x.device)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     all_hs, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwav_lens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m     hidden   = all_hs[-\u001b[32m1\u001b[39m]\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\s3prl\\nn\\upstream.py:212\u001b[39m, in \u001b[36mS3PRLUpstream.forward\u001b[39m\u001b[34m(self, wavs, wavs_len)\u001b[39m\n\u001b[32m    209\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.upstream(wavs_list)[\u001b[33m\"\u001b[39m\u001b[33mhidden_states\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(hidden_states, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m))\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m     \u001b[38;5;28mlen\u001b[39m(hidden_states) == \u001b[38;5;28mself\u001b[39m.num_layers\n\u001b[32m    213\u001b[39m ), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(hidden_states)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.num_layers\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    215\u001b[39m max_wav_len = \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mmax\u001b[39m(wavs_len))\n\u001b[32m    216\u001b[39m all_hs = []\n",
            "\u001b[31mAssertionError\u001b[39m: 4, 5"
          ]
        }
      ],
      "source": [
        "# Training Loops\n",
        "for cfg in model_configs:\n",
        "    dm = WMMDDataModule(\n",
        "        dataset_dict=ds,\n",
        "        backbone=cfg[\"backbone\"],\n",
        "        batch_size=2,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    model = WMMDClassifier(\n",
        "        num_classes=cfg['num_classes'], lr=cfg['lr'],\n",
        "        backbone=cfg['backbone'], finetune=cfg['finetune'],\n",
        "        class_weights=cfg['class_weights']\n",
        "    )\n",
        "    metrics_cb = MetricsLogger()\n",
        "    callbacks = [metrics_cb, early_stopping]\n",
        "\n",
        "    trainer = pl.Trainer(\n",
        "        max_epochs=cfg['max_epochs'],\n",
        "        accelerator='gpu', devices=1,\n",
        "        precision='16-mixed', accumulate_grad_batches=2,\n",
        "        check_val_every_n_epoch=1,\n",
        "        num_sanity_val_steps=0,\n",
        "        enable_progress_bar=True, log_every_n_steps=1,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    # Train and test\n",
        "    trainer.fit(model, dm)\n",
        "    test_res = trainer.test(model, dm)[0]\n",
        "\n",
        "    # Attach metadata and save\n",
        "    model.test_results = test_res\n",
        "    model.finish_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    model.epochs_trained = trainer.current_epoch + 1\n",
        "    model.save_model()\n",
        "\n",
        "    # Plot train/val curves\n",
        "    metrics_map = {\n",
        "        'accuracy':   ('train_accs',      'val_accs'),\n",
        "        'precision':  ('train_precisions','val_precisions'),\n",
        "        'recall':     ('train_recalls',   'val_recalls'),\n",
        "        'f1_score':   ('train_f1s',       'val_f1s'),\n",
        "    }\n",
        "    \n",
        "    for metric_name, (train_attr, val_attr) in metrics_map.items():\n",
        "        train_vals = getattr(metrics_cb, train_attr)\n",
        "        val_vals = getattr(metrics_cb, val_attr)\n",
        "        epochs = list(range(1, len(train_vals) + 1))\n",
        "\n",
        "        plt.figure()\n",
        "        plt.plot(epochs, train_vals, label=f'train_{metric_name}')\n",
        "        plt.plot(epochs, val_vals,   label=f'val_{metric_name}')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel(metric_name.replace('_', ' ').title())\n",
        "        plt.title(f\"{metric_name.replace('_', ' ').title()} over Epochs {model._last_timestamp}\")\n",
        "        plt.grid(True)\n",
        "        plt.legend(loc='best')\n",
        "\n",
        "        plot_file = os.path.join(model._last_save_dir, f\"{model._last_timestamp}_{metric_name}.png\")\n",
        "        plt.savefig(plot_file)\n",
        "        plt.close()\n",
        "\n",
        "    print(f\"Completed {cfg['backbone']} ({'FT' if cfg['finetune'] else 'Frozen'}), artifacts in {model._last_save_dir}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "wmmd_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
