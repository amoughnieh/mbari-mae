{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zqEAfZZYIaiX"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "c:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\s3prl\\upstream\\byol_s\\byol_a\\common.py:20: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
            "  torchaudio.set_audio_backend(\"sox_io\")\n",
            "ESPnet is not installed, cannot use espnet_hubert upstream\n"
          ]
        }
      ],
      "source": [
        "# Standard library\n",
        "import datetime\n",
        "import json\n",
        "import os\n",
        "from collections import Counter\n",
        "import random\n",
        "\n",
        "# Numerical computing\n",
        "import numpy as np\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchaudio\n",
        "from torch.utils.data import DataLoader\n",
        "import torchaudio.transforms as T\n",
        "import torchaudio.functional as F\n",
        "\n",
        "# Datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "# PyTorch Lightning\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import EarlyStopping\n",
        "from pytorch_lightning.utilities.model_summary import ModelSummary\n",
        "\n",
        "# Metrics\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from torchmetrics.classification import (\n",
        "    MulticlassF1Score,\n",
        "    MulticlassPrecision,\n",
        "    MulticlassRecall,\n",
        ")\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ==== BACKBONE ====\n",
        "# Hugging Face Transformers\n",
        "from transformers import (\n",
        "    Wav2Vec2FeatureExtractor,\n",
        "    Wav2Vec2Model,\n",
        "    ViTFeatureExtractor, \n",
        "    ViTModel\n",
        ")\n",
        "\n",
        "# MAE-AST Library\n",
        "from s3prl.nn.upstream import S3PRLUpstream\n",
        "\n",
        "# Fix the length of the input audio to the same length\n",
        "def _match_length_force(self, xs, target_max_len):\n",
        "    xs_max_len = xs.size(1)\n",
        "    if xs_max_len > target_max_len:\n",
        "        xs = xs[:, :target_max_len, :]\n",
        "    elif xs_max_len < target_max_len:\n",
        "        pad_len = target_max_len - xs_max_len\n",
        "        xs = torch.cat(\n",
        "            (xs, xs[:, -1:, :].repeat(1, pad_len, 1)),\n",
        "            dim=1\n",
        "        )\n",
        "    return xs\n",
        "\n",
        "S3PRLUpstream._match_length = _match_length_force"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "C5aQ8ZsGkD7s"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dataset: 1017 examples, 31 classes\n",
            "  Clymene_Dolphin: 38\n",
            "  Bottlenose_Dolphin: 15\n",
            "  Spinner_Dolphin: 69\n",
            "  Beluga,_White_Whale: 30\n",
            "  Bearded_Seal: 22\n",
            "  Minke_Whale: 10\n",
            "  Humpback_Whale: 38\n",
            "  Southern_Right_Whale: 15\n",
            "  White-sided_Dolphin: 33\n",
            "  Narwhal: 30\n",
            "  White-beaked_Dolphin: 34\n",
            "  Northern_Right_Whale: 32\n",
            "  Frasers_Dolphin: 52\n",
            "  Grampus,_Rissos_Dolphin: 40\n",
            "  Harp_Seal: 28\n",
            "  Atlantic_Spotted_Dolphin: 35\n",
            "  Fin,_Finback_Whale: 30\n",
            "  Ross_Seal: 30\n",
            "  Rough-Toothed_Dolphin: 30\n",
            "  Killer_Whale: 21\n",
            "  Pantropical_Spotted_Dolphin: 40\n",
            "  Short-Finned_Pacific_Pilot_Whale: 40\n",
            "  Bowhead_Whale: 36\n",
            "  False_Killer_Whale: 35\n",
            "  Melon_Headed_Whale: 38\n",
            "  Long-Finned_Pilot_Whale: 42\n",
            "  Striped_Dolphin: 49\n",
            "  Leopard_Seal: 6\n",
            "  Walrus: 23\n",
            "  Sperm_Whale: 45\n",
            "  Common_Dolphin: 31\n",
            "Validation dataset: 339 examples, 31 classes\n",
            "  Clymene_Dolphin: 12\n",
            "  Bottlenose_Dolphin: 5\n",
            "  Spinner_Dolphin: 23\n",
            "  Beluga,_White_Whale: 10\n",
            "  Bearded_Seal: 7\n",
            "  Minke_Whale: 4\n",
            "  Humpback_Whale: 13\n",
            "  Southern_Right_Whale: 5\n",
            "  White-sided_Dolphin: 11\n",
            "  Narwhal: 10\n",
            "  White-beaked_Dolphin: 11\n",
            "  Northern_Right_Whale: 11\n",
            "  Frasers_Dolphin: 18\n",
            "  Grampus,_Rissos_Dolphin: 13\n",
            "  Harp_Seal: 9\n",
            "  Atlantic_Spotted_Dolphin: 12\n",
            "  Fin,_Finback_Whale: 10\n",
            "  Ross_Seal: 10\n",
            "  Rough-Toothed_Dolphin: 10\n",
            "  Killer_Whale: 7\n",
            "  Pantropical_Spotted_Dolphin: 13\n",
            "  Short-Finned_Pacific_Pilot_Whale: 13\n",
            "  Bowhead_Whale: 12\n",
            "  False_Killer_Whale: 12\n",
            "  Melon_Headed_Whale: 13\n",
            "  Long-Finned_Pilot_Whale: 14\n",
            "  Striped_Dolphin: 16\n",
            "  Leopard_Seal: 2\n",
            "  Walrus: 8\n",
            "  Sperm_Whale: 15\n",
            "  Common_Dolphin: 10\n",
            "Test dataset: 339 examples, 31 classes\n",
            "  Clymene_Dolphin: 13\n",
            "  Bottlenose_Dolphin: 4\n",
            "  Spinner_Dolphin: 22\n",
            "  Beluga,_White_Whale: 10\n",
            "  Bearded_Seal: 8\n",
            "  Minke_Whale: 3\n",
            "  Humpback_Whale: 13\n",
            "  Southern_Right_Whale: 5\n",
            "  White-sided_Dolphin: 11\n",
            "  Narwhal: 10\n",
            "  White-beaked_Dolphin: 12\n",
            "  Northern_Right_Whale: 11\n",
            "  Frasers_Dolphin: 17\n",
            "  Grampus,_Rissos_Dolphin: 14\n",
            "  Harp_Seal: 10\n",
            "  Atlantic_Spotted_Dolphin: 11\n",
            "  Fin,_Finback_Whale: 10\n",
            "  Ross_Seal: 10\n",
            "  Rough-Toothed_Dolphin: 10\n",
            "  Killer_Whale: 7\n",
            "  Pantropical_Spotted_Dolphin: 13\n",
            "  Short-Finned_Pacific_Pilot_Whale: 14\n",
            "  Bowhead_Whale: 12\n",
            "  False_Killer_Whale: 12\n",
            "  Melon_Headed_Whale: 12\n",
            "  Long-Finned_Pilot_Whale: 14\n",
            "  Striped_Dolphin: 16\n",
            "  Leopard_Seal: 2\n",
            "  Walrus: 7\n",
            "  Sperm_Whale: 15\n",
            "  Common_Dolphin: 11\n"
          ]
        }
      ],
      "source": [
        "# loading the dataset\n",
        "data_dir = \"data/watkins\"\n",
        "annotations_file_train = os.path.join(data_dir, \"annotations.train.csv\")\n",
        "annotations_file_valid = os.path.join(data_dir, \"annotations.valid.csv\")\n",
        "annotations_file_test = os.path.join(data_dir, \"annotations.test.csv\")\n",
        "\n",
        "ds = load_dataset(\n",
        "    \"csv\",\n",
        "    data_files={\"train\": annotations_file_train,\n",
        "                \"validation\": annotations_file_valid,\n",
        "                \"test\": annotations_file_test},\n",
        ")\n",
        "\n",
        "for split_name in [\"train\", \"validation\", \"test\"]:\n",
        "    split_dataset = ds[split_name]\n",
        "    labels = split_dataset[\"label\"]\n",
        "    total = len(labels)\n",
        "    counts = Counter(labels)\n",
        "\n",
        "    print(f\"{split_name.capitalize()} dataset: {total} examples, {len(counts)} classes\")\n",
        "    if \"label\" in split_dataset.features and hasattr(split_dataset.features[\"label\"], \"names\"):\n",
        "        class_names = split_dataset.features[\"label\"].names\n",
        "        for idx, name in enumerate(class_names):\n",
        "            print(f\"  {idx} ({name}): {counts.get(name, 0)}\")\n",
        "    else:\n",
        "        for label, count in counts.items():\n",
        "            print(f\"  {label}: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# class weights calculation\n",
        "train_labels = ds[\"train\"][\"label\"]\n",
        "unique_labels = sorted(set(train_labels))\n",
        "label_to_int = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "y_train = [label_to_int[lbl] for lbl in train_labels]\n",
        "\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.arange(len(unique_labels)),\n",
        "    y=y_train\n",
        ")\n",
        "\n",
        "num_classes = len(class_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6JYR66KeIUsg"
      },
      "outputs": [],
      "source": [
        "# model definition\n",
        "class WMMDClassifier(pl.LightningModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes: int,\n",
        "        lr: float = 1e-3,\n",
        "        backbone: str = \"facebook/wav2vec2-base\",\n",
        "        ckpt_path: str = \"\",\n",
        "        finetune: bool = False,\n",
        "        class_weights=None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        # ========== WAV2VEC2 ==========\n",
        "        if backbone == \"facebook/wav2vec2-base\":\n",
        "            self.backbone     = Wav2Vec2Model.from_pretrained(backbone)\n",
        "            self.embedding_dim = self.backbone.config.hidden_size\n",
        "        \n",
        "        # ========== TINY WAV2VEC2 ==========\n",
        "        elif backbone == \"patrickvonplaten/tiny-wav2vec2-no-tokenizer\":\n",
        "            self.backbone     = Wav2Vec2Model.from_pretrained(backbone)\n",
        "            self.embedding_dim = self.backbone.config.hidden_size\n",
        "        \n",
        "        # ========== MAE-AST ==========\n",
        "        elif backbone == \"mae-ast\":\n",
        "            up_kwargs = {\"name\": \"mae_ast_patch\"}\n",
        "            s3 = S3PRLUpstream(**up_kwargs)\n",
        "\n",
        "            enc = s3.upstream.model.encoder\n",
        "            enc.layers = nn.ModuleList(list(enc.layers)[:4])\n",
        "            s3.upstream.model.dec_sine_pos_embed = None\n",
        "            s3.upstream.model.decoder = None\n",
        "            s3.upstream.model.final_proj_reconstruction = None\n",
        "            s3.upstream.model.final_proj_classification  = None\n",
        "\n",
        "            new_n = len(enc.layers)\n",
        "            s3._num_layers       = new_n\n",
        "            s3._hidden_sizes     = s3._hidden_sizes[:new_n]\n",
        "            s3._downsample_rates = s3._downsample_rates[:new_n]\n",
        "\n",
        "            self.backbone      = s3\n",
        "            self.embedding_dim = s3.hidden_sizes[-1]\n",
        "\n",
        "            # Load the checkpoint for mae ast\n",
        "            if ckpt_path:\n",
        "                self.load_mae_ckpt(ckpt_path)\n",
        "            \n",
        "            mel_transform = T.MelSpectrogram(\n",
        "                sample_rate=2000, \n",
        "                n_fft=1024, \n",
        "                win_length=512,\n",
        "                hop_length=20, \n",
        "                n_mels=128,\n",
        "            )\n",
        "\n",
        "            class DBWithDeltas(nn.Module):\n",
        "                def __init__(self):\n",
        "                    super().__init__()\n",
        "\n",
        "                def forward(self, spec):\n",
        "                    spec_db = F.amplitude_to_DB(\n",
        "                        spec,\n",
        "                        multiplier=10.0,\n",
        "                        amin=1e-10,\n",
        "                        db_multiplier=0\n",
        "                    )\n",
        "                    t = spec_db.transpose(0, 1)\n",
        "                    d1 = F.compute_deltas(t.transpose(0,1))\n",
        "                    d2 = F.compute_deltas(d1)\n",
        "                    return torch.cat([\n",
        "                        t,\n",
        "                        d1.transpose(0,1),\n",
        "                        d2.transpose(0,1)\n",
        "                    ], dim=1)\n",
        "            \n",
        "            s3.upstream.model.fbank   = mel_transform\n",
        "            s3.upstream.model.db_norm = DBWithDeltas()\n",
        "        \n",
        "        # ========== CNN-SPECTROGRAM ==========\n",
        "        elif backbone == \"cnn-spect\":\n",
        "            self.mel_spec = torchaudio.transforms.MelSpectrogram(\n",
        "                sample_rate=2000,\n",
        "                n_fft=1024,\n",
        "                hop_length=20,\n",
        "                win_length=512,\n",
        "                n_mels=128\n",
        "            )\n",
        "            self.to_db = torchaudio.transforms.AmplitudeToDB(top_db=80)\n",
        "\n",
        "            self.backbone = nn.Sequential(\n",
        "                nn.Conv2d(3, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(),\n",
        "                nn.Conv2d(32,32,3,padding=1), nn.BatchNorm2d(32), nn.ReLU(),\n",
        "                nn.MaxPool2d(2),\n",
        "\n",
        "                nn.Conv2d(32,64,3,padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
        "                nn.Conv2d(64,64,3,padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
        "                nn.MaxPool2d(2),\n",
        "\n",
        "                nn.Conv2d(64,128,3,padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
        "                nn.Conv2d(128,128,3,padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
        "                nn.MaxPool2d(2),\n",
        "\n",
        "                nn.Conv2d(128,256,3,padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n",
        "                nn.Conv2d(256,256,3,padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n",
        "                nn.MaxPool2d(2),\n",
        "\n",
        "                nn.Conv2d(256,512,3,padding=1), nn.BatchNorm2d(512), nn.ReLU(),\n",
        "                nn.Conv2d(512,512,3,padding=1), nn.BatchNorm2d(512), nn.ReLU(),\n",
        "                nn.AdaptiveAvgPool2d((1,1)),\n",
        "                nn.Flatten()\n",
        "            )\n",
        "            self.embedding_dim = 512\n",
        "        \n",
        "        # ========== VIT-IMAGENET ==========\n",
        "        elif backbone == \"vit-imagenet\":\n",
        "            self.mel_spec = torchaudio.transforms.MelSpectrogram(\n",
        "                sample_rate=2000,\n",
        "                n_fft=1024,\n",
        "                hop_length=20,\n",
        "                win_length=512,\n",
        "                n_mels=128\n",
        "            )\n",
        "            self.to_db = torchaudio.transforms.AmplitudeToDB(top_db=80)\n",
        "\n",
        "            self.feature_extractor = ViTFeatureExtractor.from_pretrained(\n",
        "                \"google/vit-base-patch16-224-in21k\"\n",
        "            )\n",
        "\n",
        "            self.backbone = ViTModel.from_pretrained(\n",
        "                \"google/vit-base-patch16-224-in21k\",\n",
        "                add_pooling_layer=False\n",
        "            )\n",
        "            self.embedding_dim = self.backbone.config.hidden_size\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported backbone '{backbone}'\")\n",
        "\n",
        "        try:\n",
        "            self.backbone.gradient_checkpointing_enable()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        for param in self.backbone.parameters():\n",
        "            param.requires_grad = finetune\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(self.embedding_dim, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(1024, num_classes),\n",
        "        )\n",
        "\n",
        "        if class_weights is not None:\n",
        "            cw = torch.tensor(class_weights, dtype=torch.float)\n",
        "            self.criterion = nn.CrossEntropyLoss(weight=cw)\n",
        "        else:\n",
        "            self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        metrics_kwargs = dict(num_classes=num_classes, average='macro')\n",
        "        self.train_precision = MulticlassPrecision(**metrics_kwargs)\n",
        "        self.train_recall = MulticlassRecall(**metrics_kwargs)\n",
        "        self.train_f1 = MulticlassF1Score(**metrics_kwargs)\n",
        "        self.val_precision = MulticlassPrecision(**metrics_kwargs)\n",
        "        self.val_recall = MulticlassRecall(**metrics_kwargs)\n",
        "        self.val_f1 = MulticlassF1Score(**metrics_kwargs)\n",
        "        self.test_precision = MulticlassPrecision(**metrics_kwargs)\n",
        "        self.test_recall = MulticlassRecall(**metrics_kwargs)\n",
        "        self.test_f1 = MulticlassF1Score(**metrics_kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        bname = self.hparams.backbone.lower()\n",
        "        if bname in {\"facebook/wav2vec2-base\",\n",
        "             \"patrickvonplaten/tiny-wav2vec2-no-tokenizer\"}:\n",
        "            hidden = self.backbone(x).last_hidden_state\n",
        "\n",
        "        elif bname == \"mae-ast\":\n",
        "            if x.dim() == 3:\n",
        "                x = x.squeeze(-1)\n",
        "            wav_lens = torch.full((x.size(0),), x.size(1),\n",
        "                                dtype=torch.long, device=x.device)\n",
        "            hidden = self.backbone(x, wav_lens)[0][-1]\n",
        "\n",
        "        elif bname == \"cnn-spect\":\n",
        "            spec = self._safe_logmelspec(x, self.mel_spec, self.to_db)  # (B,1,F,L) in [0,1]\n",
        "            hidden = self.backbone(spec)\n",
        "\n",
        "        elif bname == \"vit-imagenet\":\n",
        "            spec = self._safe_logmelspec(x, self.mel_spec, self.to_db)  # (B,1,F,L) in [0,1]\n",
        "\n",
        "            img = torch.nn.functional.interpolate(\n",
        "                spec, size=(224, 224), mode=\"bilinear\", align_corners=False\n",
        "            )\n",
        "\n",
        "            pixel_values = self.feature_extractor(images=img, return_tensors=\"pt\").pixel_values.to(img.device)\n",
        "            outputs = self.backbone(pixel_values=pixel_values)\n",
        "            hidden = outputs.last_hidden_state.mean(dim=1)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported backbone '{self.hparams.backbone}'\")\n",
        "\n",
        "        emb = hidden if hidden.dim() == 2 else hidden.mean(dim=1)\n",
        "        return self.classifier(emb)\n",
        "\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = self.criterion(logits, y)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        self.log_batch_metrics(loss, preds, y, prefix='train')\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = self.criterion(logits, y)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        self.log_batch_metrics(loss, preds, y, prefix='val')\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = self.criterion(logits, y)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        self.log_batch_metrics(loss, preds, y, prefix='test')\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.AdamW(self.parameters(), lr=self.hparams.lr)\n",
        "\n",
        "    def log_batch_metrics(self, loss, preds, targets, prefix):\n",
        "        self.log(f'{prefix}_loss', loss, prog_bar=True, on_epoch=True)\n",
        "        acc = (preds == targets).float().mean()\n",
        "        self.log(f'{prefix}_acc', acc, prog_bar=True, on_epoch=True)\n",
        "        precision = getattr(self, f'{prefix}_precision')(preds, targets)\n",
        "        recall = getattr(self, f'{prefix}_recall')(preds, targets)\n",
        "        f1 = getattr(self, f'{prefix}_f1')(preds, targets)\n",
        "        self.log(f'{prefix}_precision', precision, on_epoch=True)\n",
        "        self.log(f'{prefix}_recall', recall, on_epoch=True)\n",
        "        self.log(f'{prefix}_f1', f1, on_epoch=True)\n",
        "\n",
        "    def on_train_end(self):\n",
        "        save_dir = getattr(self, 'save_dir', None)\n",
        "        if save_dir:\n",
        "            self.save_model(save_dir)\n",
        "\n",
        "    def save_model(self):\n",
        "        base_dir = 'model'\n",
        "        bn = self.hparams.backbone.replace('/', '_')\n",
        "        cw = getattr(self.hparams, 'class_weights', None)\n",
        "        balance_flag = 'imbalance' if cw is not None else 'balance'\n",
        "        timestamp = getattr(self, 'finish_time', datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
        "        folder = os.path.join(base_dir, f\"{bn}_{balance_flag}\", timestamp)\n",
        "        os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "        ckpt_path = os.path.join(folder, f\"{timestamp}.pt\")\n",
        "        payload = {\n",
        "            'state_dict': self.state_dict(),\n",
        "            'hparams': dict(self.hparams)\n",
        "        }\n",
        "        for attr in ('test_results', 'finish_time', 'epochs_trained'):\n",
        "            if hasattr(self, attr):\n",
        "                payload[attr] = getattr(self, attr)\n",
        "        torch.save(payload, ckpt_path)\n",
        "\n",
        "        stats_path = os.path.join(folder, f\"{timestamp}.txt\")\n",
        "        raw_hparams = dict(self.hparams)\n",
        "        serializable_hparams = {}\n",
        "        for k, v in raw_hparams.items():\n",
        "            if isinstance(v, np.ndarray):\n",
        "                serializable_hparams[k] = v.tolist()\n",
        "            elif isinstance(v, torch.Tensor):\n",
        "                serializable_hparams[k] = v.cpu().item() if v.ndim == 0 else v.cpu().tolist()\n",
        "            else:\n",
        "                serializable_hparams[k] = v\n",
        "\n",
        "        serializable_results = {}\n",
        "        if hasattr(self, 'test_results'):\n",
        "            for k, v in self.test_results.items():\n",
        "                serializable_results[k] = v.cpu().item() if isinstance(v, torch.Tensor) else v\n",
        "\n",
        "        with open(stats_path, 'w') as f:\n",
        "            f.write(f\"Model architecture:\\n{self}\\n\\n\")\n",
        "            f.write(\"Hyperparameters:\\n\")\n",
        "            f.write(json.dumps(serializable_hparams, indent=4))\n",
        "            f.write(\"\\n\\n\")\n",
        "            if serializable_results:\n",
        "                f.write(\"Test results:\\n\")\n",
        "                f.write(json.dumps(serializable_results, indent=4))\n",
        "                f.write(\"\\n\\n\")\n",
        "            if hasattr(self, 'epochs_trained'):\n",
        "                f.write(f\"Epochs trained: {self.epochs_trained}\\n\")\n",
        "\n",
        "        self._last_save_dir = folder\n",
        "        self._last_timestamp = timestamp\n",
        "        print(f\"Artifacts saved to {folder}/\")\n",
        "    \n",
        "    def load_mae_ckpt(self, ckpt_source: str):\n",
        "        \"\"\"\n",
        "        Load MAE-AST checkpoint into the truncated upstream model\n",
        "        and verify exact weight equality for each loaded parameter.\n",
        "        \"\"\"\n",
        "        loaded = torch.load(ckpt_source)\n",
        "        state_dict = loaded.get('model', loaded)\n",
        "\n",
        "        up = self.backbone.upstream.model\n",
        "        up_state = up.state_dict()\n",
        "\n",
        "        to_load = {k: v for k, v in state_dict.items()\n",
        "                   if k in up_state and v.shape == up_state[k].shape}\n",
        "\n",
        "        missing, unexpected = up.load_state_dict(to_load, strict=False)\n",
        "\n",
        "        for k, v in to_load.items():\n",
        "            if not torch.equal(up_state[k], v):\n",
        "                raise RuntimeError(f\"Weight mismatch at '{k}' after loading checkpoint\")\n",
        "\n",
        "        print(f\"Successfully loaded {len(to_load)} parameters; missing: {len(missing)}, unexpected: {len(unexpected)}\")\n",
        "    \n",
        "    @staticmethod\n",
        "    def _safe_logmelspec(waveform, mel_spec, to_db, db_range=80.0):\n",
        "        if waveform.dim() == 2:\n",
        "            waveform = waveform.unsqueeze(1)\n",
        "        spec = mel_spec(waveform) + 1e-10\n",
        "        spec_db = to_db(spec)\n",
        "        spec_db = torch.nan_to_num(spec_db, neginf=-db_range)\n",
        "        t = spec_db\n",
        "        d1 = F.compute_deltas(t.squeeze(1)).unsqueeze(1)\n",
        "        d2 = F.compute_deltas(d1.squeeze(1)).unsqueeze(1)\n",
        "        spec3 = torch.cat([t, d1, d2], dim=1)\n",
        "        spec3 = ((spec3 + db_range)/db_range).clamp(0.0, 1.0)\n",
        "        return spec3\n",
        "\n",
        "    @classmethod\n",
        "    def load_model(cls, load_dir: str, map_location=None):\n",
        "        \"\"\"\n",
        "        Load a model checkpoint and hyperparameters from a directory.\n",
        "\n",
        "        Returns:\n",
        "            model (MammalClassifier): Loaded model\n",
        "        \"\"\"\n",
        "        hparams_path = os.path.join(load_dir, 'hparams.json')\n",
        "        with open(hparams_path, 'r') as f:\n",
        "            hparams = json.load(f)\n",
        "\n",
        "        model = cls(**hparams)\n",
        "        ckpt_path = os.path.join(load_dir, f'{cls.__name__}.ckpt')\n",
        "        state = torch.load(ckpt_path, map_location=map_location)\n",
        "        model.load_state_dict(state['state_dict'])\n",
        "        return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   | Name            | Type                | Params | Mode \n",
            "-----------------------------------------------------------------\n",
            "0  | mel_spec        | MelSpectrogram      | 0      | train\n",
            "1  | to_db           | AmplitudeToDB       | 0      | train\n",
            "2  | backbone        | Sequential          | 4.7 M  | train\n",
            "3  | classifier      | Sequential          | 557 K  | train\n",
            "4  | criterion       | CrossEntropyLoss    | 0      | train\n",
            "5  | train_precision | MulticlassPrecision | 0      | train\n",
            "6  | train_recall    | MulticlassRecall    | 0      | train\n",
            "7  | train_f1        | MulticlassF1Score   | 0      | train\n",
            "8  | val_precision   | MulticlassPrecision | 0      | train\n",
            "9  | val_recall      | MulticlassRecall    | 0      | train\n",
            "10 | val_f1          | MulticlassF1Score   | 0      | train\n",
            "11 | test_precision  | MulticlassPrecision | 0      | train\n",
            "12 | test_recall     | MulticlassRecall    | 0      | train\n",
            "13 | test_f1         | MulticlassF1Score   | 0      | train\n",
            "-----------------------------------------------------------------\n",
            "5.3 M     Trainable params\n",
            "0         Non-trainable params\n",
            "5.3 M     Total params\n",
            "21.093    Total estimated model params size (MB)\n",
            "57        Modules in train mode\n",
            "0         Modules in eval mode\n"
          ]
        }
      ],
      "source": [
        "# sanity check\n",
        "model = WMMDClassifier(\n",
        "        num_classes=31, lr=1e-3,\n",
        "        backbone=\"cnn-spect\", finetune=True,\n",
        "        class_weights=class_weights,\n",
        "        ckpt_path=\"\"\n",
        "    )\n",
        "\n",
        "print(ModelSummary(model, max_depth=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ycDcNmcwQ_ON"
      },
      "outputs": [],
      "source": [
        "def WMMD_Collate(batch):\n",
        "    waveforms, labels = zip(*batch)\n",
        "\n",
        "    lengths    = [w.shape[0] for w in waveforms]\n",
        "    raw_max    = max(lengths)\n",
        "\n",
        "    min_len    = 5_000\n",
        "    max_len    = 25_000\n",
        "    target_len = min(max(raw_max, min_len), max_len)\n",
        "\n",
        "    padded_waveforms = []\n",
        "    for w in waveforms:\n",
        "        L = w.shape[0]\n",
        "\n",
        "        if L > target_len:\n",
        "            start = random.randint(0, L - target_len)\n",
        "            w2    = w[start : start + target_len]\n",
        "\n",
        "        elif L < target_len:\n",
        "            pad_amt = target_len - L\n",
        "            w2      = torch.nn.functional.pad(w, (0, pad_amt))\n",
        "\n",
        "        else:\n",
        "            w2 = w\n",
        "\n",
        "        padded_waveforms.append(w2)\n",
        "\n",
        "    batch_waveforms = torch.stack(padded_waveforms, dim=0)\n",
        "    batch_labels    = torch.tensor(labels, dtype=torch.long)\n",
        "    return batch_waveforms, batch_labels\n",
        "\n",
        "class WMMDSoundDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset, backbone: str, target_sr: int = 2000):\n",
        "        \"\"\"\n",
        "        dataset: list of dicts with keys 'path' & 'label'\n",
        "        backbone: 'facebook/wav2vec2-base' or 'mae-ast'\n",
        "        target_sr: sampling rate (e.g. 2000)\n",
        "        \"\"\"\n",
        "        self.dataset = dataset\n",
        "        self.backbone = backbone\n",
        "        self.target_sr = target_sr\n",
        "        self.resampler_cache = {}\n",
        "\n",
        "        if self.backbone in {\"facebook/wav2vec2-base\", \"patrickvonplaten/tiny-wav2vec2-no-tokenizer\"}:\n",
        "            self.processor = Wav2Vec2FeatureExtractor.from_pretrained(\n",
        "                self.backbone, return_attention_mask=False, sampling_rate=self.target_sr\n",
        "            )\n",
        "        elif self.backbone in {\"mae-ast\", \"cnn-spect\", \"vit-imagenet\"}:\n",
        "            self.processor = None\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported backbone '{self.backbone}'\")\n",
        "\n",
        "        labels = sorted({item['label'] for item in dataset})\n",
        "        self.label_to_int = {lbl: i for i, lbl in enumerate(labels)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "        audio_path = item[\"path\"]\n",
        "        waveform, orig_sr = torchaudio.load(audio_path)\n",
        "\n",
        "        if orig_sr != self.target_sr:\n",
        "            if orig_sr not in self.resampler_cache:\n",
        "                self.resampler_cache[orig_sr] = torchaudio.transforms.Resample(orig_sr, self.target_sr)\n",
        "            waveform = self.resampler_cache[orig_sr](waveform)\n",
        "\n",
        "        waveform = waveform / (waveform.abs().max() + 1e-6)\n",
        "        wav_1d = waveform.squeeze(0)  \n",
        "        \n",
        "        if self.backbone in {\"facebook/wav2vec2-base\", \"patrickvonplaten/tiny-wav2vec2-no-tokenizer\"}:\n",
        "            arr = wav_1d.numpy()\n",
        "            feats = self.processor(arr, sampling_rate=self.target_sr, return_tensors=\"pt\")\n",
        "            inp = feats.input_values.squeeze(0)\n",
        "        elif self.backbone in {\"cnn-spect\", \"vit-imagenet\", \"mae-ast\"}:\n",
        "            inp = wav_1d            \n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported backbone '{self.backbone}'\")\n",
        "\n",
        "        lbl = self.label_to_int[item['label']]\n",
        "        return inp, lbl\n",
        "\n",
        "class WMMDDataModule(pl.LightningDataModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dataset_dict,\n",
        "        backbone: str,\n",
        "        batch_size: int = 2,\n",
        "        num_workers: int = 1\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dataset_dict = dataset_dict\n",
        "        self.backbone = backbone\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        self.train_ds = WMMDSoundDataset(self.dataset_dict[\"train\"], backbone=self.backbone)\n",
        "        self.val_ds   = WMMDSoundDataset(self.dataset_dict[\"validation\"], backbone=self.backbone)\n",
        "        self.test_ds  = WMMDSoundDataset(self.dataset_dict[\"test\"], backbone=self.backbone)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_ds,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=self.num_workers,\n",
        "            pin_memory=True,\n",
        "            collate_fn=WMMD_Collate\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.val_ds,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=self.num_workers,\n",
        "            pin_memory=True,\n",
        "            collate_fn=WMMD_Collate\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.test_ds,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=self.num_workers,\n",
        "            pin_memory=True,\n",
        "            collate_fn=WMMD_Collate\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kQ2XWE-pRKAM"
      },
      "outputs": [],
      "source": [
        "# callback for logging metrics\n",
        "class MetricsLogger(pl.Callback):\n",
        "    def __init__(self):\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.train_accs = []\n",
        "        self.val_accs = []\n",
        "        self.train_precisions = []\n",
        "        self.val_precisions = []\n",
        "        self.train_recalls = []\n",
        "        self.val_recalls = []\n",
        "        self.train_f1s = []\n",
        "        self.val_f1s = []\n",
        "\n",
        "    def on_train_epoch_end(self, trainer, pl_module):\n",
        "        m = trainer.callback_metrics\n",
        "        self.train_losses.append(m['train_loss'].item())\n",
        "        self.train_accs.append(m['train_acc'].item())\n",
        "        self.train_precisions.append(m['train_precision'].item())\n",
        "        self.train_recalls.append(m['train_recall'].item())\n",
        "        self.train_f1s.append(m['train_f1'].item())\n",
        "\n",
        "    def on_validation_epoch_end(self, trainer, pl_module):\n",
        "        m = trainer.callback_metrics\n",
        "        self.val_losses.append(m['val_loss'].item())\n",
        "        self.val_accs.append(m['val_acc'].item())\n",
        "        self.val_precisions.append(m['val_precision'].item())\n",
        "        self.val_recalls.append(m['val_recall'].item())\n",
        "        self.val_f1s.append(m['val_f1'].item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_configs = [\n",
        "    # Wav2Vec 2.0 (fine-tune)\n",
        "    {\n",
        "        \"num_classes\": num_classes,\n",
        "        \"lr\": 1e-6,\n",
        "        \"backbone\": \"facebook/wav2vec2-base\",\n",
        "        \"finetune\": True,\n",
        "        \"class_weights\": class_weights,\n",
        "        \"max_epochs\": 500,\n",
        "        \"ckpt_path\": \"\",\n",
        "        \"batch_size\": 2,\n",
        "    },\n",
        "    # # Tinyâ€Wav2Vec 2.0 (fine-tune)\n",
        "    # {\n",
        "    #     \"num_classes\": num_classes,\n",
        "    #     \"lr\": 1e-6,\n",
        "    #     \"backbone\": \"patrickvonplaten/tiny-wav2vec2-no-tokenizer\",\n",
        "    #     \"finetune\": True,\n",
        "    #     \"class_weights\": class_weights,\n",
        "    #     \"max_epochs\": 500,\n",
        "    #     \"ckpt_path\": \"\",\n",
        "    #     \"batch_size\": 2,\n",
        "    # },\n",
        "    # # CNN-Spectrogram (train from scratch)\n",
        "    # {\n",
        "    #     \"num_classes\": num_classes,\n",
        "    #     \"lr\": 1e-3,\n",
        "    #     \"backbone\": \"cnn-spect\",\n",
        "    #     \"finetune\": True,\n",
        "    #     \"class_weights\": class_weights,\n",
        "    #     \"max_epochs\": 500,\n",
        "    #     \"ckpt_path\": \"\",\n",
        "    #     \"batch_size\": 2,\n",
        "    # },\n",
        "    # ViT-ImageNet (fine-tune)\n",
        "    {\n",
        "        \"num_classes\": num_classes,\n",
        "        \"lr\": 1e-5,\n",
        "        \"backbone\": \"vit-imagenet\",\n",
        "        \"finetune\": True,\n",
        "        \"class_weights\": class_weights,\n",
        "        \"max_epochs\": 500,\n",
        "        \"ckpt_path\": \"\",\n",
        "        \"batch_size\": 2,\n",
        "    },\n",
        "    # MAE-AST (fine-tune)\n",
        "    # {\n",
        "    #     \"num_classes\": num_classes,\n",
        "    #     \"lr\": 1e-6,\n",
        "    #     \"backbone\": \"mae-ast\",\n",
        "    #     \"finetune\": True,\n",
        "    #     \"class_weights\": class_weights,\n",
        "    #     \"max_epochs\": 500,\n",
        "    #     \"ckpt_path\": \"4Enc_1Dec-61epoch-0.103loss.pt\",\n",
        "    #     \"batch_size\": 2,\n",
        "    # },\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "9k7VBnf5TWfW"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\transformers\\configuration_utils.py:309: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n",
            "Using 16bit Automatic Mixed Precision (AMP)\n",
            "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "   | Name            | Type                | Params | Mode \n",
            "-----------------------------------------------------------------\n",
            "0  | backbone        | Wav2Vec2Model       | 94.4 M | eval \n",
            "1  | classifier      | Sequential          | 819 K  | train\n",
            "2  | criterion       | CrossEntropyLoss    | 0      | train\n",
            "3  | train_precision | MulticlassPrecision | 0      | train\n",
            "4  | train_recall    | MulticlassRecall    | 0      | train\n",
            "5  | train_f1        | MulticlassF1Score   | 0      | train\n",
            "6  | val_precision   | MulticlassPrecision | 0      | train\n",
            "7  | val_recall      | MulticlassRecall    | 0      | train\n",
            "8  | val_f1          | MulticlassF1Score   | 0      | train\n",
            "9  | test_precision  | MulticlassPrecision | 0      | train\n",
            "10 | test_recall     | MulticlassRecall    | 0      | train\n",
            "11 | test_f1         | MulticlassF1Score   | 0      | train\n",
            "-----------------------------------------------------------------\n",
            "95.2 M    Trainable params\n",
            "0         Non-trainable params\n",
            "95.2 M    Total params\n",
            "380.764   Total estimated model params size (MB)\n",
            "16        Modules in train mode\n",
            "220       Modules in eval mode\n",
            "c:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=21` in the `DataLoader` to improve performance.\n",
            "c:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=21` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [01:11<00:00,  7.15it/s, v_num=62, train_loss_step=3.460, train_acc_step=0.000, val_loss=3.430, val_acc=0.0472, train_loss_epoch=3.440, train_acc_epoch=0.0472]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved. New best score: 3.429\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [01:14<00:00,  6.84it/s, v_num=62, train_loss_step=3.310, train_acc_step=0.000, val_loss=3.410, val_acc=0.0944, train_loss_epoch=3.420, train_acc_epoch=0.060] "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.014 >= min_delta = 0.01. New best score: 3.414\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [01:15<00:00,  6.75it/s, v_num=62, train_loss_step=3.450, train_acc_step=0.000, val_loss=3.400, val_acc=0.124, train_loss_epoch=3.410, train_acc_epoch=0.0678]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.010 >= min_delta = 0.01. New best score: 3.404\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [01:07<00:00,  7.50it/s, v_num=62, train_loss_step=3.300, train_acc_step=0.000, val_loss=3.380, val_acc=0.159, train_loss_epoch=3.390, train_acc_epoch=0.107] "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.026 >= min_delta = 0.01. New best score: 3.378\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [00:46<00:00, 10.93it/s, v_num=62, train_loss_step=3.520, train_acc_step=0.000, val_loss=3.360, val_acc=0.142, train_loss_epoch=3.380, train_acc_epoch=0.116]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.015 >= min_delta = 0.01. New best score: 3.362\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [01:17<00:00,  6.55it/s, v_num=62, train_loss_step=3.210, train_acc_step=0.000, val_loss=3.350, val_acc=0.162, train_loss_epoch=3.350, train_acc_epoch=0.128]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.013 >= min_delta = 0.01. New best score: 3.349\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [01:27<00:00,  5.79it/s, v_num=62, train_loss_step=3.230, train_acc_step=0.000, val_loss=3.330, val_acc=0.171, train_loss_epoch=3.340, train_acc_epoch=0.124]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.023 >= min_delta = 0.01. New best score: 3.326\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [01:17<00:00,  6.61it/s, v_num=62, train_loss_step=3.420, train_acc_step=0.000, val_loss=3.300, val_acc=0.180, train_loss_epoch=3.310, train_acc_epoch=0.173]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.024 >= min_delta = 0.01. New best score: 3.303\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [00:48<00:00, 10.58it/s, v_num=62, train_loss_step=3.450, train_acc_step=0.000, val_loss=3.280, val_acc=0.201, train_loss_epoch=3.280, train_acc_epoch=0.172]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.025 >= min_delta = 0.01. New best score: 3.277\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [00:54<00:00,  9.36it/s, v_num=62, train_loss_step=3.380, train_acc_step=0.000, val_loss=3.260, val_acc=0.218, train_loss_epoch=3.260, train_acc_epoch=0.179]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.020 >= min_delta = 0.01. New best score: 3.257\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [00:51<00:00,  9.83it/s, v_num=62, train_loss_step=2.800, train_acc_step=1.000, val_loss=3.210, val_acc=0.218, train_loss_epoch=3.210, train_acc_epoch=0.231]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.049 >= min_delta = 0.01. New best score: 3.208\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [00:49<00:00, 10.25it/s, v_num=62, train_loss_step=3.160, train_acc_step=0.000, val_loss=3.190, val_acc=0.251, train_loss_epoch=3.180, train_acc_epoch=0.249]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.020 >= min_delta = 0.01. New best score: 3.188\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [00:48<00:00, 10.50it/s, v_num=62, train_loss_step=2.850, train_acc_step=1.000, val_loss=3.160, val_acc=0.268, train_loss_epoch=3.170, train_acc_epoch=0.237]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.026 >= min_delta = 0.01. New best score: 3.162\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [00:47<00:00, 10.62it/s, v_num=62, train_loss_step=3.220, train_acc_step=0.000, val_loss=3.150, val_acc=0.260, train_loss_epoch=3.140, train_acc_epoch=0.266]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.013 >= min_delta = 0.01. New best score: 3.149\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [00:50<00:00, 10.07it/s, v_num=62, train_loss_step=2.930, train_acc_step=1.000, val_loss=3.130, val_acc=0.271, train_loss_epoch=3.110, train_acc_epoch=0.296]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.021 >= min_delta = 0.01. New best score: 3.128\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [00:48<00:00, 10.41it/s, v_num=62, train_loss_step=3.210, train_acc_step=0.000, val_loss=3.100, val_acc=0.295, train_loss_epoch=3.080, train_acc_epoch=0.293]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.029 >= min_delta = 0.01. New best score: 3.098\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [00:51<00:00,  9.87it/s, v_num=62, train_loss_step=3.200, train_acc_step=0.000, val_loss=3.080, val_acc=0.313, train_loss_epoch=3.050, train_acc_epoch=0.316]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.021 >= min_delta = 0.01. New best score: 3.078\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [00:50<00:00, 10.12it/s, v_num=62, train_loss_step=3.380, train_acc_step=0.000, val_loss=3.060, val_acc=0.263, train_loss_epoch=3.020, train_acc_epoch=0.320]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.020 >= min_delta = 0.01. New best score: 3.058\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [00:50<00:00, 10.00it/s, v_num=62, train_loss_step=2.690, train_acc_step=1.000, val_loss=3.030, val_acc=0.324, train_loss_epoch=3.000, train_acc_epoch=0.331]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.025 >= min_delta = 0.01. New best score: 3.033\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [01:03<00:00,  8.00it/s, v_num=62, train_loss_step=3.210, train_acc_step=0.000, val_loss=3.010, val_acc=0.292, train_loss_epoch=2.960, train_acc_epoch=0.343]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.025 >= min_delta = 0.01. New best score: 3.008\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [00:55<00:00,  9.11it/s, v_num=62, train_loss_step=2.690, train_acc_step=0.000, val_loss=2.990, val_acc=0.351, train_loss_epoch=2.940, train_acc_epoch=0.344]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.015 >= min_delta = 0.01. New best score: 2.993\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [00:54<00:00,  9.32it/s, v_num=62, train_loss_step=2.220, train_acc_step=1.000, val_loss=2.960, val_acc=0.339, train_loss_epoch=2.910, train_acc_epoch=0.357]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.036 >= min_delta = 0.01. New best score: 2.956\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [00:51<00:00,  9.97it/s, v_num=62, train_loss_step=2.450, train_acc_step=1.000, val_loss=2.940, val_acc=0.322, train_loss_epoch=2.890, train_acc_epoch=0.373]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.015 >= min_delta = 0.01. New best score: 2.941\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [00:52<00:00,  9.78it/s, v_num=62, train_loss_step=2.520, train_acc_step=1.000, val_loss=2.920, val_acc=0.348, train_loss_epoch=2.850, train_acc_epoch=0.383]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.025 >= min_delta = 0.01. New best score: 2.916\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [00:53<00:00,  9.59it/s, v_num=62, train_loss_step=2.890, train_acc_step=0.000, val_loss=2.900, val_acc=0.342, train_loss_epoch=2.820, train_acc_epoch=0.416]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.020 >= min_delta = 0.01. New best score: 2.896\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 26: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [00:50<00:00, 10.04it/s, v_num=62, train_loss_step=2.950, train_acc_step=0.000, val_loss=2.870, val_acc=0.360, train_loss_epoch=2.790, train_acc_epoch=0.410]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.022 >= min_delta = 0.01. New best score: 2.874\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 27: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [00:53<00:00,  9.50it/s, v_num=62, train_loss_step=1.830, train_acc_step=1.000, val_loss=2.860, val_acc=0.366, train_loss_epoch=2.750, train_acc_epoch=0.413]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.017 >= min_delta = 0.01. New best score: 2.857\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 28: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [01:08<00:00,  7.47it/s, v_num=62, train_loss_step=1.710, train_acc_step=1.000, val_loss=2.830, val_acc=0.366, train_loss_epoch=2.730, train_acc_epoch=0.433]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.027 >= min_delta = 0.01. New best score: 2.830\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 29: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [01:15<00:00,  6.77it/s, v_num=62, train_loss_step=3.170, train_acc_step=0.000, val_loss=2.810, val_acc=0.363, train_loss_epoch=2.680, train_acc_epoch=0.446]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.023 >= min_delta = 0.01. New best score: 2.808\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [01:21<00:00,  6.28it/s, v_num=62, train_loss_step=2.160, train_acc_step=1.000, val_loss=2.780, val_acc=0.392, train_loss_epoch=2.650, train_acc_epoch=0.469]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.025 >= min_delta = 0.01. New best score: 2.783\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 31: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [01:32<00:00,  5.52it/s, v_num=62, train_loss_step=2.920, train_acc_step=0.000, val_loss=2.760, val_acc=0.381, train_loss_epoch=2.630, train_acc_epoch=0.485]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.022 >= min_delta = 0.01. New best score: 2.761\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [01:15<00:00,  6.75it/s, v_num=62, train_loss_step=2.600, train_acc_step=0.000, val_loss=2.740, val_acc=0.386, train_loss_epoch=2.620, train_acc_epoch=0.488]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.018 >= min_delta = 0.01. New best score: 2.743\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 33: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [01:33<00:00,  5.44it/s, v_num=62, train_loss_step=1.890, train_acc_step=1.000, val_loss=2.710, val_acc=0.395, train_loss_epoch=2.580, train_acc_epoch=0.478]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.031 >= min_delta = 0.01. New best score: 2.711\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 34: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [01:24<00:00,  6.06it/s, v_num=62, train_loss_step=2.550, train_acc_step=1.000, val_loss=2.690, val_acc=0.389, train_loss_epoch=2.540, train_acc_epoch=0.504]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.023 >= min_delta = 0.01. New best score: 2.689\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [01:17<00:00,  6.60it/s, v_num=62, train_loss_step=2.780, train_acc_step=0.000, val_loss=2.670, val_acc=0.398, train_loss_epoch=2.510, train_acc_epoch=0.507]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.015 >= min_delta = 0.01. New best score: 2.674\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 36: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [01:15<00:00,  6.74it/s, v_num=62, train_loss_step=2.870, train_acc_step=0.000, val_loss=2.660, val_acc=0.386, train_loss_epoch=2.480, train_acc_epoch=0.537]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.011 >= min_delta = 0.01. New best score: 2.662\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 37: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [01:14<00:00,  6.86it/s, v_num=62, train_loss_step=2.480, train_acc_step=1.000, val_loss=2.650, val_acc=0.381, train_loss_epoch=2.430, train_acc_epoch=0.531]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.017 >= min_delta = 0.01. New best score: 2.645\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 38: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [01:16<00:00,  6.66it/s, v_num=62, train_loss_step=1.700, train_acc_step=1.000, val_loss=2.620, val_acc=0.407, train_loss_epoch=2.420, train_acc_epoch=0.553]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.027 >= min_delta = 0.01. New best score: 2.618\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 39: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [01:15<00:00,  6.73it/s, v_num=62, train_loss_step=1.110, train_acc_step=1.000, val_loss=2.610, val_acc=0.410, train_loss_epoch=2.380, train_acc_epoch=0.572]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.012 >= min_delta = 0.01. New best score: 2.605\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 40: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [01:09<00:00,  7.34it/s, v_num=62, train_loss_step=2.190, train_acc_step=1.000, val_loss=2.590, val_acc=0.404, train_loss_epoch=2.350, train_acc_epoch=0.564]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.019 >= min_delta = 0.01. New best score: 2.587\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 41: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [01:18<00:00,  6.52it/s, v_num=62, train_loss_step=1.360, train_acc_step=1.000, val_loss=2.570, val_acc=0.410, train_loss_epoch=2.310, train_acc_epoch=0.568]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.015 >= min_delta = 0.01. New best score: 2.571\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 42: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [01:19<00:00,  6.43it/s, v_num=62, train_loss_step=1.280, train_acc_step=1.000, val_loss=2.540, val_acc=0.419, train_loss_epoch=2.290, train_acc_epoch=0.577]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.033 >= min_delta = 0.01. New best score: 2.538\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 43: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [01:16<00:00,  6.68it/s, v_num=62, train_loss_step=1.840, train_acc_step=1.000, val_loss=2.520, val_acc=0.413, train_loss_epoch=2.260, train_acc_epoch=0.599]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.015 >= min_delta = 0.01. New best score: 2.523\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 44: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [01:22<00:00,  6.19it/s, v_num=62, train_loss_step=2.350, train_acc_step=0.000, val_loss=2.490, val_acc=0.416, train_loss_epoch=2.210, train_acc_epoch=0.618]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.028 >= min_delta = 0.01. New best score: 2.495\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3:   5%|â–         | 24/509 [58:21<19:39:09,  0.01it/s, v_num=60, train_loss_step=3.420, train_acc_step=0.000, val_loss=3.440, val_acc=0.0295, train_loss_epoch=3.440, train_acc_epoch=0.0265]\n",
            "Epoch 45: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [01:20<00:00,  6.31it/s, v_num=62, train_loss_step=2.390, train_acc_step=1.000, val_loss=2.480, val_acc=0.440, train_loss_epoch=2.190, train_acc_epoch=0.614]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.013 >= min_delta = 0.01. New best score: 2.482\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 46: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [01:20<00:00,  6.33it/s, v_num=62, train_loss_step=1.980, train_acc_step=1.000, val_loss=2.470, val_acc=0.413, train_loss_epoch=2.140, train_acc_epoch=0.639]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.014 >= min_delta = 0.01. New best score: 2.468\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 48: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [01:18<00:00,  6.52it/s, v_num=62, train_loss_step=1.120, train_acc_step=1.000, val_loss=2.440, val_acc=0.428, train_loss_epoch=2.100, train_acc_epoch=0.645]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.031 >= min_delta = 0.01. New best score: 2.437\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 49: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [00:49<00:00, 10.32it/s, v_num=62, train_loss_step=1.370, train_acc_step=1.000, val_loss=2.410, val_acc=0.431, train_loss_epoch=2.080, train_acc_epoch=0.662]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.032 >= min_delta = 0.01. New best score: 2.405\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [01:09<00:00,  7.32it/s, v_num=62, train_loss_step=1.010, train_acc_step=1.000, val_loss=2.380, val_acc=0.442, train_loss_epoch=2.030, train_acc_epoch=0.651]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.021 >= min_delta = 0.01. New best score: 2.384\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 51: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [01:22<00:00,  6.17it/s, v_num=62, train_loss_step=2.010, train_acc_step=1.000, val_loss=2.370, val_acc=0.425, train_loss_epoch=2.000, train_acc_epoch=0.665]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.015 >= min_delta = 0.01. New best score: 2.369\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 53: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [01:19<00:00,  6.38it/s, v_num=62, train_loss_step=1.670, train_acc_step=1.000, val_loss=2.350, val_acc=0.425, train_loss_epoch=1.930, train_acc_epoch=0.699]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.023 >= min_delta = 0.01. New best score: 2.346\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 54: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [01:25<00:00,  5.96it/s, v_num=62, train_loss_step=0.700, train_acc_step=1.000, val_loss=2.310, val_acc=0.448, train_loss_epoch=1.900, train_acc_epoch=0.693]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.031 >= min_delta = 0.01. New best score: 2.314\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 55: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [01:25<00:00,  5.94it/s, v_num=62, train_loss_step=1.800, train_acc_step=1.000, val_loss=2.300, val_acc=0.425, train_loss_epoch=1.860, train_acc_epoch=0.723]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.017 >= min_delta = 0.01. New best score: 2.297\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 57: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [01:14<00:00,  6.83it/s, v_num=62, train_loss_step=1.290, train_acc_step=1.000, val_loss=2.270, val_acc=0.463, train_loss_epoch=1.810, train_acc_epoch=0.740]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.023 >= min_delta = 0.01. New best score: 2.274\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 58: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [01:15<00:00,  6.70it/s, v_num=62, train_loss_step=0.668, train_acc_step=1.000, val_loss=2.240, val_acc=0.478, train_loss_epoch=1.760, train_acc_epoch=0.747]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.034 >= min_delta = 0.01. New best score: 2.240\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 61: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [01:19<00:00,  6.42it/s, v_num=62, train_loss_step=1.250, train_acc_step=1.000, val_loss=2.220, val_acc=0.472, train_loss_epoch=1.680, train_acc_epoch=0.772]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.019 >= min_delta = 0.01. New best score: 2.221\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 62: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [01:18<00:00,  6.52it/s, v_num=62, train_loss_step=1.020, train_acc_step=1.000, val_loss=2.210, val_acc=0.475, train_loss_epoch=1.680, train_acc_epoch=0.751]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.014 >= min_delta = 0.01. New best score: 2.208\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 63: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [01:08<00:00,  7.39it/s, v_num=62, train_loss_step=0.580, train_acc_step=1.000, val_loss=2.190, val_acc=0.457, train_loss_epoch=1.610, train_acc_epoch=0.794]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.014 >= min_delta = 0.01. New best score: 2.194\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [01:15<00:00,  6.73it/s, v_num=62, train_loss_step=0.919, train_acc_step=1.000, val_loss=2.160, val_acc=0.484, train_loss_epoch=1.550, train_acc_epoch=0.800]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.036 >= min_delta = 0.01. New best score: 2.157\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 65: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [01:30<00:00,  5.61it/s, v_num=62, train_loss_step=0.704, train_acc_step=1.000, val_loss=2.140, val_acc=0.469, train_loss_epoch=1.540, train_acc_epoch=0.815]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.020 >= min_delta = 0.01. New best score: 2.138\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 66: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [01:24<00:00,  6.04it/s, v_num=62, train_loss_step=0.752, train_acc_step=1.000, val_loss=2.110, val_acc=0.490, train_loss_epoch=1.510, train_acc_epoch=0.821]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.031 >= min_delta = 0.01. New best score: 2.107\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 69: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [00:53<00:00,  9.47it/s, v_num=62, train_loss_step=0.968, train_acc_step=1.000, val_loss=2.070, val_acc=0.507, train_loss_epoch=1.410, train_acc_epoch=0.842]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.038 >= min_delta = 0.01. New best score: 2.069\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 71: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [00:52<00:00,  9.69it/s, v_num=62, train_loss_step=1.480, train_acc_step=0.000, val_loss=2.020, val_acc=0.513, train_loss_epoch=1.360, train_acc_epoch=0.858]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.047 >= min_delta = 0.01. New best score: 2.022\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 74: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [00:50<00:00, 10.03it/s, v_num=62, train_loss_step=0.812, train_acc_step=1.000, val_loss=2.000, val_acc=0.510, train_loss_epoch=1.250, train_acc_epoch=0.887]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.022 >= min_delta = 0.01. New best score: 1.999\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 78: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [00:54<00:00,  9.41it/s, v_num=62, train_loss_step=1.960, train_acc_step=1.000, val_loss=1.940, val_acc=0.537, train_loss_epoch=1.160, train_acc_epoch=0.892]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.056 >= min_delta = 0.01. New best score: 1.943\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 80: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [01:07<00:00,  7.58it/s, v_num=62, train_loss_step=0.357, train_acc_step=1.000, val_loss=1.930, val_acc=0.516, train_loss_epoch=1.090, train_acc_epoch=0.912]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.010 >= min_delta = 0.01. New best score: 1.933\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 83: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [00:50<00:00, 10.10it/s, v_num=62, train_loss_step=0.421, train_acc_step=1.000, val_loss=1.910, val_acc=0.537, train_loss_epoch=1.020, train_acc_epoch=0.915]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.023 >= min_delta = 0.01. New best score: 1.911\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 86: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [00:48<00:00, 10.59it/s, v_num=62, train_loss_step=0.506, train_acc_step=1.000, val_loss=1.880, val_acc=0.540, train_loss_epoch=0.940, train_acc_epoch=0.920]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.033 >= min_delta = 0.01. New best score: 1.878\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 87: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [00:51<00:00,  9.94it/s, v_num=62, train_loss_step=0.525, train_acc_step=1.000, val_loss=1.850, val_acc=0.534, train_loss_epoch=0.923, train_acc_epoch=0.932]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.027 >= min_delta = 0.01. New best score: 1.850\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 96: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [00:53<00:00,  9.47it/s, v_num=62, train_loss_step=0.333, train_acc_step=1.000, val_loss=1.840, val_acc=0.522, train_loss_epoch=0.688, train_acc_epoch=0.955]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.012 >= min_delta = 0.01. New best score: 1.839\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 97: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [00:49<00:00, 10.25it/s, v_num=62, train_loss_step=0.193, train_acc_step=1.000, val_loss=1.810, val_acc=0.537, train_loss_epoch=0.689, train_acc_epoch=0.954]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.029 >= min_delta = 0.01. New best score: 1.810\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 99: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [00:49<00:00, 10.27it/s, v_num=62, train_loss_step=0.716, train_acc_step=1.000, val_loss=1.790, val_acc=0.543, train_loss_epoch=0.647, train_acc_epoch=0.950]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.018 >= min_delta = 0.01. New best score: 1.792\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [00:50<00:00, 10.10it/s, v_num=62, train_loss_step=0.724, train_acc_step=1.000, val_loss=1.760, val_acc=0.560, train_loss_epoch=0.628, train_acc_epoch=0.960]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.030 >= min_delta = 0.01. New best score: 1.762\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 107: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [00:50<00:00, 10.12it/s, v_num=62, train_loss_step=0.286, train_acc_step=1.000, val_loss=1.730, val_acc=0.558, train_loss_epoch=0.518, train_acc_epoch=0.968] "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.035 >= min_delta = 0.01. New best score: 1.727\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 127: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [01:01<00:00,  8.25it/s, v_num=62, train_loss_step=0.152, train_acc_step=1.000, val_loss=1.750, val_acc=0.555, train_loss_epoch=0.304, train_acc_epoch=0.974] "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Monitored metric val_loss did not improve in the last 20 records. Best score: 1.727. Signaling Trainer to stop.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 127: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [01:06<00:00,  7.68it/s, v_num=62, train_loss_step=0.152, train_acc_step=1.000, val_loss=1.750, val_acc=0.555, train_loss_epoch=0.304, train_acc_epoch=0.974]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "c:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=21` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:10<00:00, 15.68it/s]\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "       Test metric             DataLoader 0\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "        test_acc            0.5427728891372681\n",
            "         test_f1            0.4709932208061218\n",
            "        test_loss           1.8143362998962402\n",
            "     test_precision         0.5349065661430359\n",
            "       test_recall          0.43903636932373047\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Artifacts saved to model\\facebook_wav2vec2-base_imbalance\\20250703_165539/\n",
            "Completed facebook/wav2vec2-base (FT), artifacts in model\\facebook_wav2vec2-base_imbalance\\20250703_165539\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:30: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTModel: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Using 16bit Automatic Mixed Precision (AMP)\n",
            "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "   | Name            | Type                | Params | Mode \n",
            "-----------------------------------------------------------------\n",
            "0  | mel_spec        | MelSpectrogram      | 0      | train\n",
            "1  | to_db           | AmplitudeToDB       | 0      | train\n",
            "2  | backbone        | ViTModel            | 85.8 M | eval \n",
            "3  | classifier      | Sequential          | 819 K  | train\n",
            "4  | criterion       | CrossEntropyLoss    | 0      | train\n",
            "5  | train_precision | MulticlassPrecision | 0      | train\n",
            "6  | train_recall    | MulticlassRecall    | 0      | train\n",
            "7  | train_f1        | MulticlassF1Score   | 0      | train\n",
            "8  | val_precision   | MulticlassPrecision | 0      | train\n",
            "9  | val_recall      | MulticlassRecall    | 0      | train\n",
            "10 | val_f1          | MulticlassF1Score   | 0      | train\n",
            "11 | test_precision  | MulticlassPrecision | 0      | train\n",
            "12 | test_recall     | MulticlassRecall    | 0      | train\n",
            "13 | test_f1         | MulticlassF1Score   | 0      | train\n",
            "-----------------------------------------------------------------\n",
            "86.6 M    Trainable params\n",
            "0         Non-trainable params\n",
            "86.6 M    Total params\n",
            "346.472   Total estimated model params size (MB)\n",
            "20        Modules in train mode\n",
            "212       Modules in eval mode\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0:   0%|          | 0/509 [00:00<?, ?it/s] "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [01:00<00:00,  8.40it/s, v_num=63, train_loss_step=3.280, train_acc_step=1.000, val_loss=3.420, val_acc=0.0678, train_loss_epoch=3.430, train_acc_epoch=0.0472]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved. New best score: 3.420\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [00:52<00:00,  9.66it/s, v_num=63, train_loss_step=3.490, train_acc_step=0.000, val_loss=3.410, val_acc=0.0678, train_loss_epoch=3.420, train_acc_epoch=0.0393]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.012 >= min_delta = 0.01. New best score: 3.408\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [00:55<00:00,  9.25it/s, v_num=63, train_loss_step=3.310, train_acc_step=0.000, val_loss=3.390, val_acc=0.0678, train_loss_epoch=3.420, train_acc_epoch=0.0521]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.016 >= min_delta = 0.01. New best score: 3.392\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509/509 [00:54<00:00,  9.34it/s, v_num=63, train_loss_step=3.560, train_acc_step=0.000, val_loss=3.380, val_acc=0.0678, train_loss_epoch=3.410, train_acc_epoch=0.0639]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.010 >= min_delta = 0.01. New best score: 3.382\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17:  15%|â–ˆâ–        | 75/509 [00:07<00:41, 10.51it/s, v_num=63, train_loss_step=3.180, train_acc_step=0.500, val_loss=3.380, val_acc=0.0678, train_loss_epoch=3.410, train_acc_epoch=0.0659] "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 222/509 [2:42:50<3:30:30,  0.02it/s, v_num=61, train_loss_step=3.290, train_acc_step=0.000, val_loss=3.210, val_acc=0.159, train_loss_epoch=3.310, train_acc_epoch=0.106]\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'exit' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:48\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:599\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    593\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    594\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    595\u001b[39m     ckpt_path,\n\u001b[32m    596\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    597\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    598\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1012\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1016\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1056\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1055\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m-> \u001b[39m\u001b[32m1056\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1057\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:216\u001b[39m, in \u001b[36m_FitLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28mself\u001b[39m.on_advance_start()\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28mself\u001b[39m.on_advance_end()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:455\u001b[39m, in \u001b[36m_FitLoop.advance\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    454\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepoch_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:152\u001b[39m, in \u001b[36m_TrainingEpochLoop.run\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_end(data_fetcher)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:344\u001b[39m, in \u001b[36m_TrainingEpochLoop.advance\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    342\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer.lightning_module.automatic_optimization:\n\u001b[32m    343\u001b[39m     \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m     batch_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mautomatic_optimization\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:192\u001b[39m, in \u001b[36m_AutomaticOptimization.run\u001b[39m\u001b[34m(self, optimizer, batch_idx, kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m result = closure.consume_result()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:270\u001b[39m, in \u001b[36m_AutomaticOptimization._optimizer_step\u001b[39m\u001b[34m(self, batch_idx, train_step_and_backward_closure)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moptimizer_step\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:176\u001b[39m, in \u001b[36m_call_lightning_module_hook\u001b[39m\u001b[34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\pytorch_lightning\\core\\module.py:1328\u001b[39m, in \u001b[36mLightningModule.optimizer_step\u001b[39m\u001b[34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[39m\n\u001b[32m   1304\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[32m   1305\u001b[39m \u001b[33;03mthe optimizer.\u001b[39;00m\n\u001b[32m   1306\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1326\u001b[39m \n\u001b[32m   1327\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1328\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\pytorch_lightning\\core\\optimizer.py:154\u001b[39m, in \u001b[36mLightningOptimizer.step\u001b[39m\u001b[34m(self, closure, **kwargs)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_strategy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[38;5;28mself\u001b[39m._on_after_step()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:239\u001b[39m, in \u001b[36mStrategy.optimizer_step\u001b[39m\u001b[34m(self, optimizer, closure, model, **kwargs)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl.LightningModule)\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprecision_plugin\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\pytorch_lightning\\plugins\\precision\\amp.py:79\u001b[39m, in \u001b[36mMixedPrecision.optimizer_step\u001b[39m\u001b[34m(self, optimizer, model, closure, **kwargs)\u001b[39m\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[33m\"\u001b[39m\u001b[33mAMP and the LBFGS optimizer are not compatible.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m closure_result = \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# If backward was skipped in automatic optimization (return None), unscaling is not needed\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:146\u001b[39m, in \u001b[36mClosure.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> Optional[Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     \u001b[38;5;28mself\u001b[39m._result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result.loss\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:131\u001b[39m, in \u001b[36mClosure.closure\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;129m@torch\u001b[39m.enable_grad()\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> ClosureResult:\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m step_output.closure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:319\u001b[39m, in \u001b[36m_AutomaticOptimization._training_step\u001b[39m\u001b[34m(self, kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m trainer = \u001b[38;5;28mself\u001b[39m.trainer\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m training_step_output = \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtraining_step\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer.strategy.post_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:328\u001b[39m, in \u001b[36m_call_strategy_hook\u001b[39m\u001b[34m(trainer, hook_name, *args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer.strategy.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:391\u001b[39m, in \u001b[36mStrategy.training_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    390\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_redirection(\u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.lightning_module, \u001b[33m\"\u001b[39m\u001b[33mtraining_step\u001b[39m\u001b[33m\"\u001b[39m, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m391\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlightning_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 209\u001b[39m, in \u001b[36mWMMDClassifier.training_step\u001b[39m\u001b[34m(self, batch, batch_idx)\u001b[39m\n\u001b[32m    208\u001b[39m x, y = batch\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    210\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.criterion(logits, y)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 196\u001b[39m, in \u001b[36mWMMDClassifier.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    192\u001b[39m img = torch.nn.functional.interpolate(\n\u001b[32m    193\u001b[39m     spec, size=(\u001b[32m224\u001b[39m, \u001b[32m224\u001b[39m), mode=\u001b[33m\"\u001b[39m\u001b[33mbilinear\u001b[39m\u001b[33m\"\u001b[39m, align_corners=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    194\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m pixel_values = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.pixel_values.to(img.device)\n\u001b[32m    197\u001b[39m outputs = \u001b[38;5;28mself\u001b[39m.backbone(pixel_values=pixel_values)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\transformers\\image_processing_utils.py:44\u001b[39m, in \u001b[36mBaseImageProcessor.__call__\u001b[39m\u001b[34m(self, images, **kwargs)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\transformers\\utils\\generic.py:844\u001b[39m, in \u001b[36mfilter_out_non_signature_kwargs.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    837\u001b[39m     warnings.warn(\n\u001b[32m    838\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe following named arguments are not valid for `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcls_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    839\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m and were ignored: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minvalid_kwargs_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    840\u001b[39m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m    841\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    842\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m844\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mvalid_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\transformers\\models\\vit\\image_processing_vit.py:263\u001b[39m, in \u001b[36mViTImageProcessor.preprocess\u001b[39m\u001b[34m(self, images, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, return_tensors, data_format, input_data_format, do_convert_rgb)\u001b[39m\n\u001b[32m    262\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_resize:\n\u001b[32m--> \u001b[39m\u001b[32m263\u001b[39m     images = \u001b[43m[\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43msize_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_rescale:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\transformers\\models\\vit\\image_processing_vit.py:264\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    262\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_resize:\n\u001b[32m    263\u001b[39m     images = [\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43msize_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    265\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[32m    266\u001b[39m     ]\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_rescale:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\transformers\\models\\vit\\image_processing_vit.py:144\u001b[39m, in \u001b[36mViTImageProcessor.resize\u001b[39m\u001b[34m(self, image, size, resample, data_format, input_data_format, **kwargs)\u001b[39m\n\u001b[32m    143\u001b[39m output_size = (size[\u001b[33m\"\u001b[39m\u001b[33mheight\u001b[39m\u001b[33m\"\u001b[39m], size[\u001b[33m\"\u001b[39m\u001b[33mwidth\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m    \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\transformers\\image_transforms.py:373\u001b[39m, in \u001b[36mresize\u001b[39m\u001b[34m(image, size, resample, reducing_gap, data_format, return_numpy, input_data_format)\u001b[39m\n\u001b[32m    372\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image, PIL.Image.Image):\n\u001b[32m--> \u001b[39m\u001b[32m373\u001b[39m     do_rescale = \u001b[43m_rescale_for_pil_conversion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    374\u001b[39m     image = to_pil_image(image, do_rescale=do_rescale, input_data_format=input_data_format)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\transformers\\image_transforms.py:144\u001b[39m, in \u001b[36m_rescale_for_pil_conversion\u001b[39m\u001b[34m(image)\u001b[39m\n\u001b[32m    143\u001b[39m     do_rescale = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mallclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m np.all(\u001b[32m0\u001b[39m <= image) \u001b[38;5;129;01mand\u001b[39;00m np.all(image <= \u001b[32m255\u001b[39m):\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\numpy\\_core\\numeric.py:2329\u001b[39m, in \u001b[36mallclose\u001b[39m\u001b[34m(a, b, rtol, atol, equal_nan)\u001b[39m\n\u001b[32m   2245\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2246\u001b[39m \u001b[33;03mReturns True if two arrays are element-wise equal within a tolerance.\u001b[39;00m\n\u001b[32m   2247\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2327\u001b[39m \n\u001b[32m   2328\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2329\u001b[39m res = \u001b[38;5;28mall\u001b[39m(\u001b[43misclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[43m=\u001b[49m\u001b[43matol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   2330\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m builtins.bool(res)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\numpy\\_core\\numeric.py:2447\u001b[39m, in \u001b[36misclose\u001b[39m\u001b[34m(a, b, rtol, atol, equal_nan)\u001b[39m\n\u001b[32m   2446\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m errstate(invalid=\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m2447\u001b[39m     result = (\u001b[43mless_equal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mabs\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m-\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mabs\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2448\u001b[39m               & isfinite(y)\n\u001b[32m   2449\u001b[39m               | (x == y))\n\u001b[32m   2450\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m equal_nan:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     25\u001b[39m callbacks = [metrics_cb, early_stopping]\n\u001b[32m     27\u001b[39m trainer = pl.Trainer(\n\u001b[32m     28\u001b[39m     max_epochs=cfg[\u001b[33m'\u001b[39m\u001b[33mmax_epochs\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     29\u001b[39m     accelerator=\u001b[33m'\u001b[39m\u001b[33mgpu\u001b[39m\u001b[33m'\u001b[39m, devices=\u001b[32m1\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     36\u001b[39m     callbacks=callbacks\n\u001b[32m     37\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m test_res = trainer.test(model, dm)[\u001b[32m0\u001b[39m]\n\u001b[32m     42\u001b[39m model.test_results = test_res\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:561\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yohanes.setiawan\\AppData\\Local\\miniconda3\\envs\\wmmd_env\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:65\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[32m     64\u001b[39m         launcher.kill(_get_sigkill_signal())\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     \u001b[43mexit\u001b[49m(\u001b[32m1\u001b[39m)\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[32m     68\u001b[39m     _interrupt(trainer, exception)\n",
            "\u001b[31mNameError\u001b[39m: name 'exit' is not defined"
          ]
        }
      ],
      "source": [
        "# training Loops\n",
        "for cfg in model_configs:\n",
        "    dm = WMMDDataModule(\n",
        "        dataset_dict=ds,\n",
        "        backbone=cfg[\"backbone\"],\n",
        "        batch_size = cfg.get(\"batch_size\", 2),\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    model = WMMDClassifier(\n",
        "        num_classes=cfg['num_classes'], lr=cfg['lr'],\n",
        "        backbone=cfg['backbone'], finetune=cfg['finetune'],\n",
        "        class_weights=cfg['class_weights'], \n",
        "        ckpt_path=cfg['ckpt_path']\n",
        "    )\n",
        "    metrics_cb = MetricsLogger()\n",
        "    # callback for early stopping\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        mode='min',\n",
        "        patience=20,\n",
        "        min_delta=0.01,\n",
        "        verbose=True\n",
        "    )\n",
        "    callbacks = [metrics_cb, early_stopping]\n",
        "\n",
        "    trainer = pl.Trainer(\n",
        "        max_epochs=cfg['max_epochs'],\n",
        "        accelerator='gpu', devices=1,\n",
        "        precision='16-mixed', \n",
        "        accumulate_grad_batches=2,\n",
        "        check_val_every_n_epoch=1,\n",
        "        num_sanity_val_steps=0,\n",
        "        enable_progress_bar=True,\n",
        "        log_every_n_steps=1,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    trainer.fit(model, dm)\n",
        "    test_res = trainer.test(model, dm)[0]\n",
        "\n",
        "    model.test_results = test_res\n",
        "    model.finish_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    model.epochs_trained = trainer.current_epoch + 1\n",
        "    model.save_model()\n",
        "\n",
        "    metrics_map = {\n",
        "        'accuracy':   ('train_accs',      'val_accs'),\n",
        "        'precision':  ('train_precisions','val_precisions'),\n",
        "        'recall':     ('train_recalls',   'val_recalls'),\n",
        "        'f1_score':   ('train_f1s',       'val_f1s'),\n",
        "    }\n",
        "    \n",
        "    for metric_name, (train_attr, val_attr) in metrics_map.items():\n",
        "        train_vals = getattr(metrics_cb, train_attr)\n",
        "        val_vals = getattr(metrics_cb, val_attr)\n",
        "        epochs = list(range(1, len(train_vals) + 1))\n",
        "\n",
        "        plt.figure()\n",
        "        plt.plot(epochs, train_vals, label=f'train_{metric_name}')\n",
        "        plt.plot(epochs, val_vals,   label=f'val_{metric_name}')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel(metric_name.replace('_', ' ').title())\n",
        "        plt.title(f\"{metric_name.replace('_', ' ').title()} over Epochs {model._last_timestamp}\")\n",
        "        plt.grid(True)\n",
        "        plt.legend(loc='best')\n",
        "\n",
        "        plot_file = os.path.join(model._last_save_dir, f\"{model._last_timestamp}_{metric_name}.png\")\n",
        "        plt.savefig(plot_file)\n",
        "        plt.close()\n",
        "\n",
        "    print(f\"Completed {cfg['backbone']} ({'FT' if cfg['finetune'] else 'Frozen'}), artifacts in {model._last_save_dir}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "wmmd_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
