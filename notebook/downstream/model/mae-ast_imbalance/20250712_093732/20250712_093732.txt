Model architecture:
WMMDClassifier(
  (backbone): UpstreamExpert(
    (model): MAE_AST(
      (feature_extractor): Identity()
      (post_extract_proj): Linear(in_features=256, out_features=768, bias=True)
      (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (batch_norm): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
      (unfold): Unfold(kernel_size=(16, 16), dilation=1, padding=0, stride=(16, 16))
      (dropout_input): Dropout(p=0.1, inplace=False)
      (enc_sine_pos_embed): SinusoidalPositionalEncoding()
      (encoder): TransformerEncoder(
        (layers): ModuleList(
          (0-3): 4 x TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.0, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (dec_sine_pos_embed): SinusoidalPositionalEncoding()
      (decoder): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.0, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (final_proj_reconstruction): Linear(in_features=768, out_features=256, bias=True)
      (final_proj_classification): Linear(in_features=768, out_features=256, bias=True)
    )
  )
  (classifier): Sequential(
    (0): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)
    (1): Dropout(p=0.5, inplace=False)
    (2): Linear(in_features=6144, out_features=31, bias=True)
  )
  (criterion): CrossEntropyLoss()
  (train_precision): MulticlassPrecision()
  (train_recall): MulticlassRecall()
  (train_f1): MulticlassF1Score()
  (val_precision): MulticlassPrecision()
  (val_recall): MulticlassRecall()
  (val_f1): MulticlassF1Score()
  (test_precision): MulticlassPrecision()
  (test_recall): MulticlassRecall()
  (test_f1): MulticlassF1Score()
)

Hyperparameters:
{
    "num_classes": 31,
    "backbone_lr": 0.0001,
    "head_lr": 0.0001,
    "weight_decay": 0.05,
    "max_epochs": 20,
    "backbone": "mae-ast",
    "ckpt_path": "notebook/downstream/load_model/random-4en1de-seed31415.pt",
    "finetune": true,
    "class_weights": [
        0.9373271889400921,
        1.4912023460410557,
        1.0935483870967742,
        2.1870967741935483,
        0.9112903225806451,
        0.8633276740237691,
        1.0582726326742975,
        0.9373271889400921,
        1.0935483870967742,
        0.6308933002481389,
        0.8201612903225807,
        1.1716589861751152,
        0.8633276740237691,
        1.5622119815668203,
        5.467741935483871,
        0.7811059907834101,
        0.8633276740237691,
        3.2806451612903227,
        1.0935483870967742,
        1.0252016129032258,
        0.8201612903225807,
        1.0935483870967742,
        1.0935483870967742,
        0.8201612903225807,
        2.1870967741935483,
        0.7290322580645161,
        0.47545582047685836,
        0.6695194206714944,
        1.426367461430575,
        0.9648956356736242,
        0.9941348973607038
    ]
}

Test results:
{
    "test_loss": 1.2760435342788696,
    "test_acc": 0.6460176706314087,
    "test_precision": 0.6386430859565735,
    "test_recall": 0.5575221180915833,
    "test_f1": 0.5840712189674377
}

Epochs trained: 21
